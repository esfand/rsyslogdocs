= Rsyslog Guide

[[chap-Rsyslog_Guide-Preface]]

== Overview

Rsyslog is an open-source *nix implementation of the syslog protocol. 
It supports reliable syslog transport over TCP, local buffering, SSL/TLS/RELP, 
logging to databases, and email alerting.


== Message Life Cycle

message life cyle to be written.....


== Architecture

Rsyslog architecture description

== Configuration Scripts

confi scripts


== rsyslog.conf Configuration File

Rsyslog is configured via the rsyslog.conf file, typically found in /etc. 
By default, rsyslogd reads the file /etc/rsyslog.conf. 
This may be changed by command line option "-f".

Configuration file examples can be found in the rsyslog wiki. Also keep the 
rsyslog  config snippets on your mind. These are ready-to-use real building 
blocks for rsyslog configuration.

While rsyslogd contains enhancements over standard syslogd, efforts have been 
made to keep the configuration file as compatible as possible.  While, for 
obvious reasons, enhanced features require a different config file syntax, 
rsyslogd should be able to work with a standard syslog.conf file.  This is 
especially useful while you are migrating from syslogd to rsyslogd.

Follow the links below to learn more about specific topics:

* http://www.rsyslog.com/doc/rsyslog_conf_basic_structure.html[Basic Structure]
* http://www.rsyslog.com/doc/rsyslog_conf_modules.html[Modules]
* http://www.rsyslog.com/doc/rsyslog_conf_templates.html[Templates]
* http://www.rsyslog.com/doc/rsyslog_conf_filter.html[Filter Conditions]
* http://www.rsyslog.com/doc/rsyslog_conf_actions.html[Actions]
* http://www.rsyslog.com/doc/rsyslog_conf_output.html[Output Channels]
* Legacy Configuration Directives
* sysklogd compatibility


[[chap-Rsyslog_Guide-Introduction]]

== Introduction

Rsyslog is a component library for capturing and processing log messages 
([acronym]#JSF#).  The framework extends the Ajax capabilities of JSF with 
advanced features for the development of enterprise web applications.

RichFaces leverages several parts of the JSF 2 framework including the lifecycle, 
validation, conversion facilities, and management of static and dynamic resources. 
The RichFaces framework includes components with built-in Ajax support and a 
customizable look-and-feel that can be incorporated into JSF applications.

RichFaces provides a number of advantages for enterprise web application development:

* Create complex application views using out-of-the-box components. 
The RichFaces user interface ([acronym]#UI#) library contains components for adding 
rich interactive features to JSF applications. It extends the RichFaces framework to 
include a large set of Ajax-enabled components that come with extensive skinning support. 
Additionally, the RichFaces framework is designed to be used seamlessly with other 
3d-party libraries on the same page, so you have more options for developing applications.

* Write your own customized rich components with built-in Ajax support. 
The Component Development Kit ([acronym]#CDK#), used for the RichFaces UI library creation, 
includes a code-generation facility and a templating facility 
using [acronym]#XHTML# (extended hyper-text markup language) syntax.

* Generate binary resources on the fly. Extensions to JSF 2 resource-handling facilities 
can generate images, sounds, [application]#Microsoft Excel# spreadsheets, 
and more during run-time.

* Create a modern rich user-interface with skinning technology. RichFaces provides a 
skinning feature that allows you to define and manage different color schemes and other 
parameters of the look and feel. It is possible to access the skin parameters from page 
code during run-time. RichFaces comes packaged with a number of skins to get you started, 
but you can also easily create your own customized skins too.

[[chap-Rsyslog_Guide-Getting_started_with_RichFaces]]

== Getting started with RichFaces

Follow the instructions in this chapter to configure the RichFaces framework and get started 
with application development.

If you have existing projects that use a previous version of RichFaces, 
refer to the *_RichFaces Migration Guide_* .

[[sect-Rsyslog_Guide-Getting_started_with_RichFaces-Technical_Requirements]]

=== Technical Requirements

The minimum technical requirements needed to get started with RichFaces are outlined below.

* Java Development Kit ([acronym]#JDK#) 6 or higher
* An application server compliant with Java Platform, Enterprise Edition 6 ([acronym]#JEE6#), such as [productname]#JBoss EAP 6#, [productname]#WildFly# or a servlet container coupled with a JSF implementation, such as Apache Tomcat + Mojarra 2.x.
* A compliant web browser, such as [productname]#Firefox 17#, [productname]#Chrome 23#, or [productname]#Internet Explorer 9#


[[sect-Rsyslog_Guide-Getting_started_with_RichFaces-Technical_Requirements-Project_libraries_and_dependencies]]

==== Project libraries and dependencies

The RichFaces library is distributed as a sinlge jar providing all the components and services of the RichFaces framework.

.RichFaces Library
* [filename]+richfaces.jar+

The framework depends on both mandatory and optional third-party dependencies.  Some of the framework services are only enabled when the optional libraries are present.

Note that these dependencies may further depend on their own runtime dependencies.

.Mandatory third-party dependencies
* Java Server Faces 2.x implementation
** [filename]+javax.faces.jar+ (version [code]+2.1.19+ or higher)
** or [filename]+myfaces-impl.jar+ (version [code]+2.1.10+ or higher)

* Google Guava
** [filename]+guava.jar+ (version [code]+13.0.1+)

* CSS Parser
** [filename]+cssparser.jar+ (version [code]+0.9.5+)

* Simple API for CSS
** [filename]+sac.jar+ (version [code]+1.3+)

.Optional third-party dependencies

* Bean validation (JSR-303) integration for client-side validation (JSR-303 API and Implementation)
** [filename]+validation-api.jar+ (version [code]+1.0.0.GA+)
** [filename]+hibernate-validator.jar+ (version [code]+4.2.0.Final+ or higher)

* Push transport library - Atmosphere (without dependencies)
** [filename]+atmosphere-runtime.jar+ (version [code]+1.0.10+)
+
(selected compatibility modules [filename]+atmosphere-compat-*.jar+ may be necessary)

* Push JMS integration (JMS API and Implementation)
** [filename]+jms.jar+ (version [code]+1.1+)
** [filename]+hornetq-jms.jar+ (version [code]+2.2.7.Final+ or higher)

* Push CDI integration (CDI API and Implementation)
** [filename]+cdi-api.jar+ (version [code]+1.0-SP4+)
** [filename]+javax.inject.jar+ (version [code]+1+)
** [filename]+jsr-250-api.jar+ (version [code]+1.0+)
** [filename]+weld-servlet.jar+ (version [code]+1.1.4.Final+)

* Extended caching (EhCache)
** [filename]+ehcache.jar+ (version [code]+1.6.0+)

[NOTE]
.Dependencies for servlet containers
====
Some of the dependencies are part of the Java EE 6 specification and thus it is not necessary to include them in projects running on Java EE applications servers.  It is still necessary to include them when using servlet containers.

This does not apply to dependencies on the Servlet API: the JSP API and the EL API.  These APIs are integral parts of both application servers and servlet containers.
====

[[sect-Rsyslog_Guide-Getting_started_with_RichFaces-Development_environments]]

=== Development environments

RichFaces applications can be developed using a range of tools, including integrated development environments ([acronym]++IDE++s). This chapter covers only two such environments in detail:

* [productname]#JBoss Developer Studio#, as described in <<sect-Rsyslog_Guide-Getting_started_with_RichFaces-Creating_a_project_with_JBoss_Tools>>.
* [productname]#Maven#, as described in <<sect-Rsyslog_Guide-Getting_started_with_RichFaces-Creating_a_project_with_Maven>>.


Other development environments such as [productname]#Idea# or [productname]#NetBeans# could also be used for RichFaces development, but such usage is not detailed in this book.

[[sect-Rsyslog_Guide-Getting_started_with_RichFaces-Setting_up_RichFaces]]

=== Setting up RichFaces

Follow the instructions in this section to set up a project with the RichFaces framework and begin building applications.

. *Download the RichFaces archive*
+
Download RichFaces from the JBoss RichFaces Downloads area at http://www.jboss.org/richfaces/download.html. The binary files (available as a [filename]+.zip+ archive) contain the following:
+
* compiled, ready-to-use Java Archives ([acronym]#JAR# files) of the RichFaces library
* library source JAR files
* documentation, including Java documentation and JavaScript documentation
* archetypes
* example source code

. *Unzip the archive*
+
Create a new directory named [filename]+RichFaces+, then unzip the archive that contains the binaries into this new directory.

[[sect-Rsyslog_Guide-Getting_started_with_RichFaces-Creating_a_project_with_JBoss_Tools]]

=== Creating a project with [productname]#JBoss Developer Studio#

Follow the procedure in this section to create a new RichFaces application with [productname]#JBoss Developer Studio#.  Ensure you are using the latest version of [productname]#JBoss Developer Studio# to take advantage of the latest feautures and stability improvements.

. *Create a new project*
+
Create a new project based on the JSF 2 environment using the RichFaces 5 template. In [productname]#JBoss Developer Studio#, select menu:File[New JSF Project] from the menu. Name the project, select [guilabel]#JSF 2# from the [guilabel]#JSF Environment# drop-down box, and click the [guibutton]#Finish# button to create the project.
+
If necessary, update the JSF 2 JAR files to the latest versions.
+
. *Add the RichFaces libraries to the project*
+
[[step-Rsyslog_Guide-Creating_a_project-Add_the_RichFaces_libraries_to_the_project]]
Add the <<sect-Rsyslog_Guide-Getting_started_with_RichFaces-Technical_Requirements-Project_libraries_and_dependencies,RichFaces libraries and their mandatory dependencies>> to the project. Copy them from the location where you unzipped the RichFaces archive to the [filename]+WebContent/WEB-INF/lib/+ directory of your project in [application]#JBoss Developer Studio#.
+
. *[[step-Rsyslog_Guide-Creating_a_project-Reference_the_tag_libraries]]Reference the tag library*
+
The RichFaces tag library must be referenced on each XHTML page in your project:
+
[source, XML]
----
<ui:composition xmlns:r="http://richfaces.org/rich">
   ...
</ui:composition>

----

You are now ready to begin developing your RichFaces application. RichFaces components can be dragged and dropped from the [productname]#JBoss Developer Studio# RichFaces palette into your application's XHTML pages.

[[sect-Rsyslog_Guide-Getting_started_with_RichFaces-Creating_a_project_with_Maven]]

=== Creating a project with [productname]#Maven#

[productname]#Apache Maven# is a build automation and project management tool for Java projects. Follow the instructions in this section to create a [productname]#Maven# project for [productname]#RichFaces#.

[[sect-Component_Using_RichFaces_with_Maven-Setting_up_Maven]]

==== Setting up [productname]#Maven#

[productname]#Maven# can be downloaded and installed from Apache's website at http://maven.apache.org/download.html. Due to the use of dependency importing, [productname]#Maven# version 3.0.4 or above is required.

Once [productname]#Maven# has been installed, no further configuration is required to begin building Maven projects.

[[sect-Component_Reference-Using_RichFaces_with_Maven-Using_the_RichFaces_project_archetype]]

==== Using the [productname]#RichFaces# project archetype

A Maven archetype is a template for creating projects. [productname]#Maven# uses an archetype to generate a directory structure and files for a particular project, as well as creating [filename]+pom.xml+ files that contain build instructions.

The RichFaces distribution includes a Maven archetype named [filename]+richfaces-archetype-simpleapp+ for generating the basic structure and requirements of a RichFaces application project. Maven can obtain the archetype from maven central at http://search.maven.org. The archetype is also included with the RichFaces distribution in the [filename]+archetypes+ directory. Follow the procedure in this section to generate a new Maven-based RichFaces project using the archetype.

. Generate the project from the archetype
+
The project can be generated with the [filename]+richfaces-archetype-simpleapp+ archetype. Create a new directory for your project, then run the following Maven command in the directory:
+
----
mvn archetype:generate -DarchetypeGroupId=org.richfaces.archetypes -DarchetypeArtifactId=richfaces-archetype-simpleapp -DarchetypeVersion=5.0.0-SNAPSHOT -DgroupId=org.docs.richfaces -DartifactId=new_project
----
+
The following parameters can be used to customize your project:
+
_-DgroupId_:: Defines the package for the Managed Beans
_-DartifactId_:: Defines the name of the project
+
The command generates a new RichFaces project with the following structure:
+
----
new_project
├── pom.xml
├── readme.txt
└── src
    └── main
        ├── java
        │   └── org
        │       └── docs
        │           └── richfaces
        │               └── RichBean.java
        └── webapp
            ├── index.xhtml
            ├── templates
            │   └── template.xhtml
            └── WEB-INF
                ├── faces-config.xml
                └── web.xml
----

. *Add test dependencies (optional)*
+
Your root directory of your project contains a project descriptor file: [filename]+pom.xml+. If you wish to include modules for test-driven JSF development, add any dependencies for the tests to the [filename]+pom.xml+ file.
+
For testing the server-side part of your application, check out [productname]#link:$$http://www.jboss.org/arquillian$$[JBoss Arquillian project]#.
+
If you want to test JSF from client's perspective with ability to access state of JSF internals, use [productname]#link:$$https://github.com/arquillian/arquillian-extension-warp/blob/master/README.md$$[Arquillian Warp]#.
+
For automation of client-side tests in real-browser, you may want to employ [productname]#link:$$http://community.jboss.org/wiki/ArquillianGraphene$$[Arquillian Graphene]# and [productname]#link:$$https://docs.jboss.org/author/display/ARQ/Drone$$[Arquillian Drone]# extensions.

. *Build the project*
+
Build the project from the command line by entering the _mvn install_ command.
+
The *BUILD SUCCESSFUL* message indicates the project has been assembled and is ready to import into an IDE (integrated development environment), such as [productname]#JBoss Developer Studio#.

. *Import the project into an IDE*
+
To import the project into [productname]#Eclipse# and [productname]#JBoss Developer Studio#, use the JBoss Maven Integration plug-ins. These plug-ins work with plug-ins from the M2Eclipse project to import Maven projects.

.. *Install the plug-ins*

... Choose menu:Help[Install New Software] from the Eclipse menu.

... Select the JBoss Developer Studio update site to use, then open the [guilabel]#Maven Support# group and select the [guilabel]#JBoss Maven Integration# and [guilabel]#JBoss Maven JSF Configurator# plug-ins.

... Follow the prompts to install the integration plug-ins. The installation will automatically include the transitive dependencies [guilabel]#Maven Integration for Eclipse# and [guilabel]#Maven Integration for WTP# . Both of these dependencies are from the M2Eclipse project.

... Restart Eclipse to finish the installation.

.. *Open the importing wizard*
+
With the plug-ins installed, open the importing wizard by choosing menu:File[Import] from the menu.

.. *Select the project*
+
Select menu:Maven[Existing Maven Projects] as the import source and choose the directory with the [filename]+pom.xml+ file for your project.

[NOTE]
.Exporting from Maven
====
The ability to prepare the project for Eclipse and export it using Maven is deprecated in RichFaces 5.0.0-SNAPSHOT. The process does not support JBoss integration-specific features, such as JSF Facets.
====

Your project is now ready to use. Once components and functionality have been added, you can run the application on a server and access it through a web browser at the address [filename]+http://localhost:8080/jsf-app/+.

[[sect-Rsyslog_Guide-Getting_started_with_RichFaces-Using_RichFaces_in_existing_JSF2_projects]]

=== Using RichFaces in existing JSF 2 projects

RichFaces can be added to existing JSF 2 projects by adding the new RichFaces libraries. Refer to <<step-Rsyslog_Guide-Creating_a_project-Add_the_RichFaces_libraries_to_the_project, Step 2>> and <<step-Rsyslog_Guide-Creating_a_project-Reference_the_tag_libraries, Step 3>> in <<sect-Rsyslog_Guide-Getting_started_with_RichFaces-Creating_a_project_with_JBoss_Tools>> for details.

[NOTE]
.Application-level settings
====
In RichFaces 5, it is not necessary to add any extra settings to the [filename]+web.xml+ or [filename]+config.xml+ settings files to use the framework.
====

[[chap-Rsyslog_Guide-RichFaces_overview]]

== RichFaces overview

Read this chapter for technical details on the RichFaces framework.

[[sect-Component_Reference-RichFaces_overview-Full_technical_requirements]]

=== Full technical requirements

RichFaces has been developed with an open architecture to be compatible with a wide variety of environments.

[[sect-Component_Reference-RichFaces_overview-Server_requirements]]

==== Server requirements

RichFaces 5 requires either of the following server technologies:

* An application server compliant with Java Platform, Enterprise Edition 6 ([acronym]#JEE6# or [acronym]#JEE6#), such as [productname]#JBoss Application Server 7#.
* A major servlet container, such as [productname]#Jetty 8# or [productname]#Apache Tomcat 7#.


[[sect-Component_Reference-RichFaces_overview-Client_requirements]]

==== Client requirements

Clients accessing RichFaces applications require a web browser. For a list of supported web browsers, refer to the link:$$https://community.jboss.org/wiki/PrioritizedRichFacesBrowsersCompatibilityMatrix$$[browser compatibility matrix] in the RichFaces wiki.

[[sect-Component_Reference-RichFaces_overview-Development_requirements]]

==== Development requirements

Developing applications with the RichFaces framework requires the Java Development Kit ([acronym]#JDK#), an implementation of JavaServer Faces ([acronym]#JSF#), and a development environment.

Java Development Kit ([acronym]#JDK#):: RichFaces supports the following JDK versions:
+
* JDK 1.6 and higher

JavaServer Faces ([acronym]#JSF#):: RichFaces supports the following JSF implementations and frameworks:
+
* [productname]#MyFaces 2.x#
* [productname]#Mojara 2.x#

Development environment:: RichFaces can be developed using most Java development environments.  The following are recommended, and used for examples in this guide:
+
* [productname]#JBoss Developer Studio 6.x# and higher
* [productname]#Maven 3.0.4# and higher


[[sect-Rsyslog_Guide-RichFaces_overview-Architecture]]

=== Architecture

The important elements of the RichFaces framework are as follows:

* Ajax Action Components
* Ajax Containers
* Ajax Output
* Skins and Theming
* RichFaces Ajax Extensions


Read this section for details on each element.

[[sect-Rsyslog_Guide-Architecture-Ajax_Action_Components]]

==== Ajax Action Components

The RichFaces framework includes several Ajax Action Components and Submitting Bahaviors: [sgmltag]+<r:commandButton>+, [sgmltag]+<r:commandLink>+, [sgmltag]+<r:poll>+, [sgmltag]+<r:ajax>+, and more. Use Ajax Action Components to send Ajax requests from the client side.

[[sect-Rsyslog_Guide-Architecture-Ajax_Containers]]

==== Ajax Containers

[classname]+AjaxContainer+ is an interface that marks part of the JSF tree that is decoded during an Ajax request. It only marks the JSF tree if the component or behavior sending the request does not explicitly specify an alternative. [classname]+AjaxRegion+ is an implementation of this interface.

[[sect-Rsyslog_Guide-Architecture-Ajax_Output]]

==== Ajax Output

[classname]+AjaxContainer+ is an interface that marks part of the JSF tree that will be updated and rendered on the client for every Ajax request. It only marks the JSF tree if the component or behavior sending the request does not explicitly turn off automatic updates.

[[sect-Rsyslog_Guide-Architecture-Skins_and_theming]]

==== Skins and theming

RichFaces includes extensive support for application skinning. Skinning is a high-level extension to traditional [acronym]#CSS# (Cascading Style Sheets) which allows the color scheme and appearance of an application to be easily managed. The skins simplify look-and-feel design by allowing multiple elements of the interface to be handled as manageable features, which have associated color palettes and styling. Application skins can additionally be changed on the fly during run-time, allowing user experiences to be personalized and customized.

For full details on skinning and how to create skins for the components in your application, refer to <<chap-Rsyslog_Guide-Skinning_and_theming>>.

[[sect-Rsyslog_Guide-Architecture-RichFaces_Ajax_Extensions]]

==== RichFaces Ajax Extensions

The RichFaces Ajax Extensions plug in to the standard JSF 2 Ajax script facility. They extend the script facility with new features and options.

[[sect-Rsyslog_Guide-RichFaces_overview-Technologies]]

=== Technologies

RichFaces 5 features full JSF 2 integration and uses standard web application technologies such as JavaScript, [acronym]#XML# (Extensible Markup Language), and [acronym]#XHTML# (Extensible Hypertext Markup Language).

[[sect-Rsyslog_Guide-RichFaces_overview-Differences_between_JSF_and_RichFaces_mechanisms]]

=== Differences between [acronym]#JSF# and RichFaces mechanisms

JavaServer Faces 2 evaluates Ajax options, such as [code]+execute+ and [code]+render+, while rendering a page. This allows any parameters to be sent directly from the client side.

RichFaces evaluates the options when the current request is sent. This increases both the security of the data and the convenience for evaluating parameters.

For example, binding Ajax options to Java Bean properties in RichFaces allows you to evaluate the options dynamically for the current request, such as defining additional zones to render. Parameters changed manually on the client side will not influence the request processing. With JSF 2, the options have evaluated during the previous page rendering would need to be used.

[[sect-Rsyslog_Guide-RichFaces_overview-Restrictions]]

=== Restrictions

The following restrictions apply to applications implementing the RichFaces framework:

* As with most Ajax frameworks, you should not attempt to append or delete elements on a page using RichFaces Ajax, but should instead replace them. As such, elements that are rendered conditionally should not be targeted in the [varname]+render+ attributes for Ajax controls. For successful updates, an element with the same identifier as in the response must exist on the page. If it is necessary to append code to a page, include a placeholder for it (an empty element).

* JSF 2 does not allow resources such as JavaScript or Cascading Style Sheets ([acronym]#CSS#) to be added if the element requiring the resource is not initially present in the JSF tree. As such, components added to the tree via Ajax must have any required resources already loaded. In RichFaces, any components added to the JSF tree should have components with corresponding resources included on the main page initially. To facilitate this, components can use the [code]+rendered="false"+ setting to not be rendered on the page.

* JSF does render resource links (stylesheets, scripts) in order of occurence, thus if you add [code]+<h:outputStylesheet>+ to the [code]+<h:head>+ section, JSF will render it before the RichFaces resource links (dependencies of RichFaces components). To be able to overwrite RichFaces stylesheets and re-use RichFaces JavaScript implementation, you need to render [code]+<h:outputStylesheet target="head">+ to the [code]+<h:body>+ section (safe solution is to place it on the end of the section; however to keep readability, you can use start of the section).

* Switching RichFaces skins via Ajax during runtime should be avoided, as this requires all the stylesheets to be reloaded.

[[chap-Rsyslog_Guide-Basic_concepts]]

== Basic concepts

Read this chapter for the basic concepts of using RichFaces in conjunction with Ajax and JavaServer Faces.

[[sect-Rsyslog_Guide-Basic_concepts-Sending_an_Ajax_request]]

=== Sending an Ajax request

Many of the tags in the [classname]+r+ tag library are capable of sending Ajax requests from a JavaServer Faces (JSF) page.

* The [sgmltag]+<r:commandButton>+ and [sgmltag]+<r:commandLink>+ tags are used to send an Ajax request on the [varname]+click+ JavaScript event.
* The [sgmltag]+<r:poll>+ tag is used to send an Ajax request periodically using a timer.
* The [sgmltag]+<r:ajax>+ tag allows you to add Ajax functionality to standard JSF components and send Ajax request on a chosen JavaScript event, such as [varname]+keyup+ or [varname]+mouseover+, for example.
* Most components in the [classname]+r+ tag library have built-in Ajax support. Refer to the *_RichFaces Component Reference_* for details on the use of each component.


[[sect-Rsyslog_Guide-Basic_concepts-Partial_tree_processing]]

=== Partial tree processing

Use the [varname]+execute+ attribute to specify which parts of the JSF tree to process during an Ajax request. The [varname]+execute+ attribute can point to an [varname]+id+ identifier of a specific component to process. Components can also be identified through the use of Expression Language ([acronym]#EL#).

Alternatively, the [varname]+execute+ attribute accepts the following keywords:

[code]+@all+:: Every component is processed.
[code]+@none+:: No components are processed.
[code]+@this+:: The requesting component with the [varname]+execute+ attribute is processed.
[code]+@form+:: The form that contains the requesting component is processed.
[code]+@region+:: The region that contains the requesting component is processed. Use the [sgmltag]+<r:region>+ component as a wrapper element to specify regions.
Some components make use of additional keywords. These are detailed under the relevant component entry in the *_RichFaces Component Reference_* .

[[sect-Rsyslog_Guide-Basic_concepts-Partial_view_updates]]

=== Partial view updates

Use the [varname]+render+ attribute to specify which components to render for an Ajax update. The [varname]+render+ attribute can point to an [varname]+id+ identifier of a specific component to update. Components can also be identified through the use of Expression Language ([acronym]#EL#).

Alternatively, the [varname]+render+ attribute accepts the following keywords:

[code]+@all+:: Every component is updated.
[code]+@none+:: No components are updated.
[code]+@this+:: The requesting component with the [varname]+execute+ attribute is updated.
[code]+@form+:: The form that contains the requesting component is updated.
[code]+@region+:: The region that contains the requesting component is updated. Use the [sgmltag]+<r:region>+ component as a wrapper element to specify regions.

Some components make use of additional keywords. These are detailed under the relevant component entry in the *_RichFaces Component Reference_* .

Use the [sgmltag]+<r:outputPanel>+ component with the [code]+ajaxRendered="true"+ setting to always update a section irrespective of the requesting component's [varname]+render+ attribute. The [sgmltag]+<r:message>+ and [sgmltag]+<r:messages>+ components are based on the [sgmltag]+<r:outputPanel>+ component, and as such will also always be updated. To override this behavior, use the [code]+limitRender="true"+ setting on the requesting component.

[[sect-Rsyslog_Guide-Basic_concepts-Component_overview]]

=== Component overview

The RichFaces framework is made up of the sinlge tag library: the [classname]+r+ library. The [classname]+r+ tag library includes both the low-level ajax functionality, as well as the high-level components for builidng web applications.  This allows developers to make use of custom Ajax behavior with existing componentsas well as leverage the many ready-made, self-contained components. These components don't require additional configuration in order to send requests or update.

For details on the use of the various components, refer to *_RichFaces Component Reference_* .

[[chap-Rsyslog_Guide-Advanced_features]]

== Advanced features

Read this chapter for details on some of the advanced features and configuration possibilities for the RichFaces framework.

[[sect-Rsyslog_Guide-Advanced_features-JSF2_integration]]

=== JSF 2 integration

JavaServer Faces ([acronym]#JSF#) is the Java-based web application framework upon which the RichFaces framework has been built. RichFaces is now integrated with JSF 2, which features several improvements to the framework.

* The standard display technology used by JSF 1 was JavaServer Pages ([acronym]#JSP#). With JSF 2, the standard display technology has been changed to Facelets, which is a more powerful and more efficient View Declaration Language ([acronym]#VLD#) than JSP.


[[sect-Rsyslog_Guide-Advanced_features-Error_handling]]

=== Error handling

RichFaces allows standard handlers to be defined for processing different application exceptions. Custom JavaScript can be executed when these exceptions occur.

[[sect-Rsyslog_Guide-Advanced_features-Client-side_errors]]

==== Client-side errors

JSF provides a global [code]+onError+ handler on the client. The handler provides the relevant error code and other associated data. The RichFaces Ajax components provide the [varname]+error+ attribute if extra functionality needs to be defined.

Additional processing is available through a number of components, such as the following:

* The [sgmltag]+<r:status>+ component has an additional error state.
* The [sgmltag]+<r:queue>+ component can be used to process errors.


[[sect-Rsyslog_Guide-Advanced_features-Server-side_errors]]

==== Server-side errors

Use the JSF 2 [classname]+ExceptionHandler+ class to handle server-side errors such as session expiration.

[[sect-Rsyslog_Guide-Advanced_features-Other_functions]]

=== Other functions

RichFaces provides a number of advanced functions, such as managing user roles and identifying elements. Refer to the *_Functions_* chapter in the *_RichFaces Component Reference_* for further details.

[[sect-Rsyslog_Guide-Advanced_features-Resource_loading]]

=== Resource loading

The RichFaces improves a standard JSF resource handling in order to achieve following features:

* resource optimization - serves optimized component resource dependencies (JavaScript, CSS)
* resource mapping - re-routes resource requests (maps an one resource to an another resource)


[[sect-Rsyslog_Guide-Advanced_features-Configuring_ResourceServlet]]

==== Configuring ResourceServlet

For leveraging RichFaces resource loading improvements, the [sgmltag]+ResourceServlet+ needs to be registered.

[sgmltag]+ResourceServlet+ is automatically registered in the Servlet 3.0 and higher environments.

In the Servlet 2.5 and lower environments, it is necessary to register the [sgmltag]+ResourceServlet+ manually in the [sgmltag]+WEB-INF/web.xml+ configuration file:


[source, XML]
----
<servlet>
    <servlet-name>Resource Servlet</servlet-name>
    <servlet-class>org.richfaces.servlet.ResourceServlet</servlet-class>
    <load-on-startup>1</load-on-startup>
</servlet>

<servlet-mapping>
    <servlet-name>Resource Servlet</servlet-name>
    <url-pattern>/org.richfaces.resources/*</url-pattern>
</servlet-mapping>
----

[[sect-Rsyslog_Guide-Advanced_features-Resource_optimization]]

==== Resource optimization

The resource optimization feature provides optimized component dependencies - JavaScript, CSS - which are compressed and aggregated to resource packages.

The loading of compressed resource packages may lead into significant client performance boost, since many small files are aggregated into one big file - the number of HTTP connections necessary to download application resources is significantly decreased.

.Enabling resource optimization
====

To enable the resource optimization, add a following configuration to [code]+web.xml+:


[source, XML]
----
<context-param>
    <param-name>org.richfaces.resourceOptimization.enabled</param-name>
    <param-value>true</param-value>
</context-param>
----

====

.Resource optimization in development JSF project stage
====

Resource optimization is influenced by the project stage:

* resources are not compressed in the development stage and during unit-testing to enable client-side debugging
* resources are compressed in the production stage and during a system-testing to minimize network bandwidth


Switch to the development project stage during a development:


[source, XML]
----
<context-param>
    <param-name>javax.faces.PROJECT_STAGE</param-name>
    <param-value>Development</param-value>
</context-param>
----

====

[[sect-Rsyslog_Guide-Advanced_features-Resource_mapping]]

==== Resource mapping

The resource mapping feature maps an existing JSF resource (determined by library and name) to a another resource.

This feature can help to solve the following cases:

* providing alternative versions of JSF resources
* map several JSF resources to one
* using external resources
* moving resources to servers serving static content


===== Resource mapping configuration file

Configuring the resource mapping means adding new records to the class-path file [code]+META-INF/richfaces/static-resource-mappings.properties+.

Each line in the configuration file represents one relocation.

A following sample shows a JSF resource with the name [code]+resourceLibrary:resourceName+ relocated to a resource [code]+anotherResourceLibrary:anotherResourceName+:

----
resourceLibrary\:resourceName=anotherResourceLibrary/anotherResourceName
----

[NOTE]
.Mapping resource name to relative URL
====
The definition above contains a JSF resource name on the left side of the expression and a relative path on the right side.

The expression on the right side represents a path relative to a JSF resource root, thus resource path [code]+anotherResourceLibrary/anotherResourceName+ actually maps to a JSF resource with name [code]+anotherResourceLibrary:anotherResourceName+.
====

[NOTE]
.Additional mapping files
====
It is possible to define additional resource mapping configuration files by using a contextual parameter identifying the class-path locations where the files reside: [code]+org.richfaces.resourceMapping.mappingFile+ (a comma-separated list of the class-path files).
====

===== Examples of resource mapping

.Providing alternative file
====

All requests for [code]+jquery.js+ are served as requests for [code]+jquery-alternative-version.js+:

----
jquery.js=jquery-alternative-version.js
----

====

.Mapping several resources to one
====

Both [code]+some:jquery.js+ and [code]+another:jquery.js+ are mapped to [code]+final:jquery.js+:

----
some\:jquery.js=final/jquery.js
another\:jquery.js=final/jquery.js
----

====

.Using external resources
====

Mappings with a resource path starting with [code]+http://+ or [code]+https://+ are served as absolute resource locations:

A following sample instructs to load [code]+jquery.js+ from CDN:

----
jquery.js=http://ajax.googleapis.com/ajax/libs/jquery/1.7.1/jquery.min.js
----

====

[[chap-Rsyslog_Guide-Skinning_and_theming]]

== Skinning and theming

Read this chapter for a guide to skinning and theming RichFaces applications, including how to implement themes, and details on customizing and extending skins.

[[sect-Rsyslog_Guide-Skinning_and_theming-What_are_skins]]

=== What are skins?

Application skins are used with the RichFaces framework to change the appearance of an application through setting the colors and decoration of controls and components. Typically the appearance of web applications is handled through the [acronym]#CSS# (Cascading Style Sheet) files associated with the application, but skinning allows the settings in a CSS file to be abstracted and easily edited. Skins consist of a small, generalized set of font and color parameters that can be applied to multiple different styles. This avoids repetitive coding and duplication in CSS files. CSS files are not completely replaced: skins work as a high-level extension to standard CSS.

Each skin has a set of [varname]+skin-parameters+, which are used to define the theme palette and other elements of the user interface. These parameters work together with regular CSS declarations, and can be referred to from within CSS using JavaServer Faces Expression Language ([acronym]#EL#).

The skinning feature of RichFaces also allows skins to be changed at runtime, so users can personalize an application's appearance on the fly.

[[sect-Rsyslog_Guide-Skinning_and_theming-Using_skins]]

=== Using skins

RichFaces includes a number of predefined skins. These skins can be used in RichFaces web applications by specifying the skin name in the [varname]+org.richfaces.skin+ context parameter in the [filename]+web.xml+ settings file. The predefined skins are as follows:

* +DEFAULT+
* +plain+, which contains no skin parameters and is intended for embedding RichFaces components into existing projects with their own styles.
* +emeraldTown+
* +blueSky+
* +wine+
* +japanCherry+
* +ruby+
* +classic+
* +deepMarine+


To add one of these skins to your application, add the +org.richfaces.SKIN+ context parameter to the [filename]+web.xml+ configuration file:

[source, XML]
----
<context-param>
    <param-name>org.richfaces.skin</param-name>
    <param-value>skin_name</param-value>
</context-param>
----

[[sect-Rsyslog_Guide-Skinning_and_theming-Skinning_overview]]

=== Skinning overview

RichFaces skins are implemented using the following three-level scheme:

Component stylesheets:: Stylesheets are provided for each component. CSS style parameters map to skin parameters defined in the skin property file. This mapping is accomplished through the use of ECSS files. Refer to <<sect-Rsyslog_Guide-Skinning_and_theming-ECSS_files>> for details on ECSS files.

Skin property files:: Skin property files map skin parameters to constant styles. Skin properties are defined in [filename]+skin.properties+ files. Refer to <<sect-Rsyslog_Guide-Skinning_overview-Skin_parameter_tables>> for a listing of the skin parameters used in a typical skin.

Custom style classes:: Individual components can use the [varname]+styleClass+ attribute to redefine specific elements. These components then use the styles defined in a CSS file instead of the standard look for components as defined by the ECSS stylesheets.
[[sect-Rsyslog_Guide-Skinning_overview-Skin_parameter_tables]]

==== Skin parameter tables

<<tabl-Rsyslog_Guide-Skin_parameter_tables_in_RichFaces-Parameter_settings_for_the_blueSky_skin>> lists the default values for the parameter settings in the +blueSky+ skin. These values are all listed in the [filename]+blueSky.skin.properties+ file, which can be customized and extended as described in <<sect-Rsyslog_Guide-Skinning_and_theming-Customizing_skins>>.

[[tabl-Rsyslog_Guide-Skin_parameter_tables_in_RichFaces-Parameter_settings_for_the_blueSky_skin]]

.Parameter settings for the +blueSky+ skin
[options="header"]
|===============
|Parameter name|Default value
|[parameter]+headerBackgroundColor+|+#BED6F8+
|[parameter]+headerGradientColor+|+#F2F7FF+
|[parameter]+headTextColor+|+#000000+
|[parameter]+headerWeightFont+|+bold+
|[parameter]+generalBackgroundColor+|+#FFFFFF+
|[parameter]+generalTextColor+|+#000000+
|[parameter]+generalSizeFont+|+11px+
|[parameter]+generalFamilyFont+|+Arial, Verdana, sans-serif+
|[parameter]+controlTextColor+|+#000000+
|[parameter]+controlBackgroundColor+|+#FFFFFF+
|[parameter]+additionalBackgroundColor+|+#ECF4FE+
|[parameter]+shadowBackgroundColor+|+#000000+
|[parameter]+shadowOpacity+|+1+
|[parameter]+panelBorderColor+|+#BED6F8+
|[parameter]+subBorderColor+|+#FFFFFF+
|[parameter]+calendarWeekBackgroundColor+|+#F5F5F5+
|[parameter]+calendarHolidaysBackgroundColor+|+#FFEBDA+
|[parameter]+calendarHolidaysTextColor+|+#FF7800+
|[parameter]+calendarCurrentBackgroundColor+|+#FF7800+
|[parameter]+calendarCurrentTextColor+|+#FFEBDA+
|[parameter]+calendarSpecBackgroundColor+|+#E4F5E2+
|[parameter]+calendarSpecTextColor+|+#000000+
|[parameter]+editorBackgroundColor+|+#F1F1F1+
|[parameter]+editBackgroundColor+|+#FEFFDA+
|[parameter]+errorColor+|+#FF0000+
|[parameter]+gradientType+|+plain+
|[parameter]+tabBackgroundColor+|+#C6DEFF+
|[parameter]+tabDisabledTextColor+|+#8DB7F3+
|[parameter]+tableHeaderBackgroundColor+|+#D6E6FB+
|[parameter]+tableSubHeaderBackgroundColor+|+#ECF4FE+
|[parameter]+tableBorderWidth+|+1px+
|[parameter]+tableHeaderTextColor+|+#0B356C+
|[parameter]+trimColor+|+#D6E6FB+
|[parameter]+tipBackgroundColor+|+#FAE6B0+
|[parameter]+tipBorderColor+|+#E5973E+
|[parameter]+selectControlColor+|+#E79A00+
|[parameter]+generalLinkColor+|+#0078D0+
|[parameter]+hoverLinkColor+|+#0090FF+
|[parameter]+visitedLinkColor+|+#0090FF+
|[parameter]+headerSizeFont+|+11px+
|[parameter]+headerFamilyFont+|+Arial, Verdana, sans-serif+
|[parameter]+tabSizeFont+|+11px+
|[parameter]+tabFamilyFont+|+Arial, Verdana, sans-serif+
|[parameter]+buttonSizeFont+|+11px+
|[parameter]+buttonFamilyFont+|+Arial, Verdana, sans-serif+
|[parameter]+tableBackgroundColor+|+#FFFFFF+
|[parameter]+tableFooterBackgroundColor+|+#CCCCCC+
|[parameter]+tableSubfooterBackgroundColor+|+#F1F1F1+
|[parameter]+tableBorderColor+|+#C0C0C0+
|[parameter]+warningColor+|+#FFE6E6+
|[parameter]+warningBackgroundColor+|+#FF0000+
|===============

[[sect-Rsyslog_Guide-Skinning_and_theming-Round_corners]]

==== Support for round corners

Support for round borders in your skins is available via the +panelBorderRadius+ skin parameter. The value of this parameter maps to the CSS 3 +border-radius+ property. This CSS 3 property is ignored in older browsers, and the skin gracefully degrades to square corners.

Units of the +panelBorderRadius+ skin parameter must be either +px+ (pixels). or +%+ (a percentage).

[[sect-Rsyslog_Guide-Skinning_and_theming-ECSS_files]]

==== [acronym]#ECSS# files

RichFaces uses [acronym]#ECSS# files to add extra functionality to the skinning process. ECSS files are CSS files which use Expression Language ([acronym]#EL#) to connect styles with skin properties.

[[exam-Rsyslog_Guide-Skinning_and_theming-ECSS_style_mappings]]

.ECSS style mappings
====

The ECSS code for the [sgmltag]+<r:panel>+ component contains styles for the panel and its body:


[source, Java]
----
.rf-p{
   background-color:'#{richSkin.generalBackgroundColor}';
   color:'#{richSkin.panelBorderColor}';
   border-width:1px;
   border-style:solid;
   padding:1px;
}

.rf-p-b{
   font-size:'#{richSkin.generalSizeFont}';
   color:'#{richSkin.generalTextColor}';
   font-family:'#{richSkin.generalFamilyFont}';
   padding:10px;
}

----

[classname]+.rf-p+  defines the panel styles::
+
* The [property]+background-color+ CSS property maps to the [parameter]+generalBackgroundColor+ skin parameter.
* The [property]+color+ CSS property maps to the [parameter]+panelBorderColor+ skin parameter.


[classname]+.rf-p-b+  defines the panel body styles::
+
* The [property]+font-family+ CSS property maps to the [parameter]+generalFamilyFont+ skin parameter.
* The [property]+font-size+ CSS property maps to the [parameter]+generalSizeFont+ skin parameter.
* The [property]+color+ CSS property maps to the [parameter]+generalTextColor+ skin parameter.


====

[[sect-Rsyslog_Guide-Skinning_and_theming-Customizing_skins]]

=== Customizing skins

Skins in RichFaces can be customized on each of the three levels:

Skin property files:: Application interfaces can be modified by altering the values of skin parameters in the skin itself. Edit the constant values defined in the [filename]+skin.properties+ file to change the style of every component mapped to that skin property.
Component stylesheets:: Mappings and other style attributes listed in a component's ECSS file can be edited. Edit the ECSS file to change the styles of all components of that type.
Custom components style classes:: Individual components can use the [varname]+styleClass+ attribute to use a unique style class. Add the new style class to the application CSS and reference it from an individual component with the [varname]+styleClass+ attribute.
Overwriting stylesheets in application:: You can load custom stylesheets using [code]+<h:outputStylesheet>+ which rewrites of extends styles defined for style classes of components.
[NOTE]
.Customizing skins by rewriting/extending component style classes
====
If you want to extend/overwrite style sheet definitions with own stylesheets, make sure you place definitions to be rendered in right order of occurence (see <<sect-Rsyslog_Guide-RichFaces_overview-Restrictions,Restrictions>> section for details).
====

[[exam-Rsyslog_Guide-Using_skins-Simple_skinning_example]]

.Simple skinning example
====

Using any component, such as a panel, without specifying a [varname]+styleClass+ will use the default skin parameters for that component.


[source, XML]
----
<r:panel>This is a panel without a header</r:panel>
----

When rendered for display, the panel consists of two HTML elements: a wrapper [sgmltag]+<div>+ element and a [sgmltag]+<div>+ element for the body of the panel. The wrapper element for a panel without a specified [varname]+styleClass+ is rendered as follows:


[source, XML]
----
<div id="..." class="rf-p">
   <div id="..." class="rf-p-b">
      This is a panel without a header
   </div>
</div>

----

To customize the panel appearance according to the three-level scheme, adjust the styles according to the following approach:

. Change the definitions for the [parameter]+generalBackgroundColor+ or [parameter]+panelBorderColor+ parameters in the skin. This will cause all panels in the application to change to the new settings.

. Redefine the [classname]+rf-p+ class in the application CSS. This will also cause all panels in the application to change to the new settings, though the skin itself has not been altered. Any properties not mapped to skin parameters should be redefined in this way.

. Specify a different [varname]+styleClass+ attribute to style the individual component. If a [varname]+styleClass+ attribute is used, the specified style class is applied to the component, which could extend or override the default styles.
+
--
[source, XML]
----
<r:panel styleClass="customClass">...</r:panel>
----

The +customClass+ style is added to the CSS, and is applied to the component when it is rendered for display:

[source, XML]
----
<div class="rf-p customClass">
   ...
</div>
----
--

====

[[sect-Rsyslog_Guide-Customizing_skins-Creating_a_new_skin]]

==== Creating a new skin

. *Create the skin file*
+
The name of the skin file should follow the format [filename]+$$new_skin_name.skin.properties$$+ and is placed in either the [filename]+META-INF/skins/+ directory or the classpath directory of your project.

. *Define the skin constants*
+
* *Define all the skin constants*
+
Add skin parameter constants and values to the file. All the skin parameters listed in <<tabl-Rsyslog_Guide-Skin_parameter_tables_in_RichFaces-Parameter_settings_for_the_blueSky_skin>> should be included in the skin file, with settings relevant to your new skin.
+
[[exam-Rsyslog_Guide-Skinning_and_theming-blueSky.skin.properties_file]]
.[filename]+blueSky.skin.properties+ file
====

Open the [filename]+blueSky.skin.properties+ file from the [filename]+/META-INF/skins+ directory in the richfaces-impl-_++version++_.jarpackage. The file lists all the skin parameter constants shown in <<tabl-Rsyslog_Guide-Skin_parameter_tables_in_RichFaces-Parameter_settings_for_the_blueSky_skin>>.

You can use the [filename]+blueSky.skin.properties+ file as a template for your new skin.

====
+
* *Extend a base skin*
+
Instead of redefining an entire new skin, your skin can use an existing skin as a base on which to build new parameters. Specify a base skin by using the [varname]+baseSkin+ parameter in the skin file, as shown in <<exam-Rsyslog_Guide-Skinning_and_theming-Using_a_base_skin>>.
+
[[exam-Rsyslog_Guide-Skinning_and_theming-Using_a_base_skin]]
.Using a base skin
====

This example takes the +blueSky+ skin as a base and only changes the [varname]+generalSizeFont+ parameter.

----
baseSkin=blueSky
generalSizeFont=12pt
----

====

. *Reference the skin definition*
+
Add a skin definition [sgmltag]+<context-param>+ to the [filename]+web.xml+ settings file of your application:
+
[source, XML]
----
<context-param>
   <param-name>org.richfaces.skin</param-name>
   <param-value>new_skin_name</param-value>
</context-param>
----

[[sect-Rsyslog_Guide-Skinning_and_theming-Changing_skins_at_runtime]]

=== Changing skins at runtime

To allow users to change skins at runtime, use a managed bean to access the skin.

. *Create the skin bean*
+
The skin bean is a simple interface to manage the skin:
+
[source, Java]
----
public class SkinBean {

    private String skin;

    public String getSkin() {
        return skin;
    }
    public void setSkin(String skin) {
        this.skin = skin;
    }
}

----

. *Reference the skin bean*
+
Add the [code]+@ManagedBean+ and [code]+@SessionScoped+ references to the class.
+
* Alternatively, use [acronym]#EL# (Expression Language) to reference the skin bean from the [filename]+web.xml+ settings file.
+
[source, XML]
----
<context-param>
    <param-name>org.richfaces.skin</param-name>
    <param-value>#{skinBean.skin}</param-value>
</context-param>
----

. *Set initial skin*
+
The application needs an initial skin to display before the user chooses an alternative skin. Specify the skin in your class with [code]+@ManagedProperty+.
+
[source, Java]
----
@ManagedProperty(value="blueSky")
private String skin;
----
+
* Alternatively, specify the initial skin in the [filename]+web.xml+ configuration file.
+
[source, XML]
----
<managed-bean>
    <managed-bean-name>skinBean</managed-bean-name>
    <managed-bean-class>SkinBean</managed-bean-class>
    <managed-bean-scope>session</managed-bean-scope>
    <managed-property>
        <property-name>skin</property-name>
        <value>blueSky</value>
    </managed-property>
</managed-bean>
----

[[sect-Rsyslog_Guide-Skinning_and_theming-Skinning_standard_controls]]

=== Skinning standard controls

Standard HTML controls used alongside RichFaces components are also themed to create a cohesive user interface.

[[sect-Rsyslog_Guide-Skinning_standard_controls-Automatic_skinning]]

==== Automatic skinning

The skinning style properties are automatically applied to controls based on their element names and attribute types. If the HTML elements are referenced in the standard skin stylesheets, the controls will be styled according to the mapped skin properties.

Standard HTML controls are skinned in this way by default. To override this behavior and prevent the RichFaces skins from being applied to the standard HTML controls, set the +org.richfaces.enableControlSkinning+ context parameter in the [filename]+web.xml+ configuration file to +false+:


[source, XML]
----
<context-param>
   <param-name>org.richfaces.enableControlSkinning</param-name>
   <param-value>false</param-value>
</context-param>
----

[[sect-Rsyslog_Guide-Skinning_standard_controls-Skinning_with_the_rfs-ctn_class]]

==== Skinning with the [classname]+rfs-ctn+ class

The skinning style properties can be determined through a separate CSS class. This method is not available by default, but is enabled through the +org.richfaces.enableControlSkinningClasses+ context parameter in the [filename]+web.xml+ configuration file:


[source, XML]
----
<context-param>
   <param-name>org.richfaces.enableControlSkinningClasses</param-name>
   <param-value>true</param-value>
</context-param>
----

When enabled, a stylesheet with predefined classes offers a special CSS class named [classname]+rfs-ctn+. Reference the [classname]+rfs-ctn+ class from any container element (such as a [sgmltag]+<div>+ element) to skin all the standard HTML controls in the container.

Standard HTML controls can also be specifically defined in the CSS. Refer to the [filename]+$$/core/impl/src/main/resources/META-INF/resources/skinning_both.ecss$$+ file in the richfaces-ui.jarpackage for examples of specially-defined CSS classes with skin parameters for HTML controls.

[[appe-Rsyslog_Guide-Style_classes_and_skin_parameters]]


[appendix]
== Style classes and skin parameters

Each of the [productname]#RichFaces# components are listed below, along with their style classes and skin parameters. For further details on each component, refer to the relevant section in the *_RichFaces Component Reference_* .

=== Processing management

==== +<r:log>+

include::skinning/log.asciidoc[]

=== Rich inputs

==== +<r:autocomplete>+

include::skinning/autocomplete.asciidoc[]

==== +<r:calendar>+

include::skinning/calendar.asciidoc[]

==== +<r:editor>+

include::skinning/editor.asciidoc[]

==== +<r:fileUpload>+

include::skinning/fileUpload.asciidoc[]

==== +<r:inplaceInput>+

include::skinning/inplaceInput.asciidoc[]

==== +<r:inputNumberSlider>+

include::skinning/inputNumberSlider.asciidoc[]

==== +<r:inputNumberSpinner>+

include::skinning/inputNumberSpinner.asciidoc[]

=== Rich selects

==== +<r:inplaceSelect>+

include::skinning/inplaceSelect.asciidoc[]

==== +<r:select>+

include::skinning/select.asciidoc[]

==== +<r:orderingList>+

include::skinning/orderingList.asciidoc[]

==== +<r:pickList>+

include::skinning/pickList.asciidoc[]

=== Panels and containers

==== +<r:panel>+

include::skinning/panel.asciidoc[]

==== +<r:accordion>+

include::skinning/accordion.asciidoc[]

==== +<r:collapsiblePanel>+

include::skinning/collapsiblePanel.asciidoc[]

==== +<r:popupPanel>+

include::skinning/popupPanel.asciidoc[]

==== +<r:tabPanel>+

include::skinning/tabPanel.asciidoc[]

=== Tables and grids

==== +<r:dataTable>+

include::skinning/dataTable.asciidoc[]

==== +<r:collapsibleSubTable>+

include::skinning/collapsibleSubTable.asciidoc[]

==== +<r:collapsibleSubTableToggler>+

include::skinning/collapsibleSubTableToggler.asciidoc[]

==== +<r:extendedDataTable>+

include::skinning/extendedDataTable.asciidoc[]

==== +<r:dataGrid>+

include::skinning/dataGrid.asciidoc[]

==== +<r:list>+

include::skinning/list.asciidoc[]

==== +<r:dataScroller>+

include::skinning/dataScroller.asciidoc[]

=== Trees

==== +<r:tree>+

include::skinning/tree.asciidoc[]

==== +<r:treeNode>+

include::skinning/treeNode.asciidoc[]

=== Menus and toolbars

==== +<r:dropDownMenu>+

include::skinning/dropDownMenu.asciidoc[]

==== +<r:contextMenu>+

include::skinning/contextMenu.asciidoc[]

==== +<r:panelMenu>+

include::skinning/panelMenu.asciidoc[]

==== +<r:toolbar>+

include::skinning/toolbar.asciidoc[]

=== Output and messages

==== +<r:message>+

include::skinning/message.asciidoc[]

==== +<r:messages>+

include::skinning/messages.asciidoc[]

==== +<r:notify>+

include::skinning/notify.asciidoc[]

==== +<r:notifyMessage>+

include::skinning/notifyMessage.asciidoc[]

==== +<r:notifyStack>+

include::skinning/notifyStack.asciidoc[]

==== +<r:progressBar>+

include::skinning/progressBar.asciidoc[]

==== +<r:tooltip>+

include::skinning/tooltip.asciidoc[]

=== Drag and drop

==== +<r:dropTarget>+

include::skinning/dropTarget.asciidoc[]

==== +<r:dragIndicator>+

include::skinning/dragIndicator.asciidoc[]

[[appe-Rsyslog_Guide-Migration_Notes]]

[appendix]
== Migration Notes

This section of the guide will track any breaking changes introduced in new releases, and identify any steps required to accommodate those changes in your application.

[[appe-Rsyslog_Guide-Migration_Notes-4_3_0]]

=== RichFaces 4.3.0.Final

[[appe-Rsyslog_Guide-Migration_Notes-4_3_0-built_in_sorting_and_filtering_controls]]

==== Built-in sorting and filtering controls

The [sgmltag]+<extendedDataTable>+ now has built-in sorting and filtering controls. If you have existing [sgmltag]+<extendedDataTable>+ with custom sort and/or filter controls, you will want to disable the built-in sort and/or filter controls. This can be done either on a column-by-column basis, or for all columns in your applications.

For details on disabling the built-in sort and filter controls, refer to sections "External filter controls" and "External sort controls" in the RichFaces Component Reference.

[[appe-Rsyslog_Guide-Migration_Notes-4_3_0-NotifyMessage_Sting_Escaping]]

==== NotifyMessage string escaping

Prior to version 4.3.0.Final, the message summary and details of the [sgmltag]+<r:notifyMessage>+ and [sgmltag]+<r:notifyMessages>+ components were not escaped. In the 4.3.0.Final release, an attribute [varname]+escape+ was added with a default value true.

[[appe-Rsyslog_Guide-Migration_Notes-4_3_0-Select_Input_Validation]]

==== Select input validation

The [sgmltag]+<r:select>+ now validates that manually entered input values match one of the values of the provided list (including support for client-side validation).



[[objectmodel]]
== rsyslog object model and message flow

http://blog.gerhards.net/2007/08/rsyslog-v3-object-model-and-message.html[Article Source]

To understand the Rsyslog's config file formats, one needs to know more 
about its object model.  

Besides the actual object model, it will help a great deal in understanding 
what needs to be in the config file Thus it's currently a bit biased towards
configurable objects and the internal objects are almost completely missing.


[[Rsyslog_Object]]
=== rsyslog Objects

Rsyslog uses objects, even though it is written in C. This can quite well be 
done. Only at some points (like inheritance) we need to fiddle a bit with the 
language. But nothing to hard to be done. We try to keep object overhead very 
low, so it should be more like traditional C than C++.


[[|Rsyslog_object_Module]]
==== Module

The module object is the base class for anything that even remotely looks 
like a loadable object.  In the long term, that might eventually be almost 
everything.  

Module object handles the necessary plumbing like loading modules, keeping 
track of module status, querying interface and all those things...  
It does not by itself provide any module-specific actions.


[[|Rsyslog_object_Input]]
==== Input

An input gatheres messages (events) from external sources. Current typical 
examples are UDP or TCP based syslog. However, there is no architectural limit, 
so in the long term an input module may also gather SNMP traps or file lines. 
Please note that an input does not necessarily parse the obtained event by 
itself - this may be delegated to a parser module (this whole thing still needs 
to be thought out).


[[|Rsyslog_object_Output]]
==== Output
An output module receives strings from the engine and writes them to some 
ultimate destination. Popluar examples are files, databases or remote syslog 
servers.

[[|Rsyslog_object_Action]]
==== Action
The action object is the "engine wrapper" around an output module. It 
provides numerous servies to the output. Most importantly, it provides the core 
plumbing behind restarting and queueing actions.

[[|Rsyslog_object_Ruleset]]
==== Ruleset
http://www.rsyslog.com/doc/multi_ruleset.html[See this article.]

[[|Rsyslog_object_Function]]
==== Function
A function object (and lodable module) provides extensibility for internal 
processing.  Version 3.0 will support programming-like functions, which replace 
and extend the property replacer options.  For example, we may want to extract 
characters 5 to 10 and convert them to lower case.  With current rsyslog, this 
works as follows: `%msg:5:10:lower%`.  With Version 3, it will look: 
`lower(substr(%msg, 5, 10))`. 

Functions will be supported everywhere a string is supported, which means 
everywhere.  Examples are filters, output format (templates), and file name 
generation.


=== Message Flow

This is a brief description of how a message (an event) flows inside rsyslog.

The message text is received by an input module.  

The input module, possibly with the help of a parser module, transforms the 
message text to an in-memory structured message.  

This structured message enters rsyslog's main processing queue.  

As soon as  possible, the msg object is dequeued.  

The dequeued message is then processed by a ruleset.

A rule set object contains multiple rule object, which are executed in order 
until either all rules are processed or a discard action is encountered. 

If there are multiple ruleset objects bound to a single input module, they are 
also executed in the configured order. The normal discard action will discard a 
message for one ruleset, but an additional rule set will  continue to 
process the msg object.  A special case of the discard action can be
envisioned, which discards a msg object for all bound rule sets. 

Each rule object contains 
filter objects and action objects.  First, all filter objects are executed. That 
means, their expressions are evaluated.  If there are multiple filter objects, 
their result is combined in an AND operation (other modes are not supported - 
that could be easily done by crafting a specific filter expression, so we do not 
encourage an additional set of complexity be allowing additional boolean 
operators when multiple filter objects are used inside a single rule). 

If the outcome of the overall filter object evaluation results in true ("to be 
processed"), all action objects are called.  They are called in the order of 
their appearance in the config file (no exceptions here).  The msg object is 
used to generate the string required by the action (note that the msg object 
itself is NOT passed to the action, this is a security and encapsulation 
boundary). 

For  string generation, function plug-ins may possibly be called (this is also 
the case during filter procesing).  The resulting strings are  passed to the 
execute method of the action object.  That object decides if queing, 
rate-limiting, or other functionality generically available to all actions is 
to be carried out.  All of this is done by the action object itself. 

Finally, the  strings are passed from the action object to the actual output 
object (a plug-in). 

The output objct takes the strings and performs whatever processing needs to
be done. 

After all actions, rules, and rulesets have been processed (or when a 
discard action occurs), the msg object is destroyed. Please note that even then 
a copy may be held in memory, because that might be needed for duplicate
 message reduction and similiar features. Thus, msg is reference-counted and
actually only destroyed when the count reaches zero.

Please note that there are a number of utility objects involved.  For clarify, 
these have been omitted.  Also, all function calls return a meaningful return 
state. The caller will process that state with care. In some cases, however, 
this may mean ignore a failed call, because this is the most appropriate thing 
to do (e.g. when an action fails - if we'd abort processing the msg object, we'd 
do much more harm).


=== Data flow

This is a brief and not so accurate picture of the data flow.  It focuses
on the role of the plug-ins.

input module(parser) -> rule engine (custom functions) -> action (with 
aggregated output module)


=== Known Open Issues with Plug-ins

Probably the number one question is how message-modifying plug-ins can be 
created.  I think about things like implementing syslog-sign or TLS security.  
It is my current, unverified, view that we must have something like 
"filter-plug-ins" which can be placed inside the message data flow to modify the
message text and/or the msg object.



== Understanding rsyslog Queues

http://www.rsyslog.com/doc/queues.html[Article Source]

Rsyslog uses queues whenever two activities need to be loosely coupled.  With a
queue, one part of the system "produces" syslog messages while another part 
"consumes" the messages. 

The most prominent example is the main message queue. Whenever rsyslog receives 
a message (e.g. locally, via UDP, TCP or in whatever else way), it places these
messages into the main message queue.  Later, it is dequeued by the rule 
processor, which then evaluates which actions are to be carried out. In front of
each action, there is also a queue, which potentially de-couples the filter 
processing from the actual action (e.g. writing to file, database or forwarding 
to another host).

Currently, queues are used for the main message queue and for the actions.

There is a single main message queue inside rsyslog.  Each input module delivers
messages to it.  The main message queue worker filters messages based on rules 
specified in rsyslog.conf and dispatches them to the individual action queues.  
Once a message is in an action queue, it is deleted from the main message queue.

There are multiple action queues, one for each configured action.  Action queues
are fully configurable and thus can be changed to whatever is best for the given
use case.

Action queue parameters can not be  modified once the action has been specified.  
For example, to tell the main message queue to save its content on shutdown, 
use $MainMsgQueueSaveOnShutdown on".

The main message queue is created after parsing the config file and all of its potential 
includes.  An action queue is created each time an action selector is specified. 

Not all queues necessarily support the full set of queue configuration parameters, because 
not all are applicable.  For example, in current output module design, actions do not support 
multi-threading.  Consequently, the number of worker threads is fixed to one for action queues 
and can not be changed.


=== Queue Analogy

A warp-up: In rsyslog, an action queue "sits in front of" each output plugin. 
Messages are received and flow, from input to output, over various stages and two level of 
queues to the outputs.  Actions queues are always present, but may not easily be visible when 
in direct mode (where no actual queuing takes place). 

On the output side, the queue is the active component, not the consumer. As such, the consumer cannot ask the queue for anything (like n number of messages) but rather is activated by the queue itself. As such, a queue somewhat resembles a "living thing" whereas the outputs are just tools that this "living thing" uses.

Note that I left out a couple of subtleties, especially when it comes to error handling and terminating a queue (you hopefully have now at least a rough idea why I say "terminating a queue" and not "terminating an action" - who is the "living thing"?). An action returns a status to the queue, but it is the queue that ultimately decides which messages can finally be considered processed and which not. Please note that the queue may even cancel an output right in the middle of its action. This happens, if configured, if an output needs more than a configured maximum processing time and is a guard condition to prevent slow outputs from deferring a rsyslog restart for too long. Especially in this case re-queuing and cleanup is not trivial. Also, note that I did not discuss disk-assisted queue modes. The basic rules apply, but there are some additional constraints, especially in regard to the threading model. Transitioning between actual disk-assisted mode and pure-in-memory-mode (which is done automatically when needed) is also far from trivial and a real joy for an implementer to work on ;).

=== Worker Thread Pools

Each queue (except in "direct" mode) has an associated pool of worker threads.  Worker threads 
carry out the action to be performed on the data elements enqueued.  As an actual sample, the 
main message queue's worker task is to apply filter logic to each incoming message and enqueue 
them to the relevant output queues (actions).



=== Turning Lanes and Rsyslog Queues - an Analogy

If there is a single object absolutely vital to understanding the way rsyslog 
works, this object is queues.  Queues offer a variety of services, including 
support for multithreading.

While there is elaborate in-depth documentation on the ins and outs of rsyslog 
queues, some of the concepts are hard to grasp even for experienced people.  
I think this is because rsyslog uses a very high layer of abstraction which 
includes things that look quite unnatural, like queues that do not actually 
queue...

The graphic below describes the data flow inside rsyslog:

image::http://www.rsyslog.com/doc/dataflow.png[rsyslog data flow]

For our needs, the important fact to know is that messages enter rsyslog on 
"the left side" (for example, via UDP), are being preprocessed, put into the 
so-called main queue, taken off that queue, be filtered and be placed into one 
or several action queues (depending on filter results).  They leave rsyslog on 
"the right side" where output modules (like the file or database writer) consume 
them.

So there are always two stages where a message (conceptually) is queued - first in the 
main queue and later on in n action specific queues (with n being the number of actions 
that the message in question needs to be processed by, what is being decided by the 
"Filter Engine").  As such, a message will be in at least two queues during its lifetime
(with the exception of messages being discarded by the queue itself, but for the purpose 
of this document, we will ignore that possibility).

Also, it is vitally important to understand that each action has a queue sitting
in front of it.  If you have dug into the details of rsyslog configuration, you 
have probably seen that a queue mode can be set for each action. And the default 
queue mode is the so-called "direct mode", in which "the queue does not actually 
enqueue data". That sounds silly, but is not. It is an important abstraction 
that helps keep the code clean.

To understand this, we first need to look at who is the active component.  
In our data flow, the active part always sits to the left of the object.  For 
example, the "Preprocessor" is being called by the inputs and calls itself into 
the main message queue.  That is, the queue receiver is called, it is passive.  
One might think that the "Parser & Filter Engine" is an active component that 
actively pulls messages from the queue. This is wrong! Actually, it is the queue 
that has a pool of worker threads, and these workers pull data from the queue 
and then call the passively waiting Parser and Filter Engine with those 
messages.  So the main message queue is the active part, the Parser and Filter 
Engine are passive.


== Log Message Normalization Module (mmnormalize)

http://www.rsyslog.com/doc/mmnormalize.html[Article Source]

=== Description

This module provides the capability to normalize log messages via liblognorm.  
Thanks to liblognorm, unstructured text, like usually found in log messages, can very 
quickly be parsed and put into a normal form. This is done so quickly, that it should 
be possible to normalize events in realtime.

This module is implemented via the output module interface.  This means that mmnormalize 
should be called just like an action. After it has been called, the normalized message 
properties are available and can be accessed. These properties are called the "CEE/lumberjack" 
properties, because liblognorm creates a format that is inspired by the CEE/lumberjack approach.

Please note: CEE/lumberjack properties are different from regular properties.  They 
always have "$!" prepended to the property name given in the rulebase. Such a property 
needs to be called with %$!propertyname%.

Note that mmnormalize should only be called once on each message. Behaviour is undefined 
if multiple calls to mmnormalize happen for the same message.


=== Action Parameters

* ruleBase [word]
Specifies which rulebase file is to use. If there are multiple mmnormalize instances, each one 
can use a different file. However, a single instance can use only a single file. This parameter 
MUST be given, because normalization can only happen based on a rulebase. It is recommended that 
an absolute path name is given. Information on how to create the rulebase can be found in the
liblognorm manual.

* useRawMsg [boolean]
Specifies if the raw message should be used for normalization (on) or just the MSG part of 
the message (off). Default is "off".


=== Sample

This activates the module and applies normalization to all messages:


module(load="mmnormalize")
action(type="mmnormalize" ruleBase="/path/to/rulebase.rb")


== $RulesetCreateMainQueue

Type: ruleset-specific configuration directive +
Parameter Values: boolean (on/off, yes/no) + 
Default: off 

=== Description

Rulesets may use their own "main" message queue for message submission. 
Specifying this directive, inside a ruleset definition, turns this on. This is 
both a performance enhancement and also permits different rulesets (and thus 
different inputs within the same rsyslogd instance) to use different types of 
main message queues.

The ruleset queue is created with the parameters that are specified for the main
message queue at the time the directive is given. If different queue 
configurations are desired, different main message queue directives must be used
in front of the $RulesetCreateMainQueue directive. Note that this directive may 
only be given once per ruleset. If multiple statements are specified, only the 
first is used and for the others error messages are emitted.

Note that the final set of ruleset configuration directives specifies the 
parameters for the default main message queue.

To learn more about this feature, please be sure to read about multi-ruleset 
support in rsyslog.

=== Caveats

The configuration statement "$RulesetCreateMainQueue off" has no effect at all. 
The capability to specify this is an artifact of the current (ugly!) 
configuration language.

=== Example

This example sets up a tcp server with three listeners. Each of these three 
listener is bound to a specific ruleset. As a performance optimization, the 
rulesets all receive their own private queue. The result is that received 
messages can be independently processed. With only a single main message queue, 
we would have some lock contention between the messages. This does not happen 
here. Note that in this example, we use different processing. Of course, all 
messages could also have been processed in the same way ($IncludeConfig may be
useful in that case!).

[source]
----
$ModLoad imtcp

# First, this is a copy of the unmodified rsyslog.conf
#define rulesets first

$RuleSet remote10514
$RulesetCreateMainQueue on # create ruleset-specific queue
*.*     /var/log/remote10514

$RuleSet remote10515
$RulesetCreateMainQueue on # create ruleset-specific queue
*.*     /var/log/remote10515

$RuleSet remote10516
$RulesetCreateMainQueue on # create ruleset-specific queue
mail.*  /var/log/mail10516
&       ~
# note that the discard-action will prevent this messag from 
# being written to the remote10516 file - as usual...
*.*     /var/log/remote10516

# and now define listeners bound to the relevant ruleset

$InputTCPServerBindRuleset remote10514
$InputTCPServerRun 10514

$InputTCPServerBindRuleset remote10515
$InputTCPServerRun 10515

$InputTCPServerBindRuleset remote10516
$InputTCPServerRun 10516
----

=== Q & A

Q: Is there a way to specify that a ruleset should have it's own main queue
with the new ruleset() {...} statement?

A: Yes, you can do it by specifying the parameters inside ruleset(); for
example:

[source]
----
ruleset(
    name="collectors" 
    queue.type="linkedlist" 
    queue.size="50000") { ...
}
----

specifying any queue parameters in the ruleset definition will as a side effect 
perform the equivalent of a `$RulesetCreateMainQueue on`


== $RulesetParser

Type: ruleset-specific configuration directive

Parameter Values: string

Available since: 5.3.4+

Default: rsyslog.rfc5424 followed by rsyslog.rfc3164

=== Description

This directive permits to specify which message parsers should be used for the 
ruleset in question. It no ruleset is explicitely specified, the default ruleset
is used. Message parsers are contained in (loadable) parser modules with the 
most common cases (RFC3164 and RFC5424) being build-in into rsyslogd.

When this directive is specified the first time for a ruleset, it will not only add the parser to the ruleset's parser chain, it will also wipe out the default parser chain. So if you need to have them in addition to the custom parser, you need to specify those as well.

Order of directives is important. Parsers are tried one after another, in the order they are specified inside the config. As soon as a parser is able to parse the message, it will do so and no other parsers will be executed. If no matching parser can be found, the message will be discarded and a warning message be issued (but only for the first 1,000 instances of this problem, to prevent message generation loops).

Note that the rfc3164 parser will always be able to parse a message - it may just not be the format that you like. This has two important implications: 1) always place that parser at the END of the parser list, or the other parsers after it will never be tried and 2) if you would like to make sure no message is lost, placing the rfc3164 parser at the end of the parser list ensures that.

Multiple parser modules are very useful if you have various devices that emit messages that are malformed in various ways. The route to take then is

make sure you find a custom parser for that device; if there is no one, you may consider writing one yourself (it is not that hard) or getting one written as part of Adiscon's professional services for rsyslog.
load your custom parsers via $ModLoad
create a ruleset for each malformed format; assign the custom parser to it
create a specific listening port for all devices that emit the same malformed format
bind the listener to the ruleset with the required parser
Note that it may be cumbersome to add all rules to all rulesets. To avoid this, you can either use $Include or omruleset (what probably provides the best solution).

More information about rulesets in general can be found in multi-ruleset support in rsyslog.

=== Example

This example assumes there are two devices emiting malformed messages via UDP. We have two custom parsers for them, named "device1.parser" and "device2.parser". In addition to that, we have a number of other devices sending wellformed messages, also via UDP.

The solution is to listen for data from the two devices on two special ports (10514 and 10515 in this example), create a ruleset for each and assign the custom parsers to them. The rest of the messages are received via port 514 using the regular parsers. Processing shall be equal for all messages. So we simply forward the malformed messages to the regular queue once they are parsed (keep in mind that a message is never again parsed once any parser properly processed it).

[source]
----
$ModLoad imudp
$ModLoad pmdevice1 # load parser "device1.parser" for device 1
$ModLoad pmdevice2 # load parser "device2.parser" for device 2

# define ruleset for the first device sending malformed data
$Ruleset maldev1
$RulesetCreateMainQueue on # create ruleset-specific queue
$RulesetParser "device1.parser" # note: this deactivates the default parsers
# forward all messages to default ruleset:
$ActionOmrulesetRulesetName RSYSLOG_DefaultRuleset 
*.* :omruleset:

# define ruleset for the second device sending malformed data
$Ruleset maldev2
$RulesetCreateMainQueue on # create ruleset-specific queue
$RulesetParser "device2.parser" # note: this deactivates the default parsers
# forward all messages to default ruleset:
$ActionOmrulesetRulesetName RSYSLOG_DefaultRuleset 
*.* :omruleset:

# switch back to default ruleset
$Ruleset RSYSLOG_DefaultRuleset
*.*           /path/to/file
auth.info     @authlogger.example.net
# whatever else you usually do...


# now define the inputs and bind them to the rulesets
# first the default listener (utilizing the default ruleset)
$UDPServerRun 514

# now the one with the parser for device type 1:
$InputUDPServerBindRuleset maldev1
$UDPServerRun 10514

# and finally the one for device type 2:
$InputUDPServerBindRuleset maldev2
$UDPServerRun 10515
----
For an example of how multiple parser can be chained (and an actual use case), 
please see the example section on the pmlastmsg parser module.

Note the positions of the directives. With the current config language, 
sequence of statements is very important. This is ugly, but unfortunately the 
way it currently works.








[[actionlegaccy]]
== Action Statement ==
        
Action object describe what is to be done with a message.  They are implemented 
via <a href="rsyslog_conf_modules.html#om">outpout modules</a>.

The action object has different parameters:
* those that apply to all actions and are action specific.     
    These are documented below.
* parameters for the action queue.     
    While they also apply to all parameters, they are queue-specific, 
    not action-specific (e.g. they are the same that are used in rulesets).
* action-specific parameters.     
    These are specific to a certain type of actions. 
    They are documented by the output module in question.

## Legacy Format ##

Be warned that legacy action format is hard to get right. It is
recommended to use RainerScript-Style action format whenever possible!

A key problem with legacy format is that a single action is defined via
multiple configurations lines, which may be spread all across rsyslog.conf.
Even the definition of multiple actions may be intermixed (often not
intentional!). If legacy actions format needs to be used (e.g. some modules
may not yet implement the RainerScript format), it is strongly recommended
to place all configuration statements pertaining to a single action
closely together.

Please also note that legacy action parameters **do not** affect
RainerScript action objects. So if you define for example:

    $actionResumeRetryCount 10
    action(type="omfwd" target="server1.example.net")
    @@server2.example.net

server1's "action.resumeRetryCount" parameter is **not** set, instead server2's is!

A goal of the new RainerScript action format was to avoid confusion
which parameters are actually used. As such, it would be counter-productive
to honor legacy action parameters inside a RainerScript definition. As 
result, both types of action definitions are strictly (and nicely)
separated from each other. The bottom line is that if RainerScript actions
are used, one does not need to care about which legacy action parameters may 
(still...) be in effect.

Note that not all modules necessarily support legacy action format.
Especially newer modules are recommended to NOT support it.

### Legacy Description ###

Templates can be used with many actions. If used, the specified template
is used to generate the message content (instead of the default
template). To specify a template, write a semicolon after the action
value immediately followed by the template name.    
    
Beware: templates MUST be defined BEFORE they are used. It is OK to
define some templates, then use them in selector lines, define more
templates and use use them in the following selector lines. But it is
NOT permitted to use a template in a selector line that is above its
definition. If you do this, the action will be ignored.

**You can have multiple actions for a single selector ** (or
more precisely a single filter of such a selector line). Each action
must be on its own line and the line must start with an ampersand
('&amp;') character and have no filters. An example would be

    *.=crit :omusrmsg:rger
    &root
    & /var/log/critmsgs</b></code></p>

These three lines send critical messages to the user rger and
root and also store them in /var/log/critmsgs.  **Using multiple
actions per selector is** convenient and also **offers
a performance benefit**.  As the filter needs to be evaluated
only once, there is less computation required to process the directive
compared to the otherwise-equal config directives below:

    *.=crit :omusrmsg:rger
    *.=crit root
    *.=crit /var/log/critmsgs


### Regular File ###

<p>Typically messages are logged to real files. The file usually is
specified by full pathname, beginning with a slash "/".
Starting with version 4.6.2 and 5.4.1 (previous v5 version do NOT support this)
relative file names can also be specified.  To do so, these must begin with a
dot. For example, use "./file-in-current-dir.log" to specify a file in the
current directory. Please note that rsyslogd usually changes its working 
directory to the root, so relative file names must be tested with care (they
were introduced primarily as a debugging vehicle, but may have useful other applications
as well).    
    
    
You may prefix each entry with the minus "-'' sign to omit syncing the
file after every logging. Note that you might lose information if the
system crashes right behind a write attempt. Nevertheless this might
give you back some performance, especially if you run programs that use
logging in a very verbose manner.

<p>If your system is connected to a reliable UPS and you receive
lots of log data (e.g. firewall logs), it might be a very good idea to
turn of
syncing by specifying the "-" in front of the file name.

**The filename can be either static **(always
the same) or <b>dynamic</b> (different based on message
received). The later is useful if you would automatically split
messages into different files based on some message criteria. For
example, dynamic file name selectors allow you to split messages into
different files based on the host that sent them. With dynamic file
names, everything is automatic and you do not need any filters. </p>
<p>It works via the template system. First, you define a template
for the file name. An example can be seen above in the description of
template. We will use the "DynFile" template defined there. Dynamic
filenames are indicated by specifying a questions mark "?" instead of a
slash, followed by the template name. Thus, the selector line for our
dynamic file name would look as follows:</p>
<blockquote>
<code>*.* ?DynFile</code>
</blockquote>
<p>That's all you need to do. Rsyslog will now automatically
generate file names for you and store the right messages into the right
files. Please note that the minus sign also works with dynamic file
name selectors. Thus, to avoid syncing, you may use</p>
<blockquote>
<code>*.* -?DynFile</code></blockquote>
<p>And of course you can use templates to specify the output
format:</p>
<blockquote>
<code>*.* ?DynFile;MyTemplate</code></blockquote>
<p><b>A word of caution:</b> rsyslog creates files as
needed. So if a new host is using your syslog server, rsyslog will
automatically create a new file for it.</p>
<p><b>Creating directories is also supported</b>. For
example you can use the hostname as directory and the program name as
file name:</p>
<blockquote>
<code>$template DynFile,"/var/log/%HOSTNAME%/%programname%.log"</code></blockquote>

### Named Pipes ###

<p>This version of rsyslogd(8) has support for logging output to
named pipes (fifos). A fifo or named pipe can be used as a destination
for log messages by prepending a pipe symbol ("|'') to the name of the
file. This is handy for debugging. Note that the fifo must be created
with the mkfifo(1) command before rsyslogd(8) is started.</p>

### Terminal and Console ###

If the file you specified is a tty, special tty-handling is
done, same with /dev/console.

### Remote Machine ###

Rsyslogd provides full remote logging, i.e. is able to send
messages to a remote host running rsyslogd(8) and to receive messages
from remote hosts. Using this feature you're able to control all syslog
messages on one host, if all other machines will log remotely to that.
This tears down administration needs.

To forward messages to another host, prepend the hostname with
the at sign ("@"). A single at sign means that messages will
be forwarded via UDP protocol (the standard for syslog). If you prepend
two at signs ("@@"), the messages will be transmitted via TCP. Please
note that plain TCP based syslog is not officially standardized, but
most major syslogds support it (e.g. syslog-ng or
<a href="http://www.winsyslog.com/">WinSyslog</a>). The
forwarding action indicator (at-sign) can be followed by one or more
options. If they are given, they must be immediately (without a space)
following the final at sign and be enclosed in parenthesis. The
individual options must be separated by commas. The following options
are right now defined:

<table id="table2" border="1" width="100%">
<tbody>
<tr>
<td>
<p align="center"><b>z&lt;number&gt;</b></p>
</td>
<td>Enable zlib-compression for the message. The
&lt;number&gt; is the compression level. It can be 1 (lowest
gain, lowest CPU overhead) to 9 (maximum compression, highest CPU
overhead). The level can also be 0, which means "no compression". If
given, the "z" option is ignored. So this does not make an awful lot of
sense. There is hardly a difference between level 1 and 9 for typical
syslog messages. You can expect a compression gain between 0% and 30%
for typical messages. Very chatty messages may compress up to 50%, but
this is seldom seen with typically traffic. Please note that rsyslogd
checks the compression gain. Messages with 60 bytes or less will never
be compressed. This is because compression gain is pretty unlikely and
we prefer to save CPU cycles. Messages over that size are always
compressed. However, it is checked if there is a gain in compression
and only if there is, the compressed message is transmitted. Otherwise,
the uncompressed messages is transmitted. This saves the receiver CPU
cycles for decompression. It also prevents small message to actually
become larger in compressed form.

<p><b>Please note that when a TCP transport is used,
compression will also turn on syslog-transport-tls framing. See the "o"
option for important information on the implications.</b></p>
<p>Compressed messages are automatically detected and
decompressed by the receiver. There is nothing that needs to be
configured on the receiver side.</p>
</td>
</tr>
<tr>
<td>
<p align="center"><b>o</b></p>
</td>
<td><b>This option is experimental. Use at your own
risk and only if you know why you need it! If in doubt, do NOT turn it
on.</b>
<p>This option is only valid for plain TCP based
transports. It selects a different framing based on IETF internet draft
syslog-transport-tls-06. This framing offers some benefits over
traditional LF-based framing. However, the standardization effort is
not yet complete. There may be changes in upcoming versions of this
standard. Rsyslog will be kept in line with the standard. There is some
chance that upcoming changes will be incompatible to the current
specification. In this case, all systems using -transport-tls framing
must be upgraded. There will be no effort made to retain compatibility
between different versions of rsyslog. The primary reason for that is
that it seems technically impossible to provide compatibility between
some of those changes. So you should take this note very serious. It is
not something we do not *like* to do (and may change our mind if enough
people beg...), it is something we most probably *can not* do for
technical reasons (aka: you can beg as much as you like, it won't
change anything...).</p>
<p>The most important implication is that compressed syslog
messages via TCP must be considered with care. Unfortunately, it is
technically impossible to transfer compressed records over traditional
syslog plain tcp transports, so you are left with two evil choices...</p>
</td>
</tr>
</tbody>
</table>
<p><br>
The hostname may be followed by a colon and the destination port.</p>
<p>The following is an example selector line with forwarding:</p>
<p>*.*&nbsp;&nbsp;&nbsp; @@(o,z9)192.168.0.1:1470</p>
<p>In this example, messages are forwarded via plain TCP with
experimental framing and maximum compression to the host 192.168.0.1 at
port 1470.</p>
<p>*.* @192.168.0.1</p>
<p>In the example above, messages are forwarded via UDP to the
machine 192.168.0.1, the destination port defaults to 514. Messages
will not be compressed.</p>
<p>Note that IPv6 addresses contain colons. So if an IPv6 address is specified
in the hostname part, rsyslogd could not detect where the IP address ends
and where the port starts. There is a syntax extension to support this:
put squary brackets around the address (e.g. "[2001::1]"). Square
brackets also work with real host names and IPv4 addresses, too.
</p><p>A valid sample to send messages to the IPv6 host 2001::1 at port 515
is as follows:
</p><p>*.* @[2001::1]:515
</p><p>This works with TCP, too.
</p><p><b>Note to sysklogd users:</b> sysklogd does <b>not</b>
support RFC 3164 format, which is the default forwarding template in
rsyslog. As such, you will experience duplicate hostnames if rsyslog is
the sender and sysklogd is the receiver. The fix is simple: you need to
use a different template. Use that one:</p>
<p class="MsoPlainText">$template
sysklogd,"&lt;%PRI%&gt;%TIMESTAMP% %syslogtag%%msg%\""<br>
*.* @192.168.0.1;sysklogd</p>

### List of Users ###

<p>Usually critical messages are also directed to "root'' on
that machine. You can specify a list of users that shall get the
message by simply writing ":omusrmsg: followed by the login name. For example,
the send messages to root, use ":omusrmsg:root".
You may specify more than one user
by separating them with commas (",''). Do not repeat the ":omusrmsg:" prefix in
this case. For example, to send data to users root and rger, use
":omusrmsg:root,rger" (do not use ":omusrmsg:root,:omusrmsg:rger", this is invalid).
If they're logged in they get
the message.

### Everyone logged on ###

Emergency messages often go to all users currently online to
notify them that something strange is happening with the system. To
specify this wall(1)-feature use an asterisk as the user message
destination(":omusrmsg:*'').

### Call Plugin ###
This is a generic way to call an output plugin. The plugin
must support this functionality. Actual parameters depend on the
module, so see the module's doc on what to supply. The general syntax
is as follows:

<p>:modname:params;template</p>

<p>Currently, the ommysql database output module supports this
syntax (in addtion to the "&gt;" syntax it traditionally
supported). For ommysql, the module name is "ommysql" and the params
are the traditional ones. The ;template part is not module specific, it
is generic rsyslog functionality available to all modules.</p>

<p>As an example, the ommysql module may be called as follows:</p>
<p>:ommysql:dbhost,dbname,dbuser,dbpassword;dbtemplate</p>

<p>For details, please see the "Database Table" section of this
documentation.</p>

Note: as of this writing, the ":modname:" part is hardcoded
into the module. So the name to use is not necessarily the name the
module's plugin file is called.

### Database Table ###

<p>This allows logging of the message to a database table.
Currently, only MySQL databases are supported. However, other database
drivers will most probably be developed as plugins. By default, a <a href="http://www.monitorware.com/">MonitorWare</a>-compatible
schema is required for this to work. You can create that schema with
the createDB.SQL file that came with the rsyslog package. You can also<br>
use any other schema of your liking - you just need to define a proper
template and assign this template to the action.<br>
<br>
The database writer is called by specifying a greater-then sign
("&gt;") in front of the database connect information. Immediately
after that<br>
sign the database host name must be given, a comma, the database name,
another comma, the database user, a comma and then the user's password.
If a specific template is to be used, a semicolon followed by the
template name can follow the connect information. This is as follows:<br>
<br>
&gt;dbhost,dbname,dbuser,dbpassword;dbtemplate</p>

**Important: to use the database functionality, the
MySQL output module must be loaded in the config file** BEFORE
the first database table action is used. This is done by placing the

    $ModLoad ommysql 

directive some place above the first use of the database write
(we recommend doing at the the beginning of the config file).

### Discard ###

If the discard action is carried out, the received message is
immediately discarded. No further processing of it occurs. Discard has
primarily been added to filter out messages before carrying on any
further processing. For obvious reasons, the results of "discard" are
depending on where in the configuration file it is being used. Please
note that once a message has been discarded there is no way to retrieve
it in later configuration file lines.

Discard can be highly effective if you want to filter out some
annoying messages that otherwise would fill your log files. To do that,
place the discard actions early in your log files. This often plays
well with property-based filters, giving you great freedom in
specifying what you do not want.

Discard is just the single tilde character with no further parameters:

<p>~</p>
<p>For example,</p>
<p>*.*&nbsp;&nbsp; ~</p>
<p>discards everything (ok, you can achive the same by not
running rsyslogd at all...).</p>

### Output Channel ###

Binds an output channel definition (see there for details) to
this action. Output channel actions must start with a $-sign, e.g. if
you would like to bind your output channel definition "mychannel" to
the action, use "$mychannel". Output channels support template
definitions like all all other actions.

### Shell Execute ###

This executes a program in a subshell. The program is passed
the template-generated message as the only command line parameter.
Rsyslog waits until the program terminates and only then continues to
run.

^program-to-execute;template

The program-to-execute can be any valid executable. It
receives the template string as a single parameter (argv[1]).

**WARNING:** The Shell Execute action was added to serve an urgent need. 
While it is considered reasonable save when
used with some thinking, its implications must be considered. The
current implementation uses a system() call to execute the command.
This is not the best way to do it (and will hopefully changed in
further releases). Also, proper escaping of special characters is done
to prevent command injection. However, attackers always find smart ways
to circumvent escaping, so we can not say if the escaping applied will
really safe you from all hassles. Lastly, rsyslog will wait until the
shell command terminates. Thus, a program error in it (e.g. an infinite
loop) can actually disable rsyslog. Even without that, during the
programs run-time no messages are processed by rsyslog. As the IP
stacks buffers are quickly overflowed, this bears an increased risk of
message loss. You must be aware of these implications. Even though they
are severe, there are several cases where the "shell execute" action is
very useful. This is the reason why we have included it in its current
form. To mitigate its risks, always a) test your program thoroughly, b)
make sure its runtime is as short as possible (if it requires a longer
run-time, you might want to spawn your own sub-shell asynchronously),
c) apply proper firewalling so that only known senders can send syslog
messages to rsyslog. Point c) is especially important: if rsyslog is
accepting message from any hosts, chances are much higher that an
attacker might try to exploit the "shell execute" action.

### Template Name ###

Every ACTION can be followed by a template name. If so, that
template is used for message formatting. If no name is given, a
hard-coded default template is used for the action. There can only be
one template name for each given action. The default template is
specific to each action. For a description of what a template is and
what you can do with it, see "TEMPLATES" at the top of this document.



[[actionstatement]]
== Action Statement
        
Action object describe what is to be done with a message. 
They are implemented via <a href="rsyslog_conf_modules.html#om">outpout modules</a>.

The action object has different parameters:
* those that apply to all actions and are action specific.     
    These are documented below.
* parameters for the action queue.     
    While they also apply to all parameters, they are queue-specific, not action-specific 
    (they are the same that are used in rulesets, for example).
* action-specific parameters.     
    These are specific to a certain type of actions. 
    They are documented by the output module in question.

### General Action Parameters ###

* **name**  word    
    used for statistics gathering and documentation    

* **type** string    
    Mandatory parameter for every action. The name of the module that should be used.    

* **action.writeAllMarkMessages** on/off    
    Normally, mark messages are written to actions only if the action was not recently executed 
    (by default, recently means within the past 20 minutes). If this setting is switched to "on", 
    mark messages are always sent to actions, no matter how recently they have been executed. 
    In this mode, mark messages can be used as a kind of heartbeat. Note that this option 
    auto-resets to "off", so if you intend to use it with multiple actions, it must be specified 
    in front off all selector lines that should provide this functionality.    

* **action.execOnlyEveryNthTime** integer    
    If configured, the next action will only be executed every n-th time. 
    For example, if configured to 3, the first two messages that go into the action will be dropped, 
    the 3rd will actually cause the action to execute, the 4th and 5th will be dropped, 
    the 6th executed under the action, ... and so on. 
    Note: this setting is automatically re-set when the actual action is defined.    

* **action.execOnlyEveryNthTimeout** integer    
    Has a meaning only if Action.ExecOnlyEveryNthTime is also configured for the same action. 
    If so, the timeout setting specifies after which period the counting of "previous actions" 
    expires and a new action count is begun. Specify 0 (the default) to disable timeouts.
    Why is this option needed? Consider this case: a message comes in at, eg., 10am. That's count 1. 
    Then, nothing happens for the next 10 hours. At 8pm, the next one occurs. 
    That's count 2. Another 5 hours later, the next message occurs, bringing the total count to 3. 
    Thus, this message now triggers the rule.
    The question is if this is desired behavior? Or should the rule only be triggered if the 
    messages occur within an e.g. 20 minute window? If the later is the case, you need a    
    Action.ExecOnlyEveryNthTimeTimeout="1200"    
    This directive will timeout previous messages seen if they are older than 20 minutes. 
    In the example above, the count would now be always 1 and consequently no rule would 
    ever be triggered.    
    
* **action.execOnlyOnceEveryInterval** integer    
    Execute action only if the last execute is at last <seconds> seconds in the past (more info in ommail, 
    but may be used with any action)</seconds>    

* **action.execOnlyWhenpReviousIsSuspended** on/off    
    This directive allows to specify if actions should always be executed ("off," the default) or only 
    if the previous action is suspended ("on"). This directive works hand-in-hand with the multiple 
    actions per selector feature. It can be used, for example, to create rules that automatically 
    switch destination servers or databases to a (set of) backup(s), if the primary server fails. 
    Note that this feature depends on proper implementation of the suspend feature in the output module.
    All built-in output modules properly support it (most importantly the database write and the 
    syslog message forwarder).    

* **action.repeatedmsgcontainsoriginalmsg** on/off    
    "last message repeated n times" messages, if generated, have a different format that contains 
    the message that is being repeated. Note that only the first "n" characters are included, 
    with n to be at least 80 characters, most probably more (this may change from version to version, 
    thus no specific limit is given). The bottom line is that n is large enough to get a good idea 
    which message was repeated but it is not necessarily large enough for the whole message.
   (Introduced with 4.1.5). Once set, it affects all following actions.

* **action.resumeRetryCount** integer    
    [default 0, -1 means eternal]

* **action.resumeInterval** integer    
    Sets the ActionResumeInterval for the action. The interval provided is always in seconds. 
    Thus, multiply by 60 if you need minutes and 3,600 if you need hours (not recommended).
    When an action is suspended (e.g. destination can not be connected), the action is resumed 
    for the configured interval. Thereafter, it is retried. If multiple retires fail, the interval 
    is automatically extended. This is to prevent excessive ressource use for retires. 
    After each 10 retries, the interval is extended by itself. To be precise, the actual interval 
    is (numRetries / 10 + 1) * Action.ResumeInterval. so after the 10th try, it by default is 60 
    and after the 100th try it is 330.


### Queue Parameters ###


* **queue.filename**  word     
    Specifes the base name to be used for queue files.    
    Default: none    
    Mandatory: yes (for disk-based queues)    
     
    Disk-based queues create a set of files for queue content. The value set via queue.filename acts 
    as the basename to be used for filename creation. For actual log data, a number is appended to 
    the file name. There is also a so-called "queue information" (qi) file created, which holds 
    administrative information about the queue status. This file is named with the base name plus 
    ".qi" as suffix.    


* **queue.size**  size      
    Specifes the maximum number of (in-core) messages a queue can hold.    
    Default: 10,000 for ruleset queues, 1,000 for action queues    
    Mandatory: no    
     
    This setting affects the in-memory queue size. Disk based queues may hold more data inside the queue, 
    but not in main memory but on disk. The size is specified in number of messages. The representation 
    of a typical syslog message object should require less than 1K, but excessively large messages may 
    also result in excessively large objects. Note that not all message types may utilize the full queue. 
    This depends on other queue parameters like the watermark settings. Most importantly, a small amount
    (seven percent) is reserved for messages with high loss potential (like UDP-received messages) and 
    will not be utilized by messages with lower loss potential (like TCP-received messages).    
    
    Warning: do not set the size to extremely small values (like less than 500 messages) unless you know 
    exactly what you do (and why!). This could interfere with other internal settings like watermarks and 
    batch sizes. It is possible to specify very small values in order to support power users who customize
    the other settings accordingly. Usually there is no need to do that. Queues take only up memory when 
    messages are stored in them. So reducing queue sizes does not reduce memory usage, except in cases 
    where queues are actually full. The default settings permit small message bursts to be buffered 
    without message loss.


* **queue.dequeuebatchsize** number     
    Specifies how many messages can be dequeued at once.    
    Default:    
    Mandatory: no    
    
    Specifies the batch size for dequeue operations. This setting affects performance. As a rule of thumb, 
    larger batch sizes (up to a environment-induced upper limit) provide better performance. 
    For the average system, there usually should be no need to adjust batch sizes as the defaults are sufficient.


* **queue.maxdiskspace** size
    Specifies maximum amount of disk space a queue may use.    
    Default: unlimited    
    Mandatory: no    
     
    This setting permits to limit the maximum amount of disk space the queue data files will use. Note that actual disk allocation may be slightly larger due to block allocation. Also, no partial messages are written to queue, so writing a message is completed even if that means going slightly above the limit. Note that, contrary to queue.size, the size is specified in bytes and not messages. It is recommended to limit queue disk allocation, as otherwise the filesystem free space may be exhausted if the queue needs to grow very large.
If the size limit is hit, messages are discarded until sufficient messages have been dequeued and queue files been deleted


* **queue.highwatermark** number    
    Specifies ...    
    Default:    
    Mandatory: no

* **queue.lowwatermark** number    
    Specifies ...    
    Default:    
    Mandatory: no

* **queue.fulldelaymark**
Specifies .

Available Since: 6.3.3    
Format: number    
Default:    
Mandatory: no


* **queue.discardmark**
Specifies .

Available Since:    6.3.3
Format: number
Default:     
Mandatory:  no


* **queue.discardseverity**
Specifies ...

Available Since:    6.3.3
Format: severity
Default:     
Mandatory:  no

* **queue.checkpointinterval**
Specifies ...

Available Since:    6.3.3
Format: number
Default:     
Mandatory:  no


* **queue.syncqueuefiles**
Specifies

Available Since:    6.3.3
Format: binary
Default:     
Mandatory:  no

* **queue.type**
Specifies ...

Available Since:    6.3.3
Format: queue type
Default: LinkedList for ruleset queues, Direct for action queues
Mandatory:  no


* **queue.workerthreads**
Specifies ...

Available Since:    6.3.3
Format: number
Default:     
Mandatory:  no

* **queue.timeoutshutdown**
Specifies ...

Available Since:    6.3.3
Format: number
Default:     
Mandatory:  no


* **queue.timeoutactioncompletion**
Specifies ...

Available Since:    6.3.3
Format: number
Default:     
Mandatory:  no


* **queue.timeoutenqueue**
Specifies ...

Available Since:    6.3.3
Format: number
Default:     
Mandatory:  no


* **queue.timeoutworkerthreadshutdown**
Specifies ...

Available Since:    6.3.3
Format: number
Default:     
Mandatory:  no

* **queue.workerthreadminimummessages**
Specifies ...

Available Since:    6.3.3
Format: number
Default:     
Mandatory:  no


* **queue.maxfilesize**
Specifies ...

Available Since:    6.3.3
Format: size
Default:     
Mandatory:  no


* **queue.saveonshutdown**
Specifies ...

Available Since:    6.3.3
Format: binary
Default:    no
Mandatory:  no

* **queue.dequeueslowdown**
Specifies

Available Since:    6.3.3
Format: number
Default:     
Mandatory:  no

* **queue.dequeuetimebegin**
Specifies

Available Since:    6.3.3
Format: number
Default:     
Mandatory:  no

* **queue.dequeuetimeend**
Specifies

Available Since:    6.3.3
Format: number
Default:     
Mandatory:  no




[[config]]
== Configure rsyslog 

We configure rsyslog 
* to recive UDP messages, 
* to filter them depending on the IP of the host, and
* to store them in a file.

### How to configure the module ###

The module has to be configured first. The general line for this configuration is: 

    module (load=”im<type of protocol>”)

So in our example, where we want UDP, it will look like this:

    module (load=”imudp”)

### How to configure the input for rsyslog ###

For the input, you have to give two different information to rsyslog. 

The first information needed is the protocol type of the input; in our example again `UDP`. 
Like in the first line there is an `im` in front of the protocol-type.

The other information is to configure a port for rsyslog, in our example 514. These two 
information items are together in only one line. The line is:

    input (type=”<protocol of input>“ port=”<number of port>“)

This means for the example, the line has to be

    input (type=”imudp” port=”514”)

### How to configure a filter for fromhost-IPs and store them in a file ###

A filter always has, like a normal conditional sentence, an “if…then” part. If you want to configure 
it to do something with all notes from a specific IP, between “if” and “then” will be the property 
“$fromhost-ip ==”-IP, you want to filter-”. After this stays a “then” and after the “then” follows 
an action in brackets, which I will explain later. 

In my example I want only the notes from the host with the IP 172.19.1.135. So the line will be

    If $fromhost-ip == “172.19.1.135” then [

After this we have to tell the computer, what to do if that case is given. In this example we want it
to store these messages in the file “/var/log/network1.log”. This is an action with the type “omfile”. 

To configure the file where to store the messages, the action is “action (type=”omfile” File=”-filename-“). So in this example, it will look like this:

    Action (type=”omfile” file=”/var/log/network1.log”)
    ]
 

All the lines together now are

    Module (load=“imupd“)

    Input (type=”imudp” port=”514”)
    If $fromhost-ip == “172.19.1.135“ then [
        Action (type=”omfile” File=”/var/log/network1.log”)
    ]

All in all it means: The input for rsyslog will listen to syslog via UDP on port 514. If the IP from the Computer, which sends the messages, is 172.19.1.135, then the action in the brackets will get activated for these. In the action the messages will be stored in the file /var/log/network1.log.

 

Rsyslog and rulesets

Rulesets are a bit more complicated. A ruleset is a set of rules, as the name implies. These are bound to an input. This works by adding an option to the input, namely “ruleset=”-rulesetname-“”. For example, if I want to bind a ruleset “rs1” to a input the line will look like this:

Input (type=”imudp” port=”514” ruleset=”rs1”)
But you still have to define, what the ruleset should do. In this guide I will limit myself to explain, how to create a ruleset, which has one action: to store all the messages in a file. In my example I want to store the messages in the file /var/log/network1.log”.

You define a ruleset like the normal configuration. To define it, you first name it with ruleset (name=”-rulesetname-“). After this you write what it does, in my example the action action (type=”omfile” file=”/var/log/network1.log”). This action you write in these curly brackets: {}.

So my full example looks like this

    Module (load=”imudp”)

    Input (type=”imudp” port=”514” ruleset=”rs1”)

    Ruleset (name=”rs1”) {
        Action (type=”omfile” file=”/var/log/network1.log”)
    }

In that second example for configurations you can see, how to store all messages from the input into a file by using a ruleset. A rulesset can consist of multiple rules, but without binding it to the input it is useless. It can be bound to an input multiple times or even other rulesets can be called.



[[configuration]]
== Configure rsyslog

In this part I’ll explain some basic configuration steps for rsyslog. We configure rsyslog to recive UDP messages, to filter them depending on the IP of the host and to store them in a file.

### How to configure the module
The module has to be configured first. The general line for this configuration is: “module (load=”im-type of protocol-”). So in our example, where we want UDP, it will look like this:

    Module (load=”imudp”)

### How to configure the input for rsyslog
For the input, you have to give two different information to rsyslog. The first information needed is the protocol type of the input; in my example again UDP. Like in the first line there is an “im-” in front of the protocol-type. The other information is to configure a port for rsyslog, in my example 514. These two information are together in only one line. The line is: “Input (type=”-protocol of input-“port=”-number of port-“). This means for my example, the line has to be
Input (type=”imudp” port=”514”)

### How to configure a filter for fromhost-IPs and store them in a file
A filter always has, like a normal conditional sentence, an “if…then” part. If you want to configure it to do something with all notes from a specific IP, between “if” and “then” will be the property “$fromhost-ip ==”-IP, you want to filter-”. After this stays a “then” and after the “then” follows an action in brackets, which I will explain later. In my example I want only the notes from the host with the IP 172.19.1.135. So the line will be
If $fromhost-ip == “172.19.1.135” then [
After this we have to tell the computer, what to do if that case is given. In this example we want him to store these messages in the file “/var/log/network1.log”. This is an action with the type “omfile”. To configure the file where to store the messages, the action is “action (type=”omfile” File=”-filename-“). So in this example, it will look like this:

Action (type=”omfile” file=”/var/log/network1.log”)
]
 

### All the lines together now are

[source]
----
    Module (load=“imupd“)
    
    Input (type=”imudp” port=”514”)
    If $fromhost-ip == “172.19.1.135“ then [
    Action (type=”omfile” File=”/var/log/network1.log”)
    ]
----    

All in all it means: The input for rsyslog will listen to syslog via UDP on port 514. If the IP from the Computer, which sends the messages, is 172.19.1.135, then the action in the brackets will get activated for these. In the action the messages will be stored in the file /var/log/network1.log.

 

Rsyslog and rulesets
====================
Rulesets are a bit more complicated. A ruleset is a set of rules, as the name implies. These are bound to an input. This works by adding an option to the input, namely “ruleset=”-rulesetname-“”. For example, if I want to bind a ruleset “rs1” to a input the line will look like this:

Input (type=”imudp” port=”514” ruleset=”rs1”)
But you still have to define, what the ruleset should do. In this guide I will limit myself to explain, how to create a ruleset, which has one action: to store all the messages in a file. In my example I want to store the messages in the file /var/log/network1.log”.

You define a ruleset like the normal configuration. To define it, you first name it with ruleset (name=”-rulesetname-“). After this you write what it does, in my example the action action (type=”omfile” file=”/var/log/network1.log”). This action you write in these curly brackets: {}.

So my full example looks like this

    Module (load=”imudp”)
    
    Input (type=”imudp” port=”514” ruleset=”rs1”)
    
    Ruleset (name=”rs1”) {
        Action (type=”omfile” file=”/var/log/network1.log”)
    }

In that second example for configurations you can see, how to store all messages from the input into a file by using a ruleset. A rulesset can consist of multiple rules, but without binding it to the input it is useless. It can be bound to an input multiple times or even other rulesets can be called.


[[elasticsearch]]
== Logging to ElasticSearch

This HOWTO should explain the steps of creating a basic setup where host(s) running rsyslog 
is sending logs to host(s) running Elasticsearch.  This would enable you to aggregate logs 
and search for them. Much like Graylog2 does, only not as nice but more flexible and scalable.

I'm running Ubuntu 12.04 x86_64, but I guess on any Linux the steps would be similar.

### Installing Elasticsearch ###
Download it from here: http://www.elasticsearch.org/download/

For Ubuntu there's a nice .deb package which you can simply install. For any other Linux, 
it's as easy as extracting the .tar.gz archive and running bin\elasticsearch

If you have a complex setup, with many logs maybe, you would probably want to build or 
adapt a custom interface. But for now we'll use elasticsearch-head as our GUI. To install
it, simply do:

    git clone git://github.com/mobz/elasticsearch-head.git

Then open index.html in your browser. In the Overview tab you will see your shards and 
replicas, while in the Browser tab you can search for your logs. Trouble is, at this
point we have no shards/replicas and no logs in it.  But that's going to change soon :D

The default settings for Elasticsearch are quite sensible, but if you have a lot of
logs, you might find this tutorial useful:
http://www.elasticsearch.org/tutorials/2012/05/19/elasticsearch-for-logging.html

### Installing rsyslog with omelasticsearch ###
At the time of writing this omelasticsearch is experimental, so you would have to 
download it from the master-elasticsearch branch here:
http://git.adiscon.com/?p=rsyslog.git;a=shortlog;h=refs/heads/master-elasticsearch

Before compiling it, you need libestr:
http://libestr.adiscon.com/download/
and libee:
http://www.libee.org/download/

Here, it was as easy as:
 # tar zxf $PACKAGE_NAME.tar.gz
 # cd $PACKAGE_NAME*
 # ./configure
 # make && make install

When doing the same thing with rsyslog, you would need to add "--enable-elasticsearch" 
when you run the configure script.

### Configuring rsyslog for elasticsearch ###
For a basic setup, you need to add the following lines:

    $ModLoad /usr/local/lib/rsyslog/omelasticsearch.so
    *.*     action(type="omelasticsearch" server="myelasticsearch.mydomain.com")

This would add send all your logs to the specified Elasticsearch server. Your index will be named "system" and your type would be "events".

Now let's suppose you want to add just some specific properties. For that, you would need to define a custom template, that would properly escape the JSON fields for you, and then tell omelasticsearch to use that template.

You can also use templates for defining index names. For example, you might want to have an index per day. This way, for "rotating" logs, you can just remove old indices.

Our config might become something like this:

 $ModLoad /usr/local/lib/rsyslog/omelasticsearch.so
 #
 # the template below will output a JSON like this:
 # {"message":"test","host":"rgheorghe","severity":"6","date":"2012-05-10T10:17:38.045","tag":"test:"}
 $template customSchema,"{\"message\":\"%msg:::json%\",\"host\":\"%HOSTNAME:::json%\",\"severity\":\"%syslogseverity%\",\"date\":\"%timereported:1:19:date-rfc3339%.%timereported:1:3:date-subseconds%\",\"tag\":\"%syslogtag:::json%\"}"
 #
 #the template below outputs something like "2012-05-10" to have our variable index names
 $template srchidx,"%timereported:1:10:date-rfc3339%"
 #
 #now we put everything together
 # "template" is for storing the syslog fields we want
 # dynSearchIndex="on" is for having variable index names
 # searchIndex is for letting rsyslog know where to get these names
 *.*     action(type="omelasticsearch" template="customSchema" searchIndex="srchidx" dynSearchIndex="on" server="myserver")

There are some other nice things you can use:
* searchType="mycustomtype" - to specify a different type than "events". You can have dynSearchType="on" to have it variable, like you can with indices
* serverport="9200" - this is the default setting, but you can specify a different port
* asyncrepl="on" to enable asyncronous replication. That is, Elasticsearch gives an answer imediately after inserting to the main shard(s). It doesn't wait for replicas to be updated as well, which is the default setting
* timeout="1m" - how long to wait for a reply from Elasticsearch. More info here, near the end: http://www.elasticsearch.org/guide/reference/api/index_.html
* basic HTTP authentication. Elasticsearch has no authentication by default, but you can enable it:

Download the http-basic plugin for Elasticsearch from here:
https://github.com/Asquera/elasticsearch-http-basic/downloads

Then, from your Elasticsearch home directory (/usr/share/elasticsearch on Ubuntu):
 # mkdir -p plugins/http-basic
 # cp elasticsearch-http-basic-1.0.3.jar plugins/http-basic/

Then you need to add the following to your config, before restarting Elasticsearch:

 http.basic.enabled: true
 http.basic.user: "myuser"
 http.basic.password: "mypass"

Which is config/elasticsearch.yml if you just extracted the elasticsearch.tar.gz. If you installed it from the .deb package, it's /etc/elasticsearch/elasticsearch.yml

On the rsyslog side, you need to add the following to your "action" line: uid="myuser" pwd="mypass".

Then restart rsyslog and it should work :)

### Using bulk indexing ###
Elasticsearch can index multiple documents at a time (eg: in the same request), which makes this approach faster than indexing one log line at a time. You can make omelasticsearch use this feature by setting bulkmode="on" in your action() line.

The bulk size depends on your queue settings. The default is 16, but, depending on your setup, a value of a few hundred will probably increase the indexing performance.

More infromation about omelasticsearch's bulk indexing here:
http://blog.gerhards.net/2012/06/using-elasticsearch-bulk-mode-with.html

And about queueing in general here:
http://www.rsyslog.com/doc/queues.html

[[elements]]
== Data Flow

<img src="http://www.rsyslog.com/doc/dataflow.png" width="680" height="305" alt="Drawing"/>

### Bird's Eye View of Rsyslog Configuration Elements ###

In a rsyslog cnfiguration file, **rulesets** are not the only elements that must
be defined at the top level.  Inputs, templates, modules, and a few directives must 
also be defined at the top level alongside the templates.

There are a few **directives** that need to be defined outside of any statements,
i.e. at the top level.  Examples of such directives are `xxx` 
and 'yyy'.  A directive always starts with a $-sign.

Among the top-level defineable elements, **primary-rulesets** are conceptually
at a higher level than other elements, including **subordinate-rulesets**.  rulesets 
can be defined hierarchically, i.e. one ruleset can call another ruleset
(called subordinate-ruleset as opposed to primary-ruleset)  A primary-ruleset is
a ruleset not reachable via any other ruleset.

Even though an **input** element must also be defined at the top level, i.e. the same
level as rulesets, it is conceptually contained in and belongs to one and only one
particular ruleset.  A ruleset contains, and is pointed to by, one or more inputs using 
the input statement's `ruleset=<ruleset-name>` option.  In other words, there is a 
one-to-many relationship between rulesets and inputs.

There is also a one-to-many relationship between **input-modules** and inputs.  Each 
input must be linked to one and only one input-module using its `type=<input-module-name>`.

Also, there is an implied many-to-many relationship between 
rulesets and **output-modules** via nested action statements.  Each nested action statement of 
a ruleset, via its `type=<output-module-name>`, must specify one and only one output-module
to be utilized for sinking the qualified messages.  On the other hand, the same output-module
can be referred to by more than one ruleset.

Similar to output-modules, **Templates** can also be syntactically bound to the action part 
of the containing rules of a ruleset, and therefore reachable via rulesets only.

It must be noted here that there is also a top-level **main-queue** configuration 
element that explicitly defines a main-queue, unfortunately, only for so-called default 
ruleset.  It is  unfortunate because trying to configure the default ruleset leads to 
an unstructured configuration file with the default ruleset's configuration items splayed 
all over the file.  The default ruleset is the legacy way to assign a ruleset to any 
input-module with a missing `ruleset=<ruleset-name>` option.  The options and rules of 
the default ruleset must be astray outside of a ruleset element and sensitive to the 
order of definitions; and therefore highly error-prone.

In a sense, primary-rulesets are the top of the food-chain reaching all the other elements,
except a few directives that don't have an structured equivalent.


### Ruleset Elements ###

A ruleset is a construct defined with the following syntax:  

    ruleset (<option> ...) { <if or action or stop statement> ... }

An example of a rullset is:

    ruleset (name=”rs1”) {
        if $fromhost-ip == '192.168.152.137' then {
            action(
                type="omfile"
                file="/var/log/remotefile02"
            )
            stop
        }
    }


Using the `ruleset=<rulesetname>` option of the input statement, a rulesets can be 
bound to an input.  For example, to bind a ruleset “rs1” to an input:

    input (
        type=”imudp” 
        port=”514”
        ruleset=”rs1”
    )

So, an a fully defined configuration may looks like:

    module (load=”imtcp”)
    module (load="omfile")

    input (type=”imtcp” port=”514” ruleset=”rs1”)

    ruleset (name=”rs1”) {
        if $fromhost-ip == '192.168.152.137' then {
            action(type="omfile" file="/var/log/remotefile02")
        }
    }

As a result of the above configuration, the rsyslog will listen to syslog via TCP on port 514.
If a received message is sent by a computr with the IP 172.19.1.135, then the messages will be 
stored in the file /var/log/network1.log.

Interestingly enough, the most important component of an explicitly defined ruleset 
(arguably even the most important component of rsyslog), i.e. its main-queue, is not an 
independently defineable element.  Each ruleset has one and only one associated main-queue.
Each main-queue is served by a pool of dedicated worker-threads.  The worker-threads are in charge
of enqueing incomming messages captured by input elements and dequeing and pushing messages
to the filter engines and parsers and then placing them in zero or more action-queues.

The only other kind of worker-threads created by rsyslog is the threads serving the 
action-queues.  In this case, there is no pool per say.  Each action-queue can only
be served by a single worker-thread dedicated to the loaded module associated to the
action element involved. 


## Input Elements ##

Input Elements are the second most important configuration elements of
rsyslog, after rulesets.   





### Multiple Rulesets ###

Starting with version 4.5.0 and 5.1.1, rsyslog supports multiple rulesets within a single configuration.
This is especially useful for routing the reception of remote messages to a set of specific rules.
 
Note that the **input module** must support binding to non-standard rulesets, so the functionality may 
not be available with all inputs.  In this document, I am using imtcp, an input module that supports 
binding to non-standard rulesets since rsyslog started to support them.

### What is a Ruleset? ###

If you have worked with (r)syslog.conf, you know that it is made up of what I call **rules** (others tend 
to call them selectors, a sysklogd term).  Each rule consist of a **filter** and one or more **actions**
to be carried out when the filter evaluates to true.  A filter may be as simple as a traditional syslog 
priority based filter (like "*.*" or "mail.info" or as complex as a script-like expression.
Details on that are covered in the config file documentation. After the filter come action specifiers,
and an action is something that does something to a message, e.g. write it to a file or forward it to
a remote logging server.

A traditional configuration file is made up of one or more of these rules.  When a new message arrives,
its processing starts with the first rule (in order of appearance in rsyslog.conf) and continues for
each rule until either all rules have been processed or a so-called `discard` action happens, in which
case processing stops and the message is thrown away (what also happens after the last rule has been 
processed).

The **multi-ruleset** support now permits to specify more than one such **rule sequence**. You can think
of a traditional config file just as a single default rule set, which is automatically bound to each 
of the inputs.  This is even what actually happens.  When rsyslog.conf is processed, the config file 
parser looks for the directive

    ruleset(name="rulesetname");

Where name is any name the user likes (but must not start with "RSYSLOG_", which is the name space 
reserved for rsyslog use).  If it finds this directive, it begins a new rule set (if the name was not 
yet known) or switches to an already-existing one (if the name was known).  All rules defined between
this `$RuleSet` directive and the next one are appended to the named ruleset.  Note that the reserved
name "RSYSLOG_DefaultRuleset" is used to specify rsyslogd's default ruleset.  You can use that name 
wherever you can use a ruleset name, including when binding an input to it.

Inside a ruleset, messages are processed as described above: they start with the first rule and rules
are processed in the order of appearance of the configuration file until either there are no more 
rules or the discard action is executed. Note that with multiple rulesets no longer all rsyslog.conf
rules are executed but only those that are contained within the specific ruleset.

Inputs must explicitly bind to rulesets. If they don't do, the default ruleset is bound.

This brings up the next question:

### What does "To bind to a Ruleset" mean? ###

This term is used in the same sense as "to bind an IP address to an interface": it means that a 
specific input, or part of an input (like a tcp listener) will use a specific ruleset to "pass its
messages to". So when a new message arrives, it will be processed via the bound ruleset. Rule from 
all other rulesets are irrelevant and will never be processed.

This makes multiple rulesets very handy to process local and remote message via separate means: bind
the respective receivers to different rule sets, and you do not need to separate the messages by any
other method.

Binding to rulesets is input-specific. For imtcp, this is done via the following directive:

    input(
        type="imptcp" 
        port="514" 
        ruleset="rulesetname"
    );

Note that "name" must be the name of a ruleset that is already defined at the time the bind
directive is given. There are many ways to make sure this happens, but I personally think that it is 
best to define all rule sets at the top of rsyslog.conf and define the inputs at the bottom. This kind
of reverses the traditional recommended ordering, but seems to be a really useful and straightforward 
way of doing things.

### Why are rulesets important for different parser configurations? ###

Custom message parsers, used to handle different (and potentially otherwise-invalid) message formats, 
can be bound to rulesets. So multiple rulesets can be a very useful way to handle devices sending 
messages in different malformed formats in a consistent way. Unfortunately, this is not uncommon in 
the syslog world. An in-depth explanation with configuration sample can be found at the $RulesetParser
configuration directive.

### Can I use a different Ruleset as the default? ###

This is possible by using the following directive:

    $DefaultRuleset <name>

Please note, however, that this directive is actually global: that is, it does not modify the
ruleset to which the next input is bound but rather provides a system-wide default rule set for those 
inputs that did not explicitly bind to one. As such, the directive can not be used as a work-around to 
bind inputs to non-default rulesets that do not support ruleset binding.

### Examples ###

#### Split local and remote logging ####

Let's say you have a pretty standard system that logs its local messages to the usual bunch of files 
that are specified in the default rsyslog.conf. As an example, your rsyslog.conf might look like this:

    # ... module loading ...
    # The authpriv file has restricted access.
    authpriv.*  /var/log/secure
    # Log all the mail messages in one place.
    mail.*      /var/log/maillog
    # Log cron stuff
    cron.*      /var/log/cron
    # Everybody gets emergency messages
    *.emerg     *
    ... more ...

Now, you want to add receive messages from a remote system and log these to a special file, but you do
not want to have these messages written to the files specified above. The traditional approach is to 
add a rule in front of all others that filters on the message, processes it and then discards it:

    # ... module loading ...
    # process remote messages
    if $fromhost-ip == '192.168.152.137' then {
        action(
            type="omfile"
            file="/var/log/remotefile02"
        )
    stop
    }

    # only messages not from 192.0.21 make it past this point

    # The authpriv file has restricted access.
    authpriv.*                            /var/log/secure
    # Log all the mail messages in one place.
    mail.*                                /var/log/maillog
    # Log cron stuff
    cron.*                                /var/log/cron
    # Everybody gets emergency messages
    *.emerg                               *
    ... more ...

Note that "stop" is the discard action!. Also note that we assume that 192.0.2.1 is the sole remote 
sender (to keep it simple).

With multiple rulesets, we can simply define a dedicated ruleset for the remote reception case and 
bind it to the receiver. This may be written as follows:

    # ... module loading ...
    # process remote messages
    # define new ruleset and add rules to it:
    ruleset(name="remote"){
    action(
            type="omfile" 
            file="/var/log/remotefile"
        )
    }
    # only messages not from 192.0.21 make it past this point

    # bind ruleset to tcp listener and activate it:
    input(type="imptcp" port="10514" ruleset="remote")

#### Split local and remote logging for three different ports ####

This example is almost like the first one, but it extends it a little bit. While it is very similar,
I hope it is different enough to provide a useful example why you may want to have more than two 
rulesets.

Again, we would like to use the "regular" log files for local logging, only. But this time we set 
up three syslog/tcp listeners, each one listening to a different port (in this example 10514, 
10515, and 10516). Logs received from these receivers shall go into different files. Also, logs 
received from 10516 (and only from that port!) with "mail.*" priority, shall be written into a 
specif file and not be written to 10516's general log file.

This is the config:

    # ... module loading ...
    # process remote messages

    ruleset(name="remote10514"){
    action(
            type="omfile" 
            file="/var/log/remote10514"
        )
    }

    ruleset(name="remote10515"){
    action(
            type="omfile" 
            file="/var/log/remote10515"
        )
    }

    ruleset(name="test1"){
        if prifilt("mail.*") then {
            /var/log/mail10516
            stop
            # note that the stop-command will prevent this message from 
            # being written to the remote10516 file - as usual...   
        }
        /var/log/remote10516
    }

    # and now define listeners bound to the relevant ruleset
    input(
        type="imptcp" 
        port="10514" 
        ruleset="remote10514"
    )
    input(
        type="imptcp"
        port="10515" 
        ruleset="remote10515"
    )
    input(
        type="imptcp" 
        port="10516"
        ruleset="remote10516"
    )

### Performance ###

#### Fewer Filters ####

No rule processing can be faster than not processing a rule at all. As such, it is useful for a 
high performance system to identify disjunct actions and try to split these off to different rule
sets. In the example section, we had a case where three different tcp listeners need to write to 
three different files. This is a perfect example of where multiple rule sets are easier to use 
and offer more performance. The performance is better simply because there is no need to check 
the reception service - instead messages are automatically pushed to the right rule set and can 
be processed by very simple rules (maybe even with "*.*"-filters, the fastest ones available).

#### Partitioning of Input Data ####

Starting with rsyslog 5.3.4, rulesets permit higher concurrency. They offer the ability to run on
their own "main" queue. What that means is that a own queue is associated with a specific rule set.
That means that inputs bound to that ruleset do no longer need to compete with each other when 
they enqueue a data element into the queue. Instead, enqueue operations can be completed in parallel.

**An example:** let us assume we have three TCP listeners. Without rulesets, each of them needs to 
insert messages into the main message queue. So if each of them wants to submit a newly arrived 
message into the queue at the same time, only one can do so while the others need to wait. 
With multiple rulesets, its own queue can be created for each ruleset. If now each listener is 
bound to its own ruleset, concurrent message submission is possible. On a machine with a 
sufficiently large number of cores, this can result in dramatic performance improvement.

It is highly advised that high-performance systems define a dedicated ruleset, with a dedicated 
queue for each of the inputs.

By default, rulesets do not have their own queue. It must be activated via the 
$RulesetCreateMainQueue directive.



[[es]]
== Elasticsearch Output Module - omelasticsearch

This module provides native support for logging to Elasticsearch.

Action Parameters:

* **server**    
Host name or IP address of the Elasticsearch server. Defaults to "localhost"
* **serverport**    
HTTP port to connect to Elasticsearch. Defaults to 9200
* **searchIndex**    
Elasticsearch index to send your logs to. Defaults to "system"
* **dynSearchIndex** <on/off>    
Whether the string provided for searchIndex should be taken as a template. 
Defaults to "off", which means the index name will be taken literally. 
Otherwise, it will look for a template with that name, and the resulting string will be the index name. 
For example, let's assume you define a template named "date-days" containing "%timereported:1:10:date-rfc3339%". 
Then, with dynSearchIndex="on", if you say searchIndex="date-days", each log will be sent to 
and index named after the first 10 characters of the timestamp, like "2013-03-22".
* **searchType**    
Elasticsearch type to send your index to. Defaults to "events"
* **dynSearchType** <on/off>    
Like dynSearchIndex, it allows you to specify a template for searchType, instead of a static string.
* **asyncrepl** <on/off>    
By default, an indexing operation returns after all replica shards have indexed the document. 
With asyncrepl="on" it will return after it was indexed on the primary shard only - thus 
trading some consistency for speed.
* **timeout**    
How long Elasticsearch will wait for a primary shard to be available for indexing your 
log before sending back an error. Defaults to "1m".
* **template**    
This is the JSON document that will be indexed in Elasticsearch. The resulting string needs to be 
a valid JSON, otherwise Elasticsearch will return an error. Defaults to:

    $template JSONDefault, 
    "{\"message\":\"%msg:::json%\",\"fromhost\":\"%HOSTNAME:::json%\",
    \"facility\":\"%syslogfacility-text%\",
    \"priority\":\"%syslogpriority-text%\",
    \"timereported\":\"%timereported:::date-rfc3339%\",
    \"timegenerated\":\"%timegenerated:::date-rfc3339%\"}"

Which will produce this sort of documents (pretty-printed here for readability):

    {
        "message": " this is a test message",
        "fromhost": "test-host",
        "facility": "user",
        "priority": "info",
        "timereported": "2013-03-12T18:05:01.344864+02:00",
        "timegenerated": "2013-03-12T18:05:01.344864+02:00"
    }

* **bulkmode** <on/off>    
The default "off" setting means logs are shipped one by one. Each in its own HTTP request, 
using the Index API. Set it to "on" and it will use Elasticsearch's Bulk API to send 
multiple logs in the same request. The maximum number of logs sent in a single bulk request 
depends on your queue settings - usually limited by the dequeue batch size. More information 
about queues can be found here.
* **parent**    
Specifying a string here will index your logs with that string the parent ID of those logs. Please note that you need to define the parent field in your mapping for that to work. By default, logs are indexed without a parent.
* **dynParent** <on/off>
Using the same parent for all the logs sent in the same action is quite unlikely. So you'd probably want to turn this "on" and specify a template that will provide meaningful parent IDs for your logs.
* **uid**    
If you have basic HTTP authentication deployed (eg: through the elasticsearch-basic plugin), you can specify your user-name here.
* **pwd**    
Password for basic authentication.
Samples:

The following sample does the following:

loads the omelasticsearch module
outputs all logs to Elasticsearch using the default settings
module(load="omelasticsearch")
*.*     action(type="omelasticsearch")

The following sample does the following:

loads the omelasticsearch module
defines a template that will make the JSON contain the following properties (more info about what properties you can use here):
RFC-3339 timestamp when the event was generated
the message part of the event
hostname of the system that generated the message
severity of the event, as a string
facility, as a string
the tag of the event
outputs to Elasticsearch with the following settings
host name of the server is myserver.local
port is 9200
JSON docs will look as defined in the template above
index will be "test-index"
type will be "test-type"
activate bulk mode. For that to work effectively, we use an in-memory queue that can hold up to 5000 events. The maximum bulk size will be 300
retry indefinitely if the HTTP request failed (eg: if the target server is down)
module(load="omelasticsearch")
template(name="testTemplate"
         type="list"
         option.json="on") {
           constant(value="{")
             constant(value="\"timestamp\":\"")      property(name="timereported" dateFormat="rfc3339")
             constant(value="\",\"message\":\"")     property(name="msg")
             constant(value="\",\"host\":\"")        property(name="hostname")
             constant(value="\",\"severity\":\"")    property(name="syslogseverity-text")
             constant(value="\",\"facility\":\"")    property(name="syslogfacility-text")
             constant(value="\",\"syslogtag\":\"")   property(name="syslogtag")
           constant(value="\"}")
         }
*.* action(type="omelasticsearch"
           server="myserver.local"
           serverport="9200"
           template="testTemplate"
           searchIndex="test-index"
           searchType="test-type"
           bulkmode="on"
           queue.type="linkedlist"
           queue.size="5000"
           queue.dequeuebatchsize="300"
           action.resumeretrycount="-1")
 


[[expression]]
== Expressions

The language supports arbitrary complex expressions. All usual operators are supported. The precedence of operations is as follows (with operations being higher in the list being carried out before those lower in the list, e.g. multiplications are done before additions.

    expressions in parenthesis
    not, unary minus
    *, /, % (modulus, as in C)
    +, -, & (string concatenation)
    ==, !=, <>, <, >, <=, >=, contains (strings!), startswith (strings!)
    and
    or

For example, "not a == b" probably returns not what you intended. The script processor will first evaluate "not a" and then compare the resulting boolean to the value of b. What you probably intended to do is "not (a == b)". And if you just want to test for inequality, we highly suggest to use "!=" or "<>". Both are exactly the same and are provided so that you can pick whichever you like best. So inquality of a and b should be tested as "a <> b". The "not" operator should be reserved to cases where it actually is needed to form a complex boolean expression. In those cases, parenthesis are highly recommended.


Rsyslog supports expressions at a growing number of places. 
So far, they are supported for filtering messages.

C-like comments `/* some comment */` are supported inside the expression, 
but not yet in the rest of the configuration file.



[[flow]]
== Message Flow

=== Message Flow
Depending on their module type, modules may access and/or modify messages at various stages 
during rsyslog's processing. Note that only the "core type" (e.g. input, output) but not any 
type derived from it (message modification module) specifies when a module is called.

=== Simplified Workflow
.The simplified workflow is as follows
image:http://www.rsyslog.com/doc/module_workflow.png[
"Message Flow",link="http://www.rsyslog.com/doc/module_workflow.png"]

As can be seen, messages are received by input modules, then passed to one or many parser 
modules, which generate the in-memory representation of the message and may also modify the 
message itself. The, the internal representation is passed to output modules, which may 
output a message and (with the interfaces newly introduced in v5) may also modify messageo 
object content.

String generator modules are not included inside this picture, because they are not a 
required part of the workflow. If used, they operate "in front of" the output modules, 
because they are called during template generation.

Note that the actual flow is much more complex and depends a lot on queue and filter settings. 
This graphic above is a high-level message flow diagram.


[[function]]
== Functions

RainerScript currently support quite a limited set of functions:

* **getenv(str)** - like the OS call, returns the value of the environment variable, if it exists. 
    Returns an empty string if it does not exist.

* **strlen(str)** - returns the length of the provided string

* **tolower(str)** - converts the provided string into lowercase

* **cstr(expr)** - converts expr to a string value

* **cnum(expr)** - converts expr to a number (integer)

* **re_match(expr, re)** - returns 1, if expr matches re, 0 otherwise

* **re_extract(expr, re, match, submatch, no-found)** - extracts data from a string (property) via a 
regular expression match. POSIX ERE regular expressions are used. The variable "match" contains 
the number of the match to use. This permits to pick up more than the first expression match. 
Submatch is the submatch to match (max 50 supported). The "no-found" parameter specifies which 
string is to be returned in case when the regular expression is not found. Note that match and 
submatch start with zero. It currently is not possible to extract more than one submatch with a 
single call.

* **field(str, delim, matchnbr)** - returns a field-based substring. str is the string to search, 
delim is the delimiter and matchnbr is the match to search for (the first match starts at 1). 
This works similar as the field based property-replacer option. Versions prior to 7.3.7 only support
a single character as delimiter character. Starting with version 7.3.7, a full string can be used 
as delimiter. If a single character is being used as delimiter, delim is the numerical ascii value 
of the field delimiter character (so that non-printable characters can by specified). 
If a string is used as delmiter, a multi-character string (e.g. "#011") is to be specified. Samples:    
    
    `set $!usr!field = field($msg, 32, 3);     -- the third field, delimited by space`    
    `set $!usr!field = field($msg, "#011", 3); -- the third field, delmited by "#011"`    
    
    Note that when a single character is specified as string [field($msg, ",", 3)] a string-based 
extraction is done, which is more performance intense than the equivalent single-character 
[field($msg, 44 ,3)] extraction.

* **prifilt(constant)** - mimics a traditional PRI-based filter (like "*.*" or "mail.info"). 
The traditional filter string must be given as a constant string. Dynamic string evaluation 
is not permitted (for performance reasons).

The following example can be used to build a dynamic filter based on some environment variable:

    if $msg contains getenv('TRIGGERVAR') then /path/to/errfile




[[ifcondition]]
== Filter Conditions
[Source](http://www.rsyslog.com/doc/rsyslog_conf_filter.html)

Rsyslog offers three different types "filter conditions":
* RainerScript</a>-based filters
* "traditional" severity and facility based selectors
* property-based filters


### RainerScript-Based Filters ###

RainerScript based filters are the prime means of creating complex rsyslog configuration.
The permit filtering on arbitrary complex expressions, which can include boolean,
arithmetic and string operations. They also support full nesting of filters, just
as you know from other scripting environments.    
Scripts based filters are indicated by the keyword "if", as usual.
They have this format:
    
    if expr then block else block

"If" and "then" are fixed keywords that mus be present. "expr" is a (potentially quite complex) expression. 
So the <a href="expression.html">expression documentation</a> for details.
The keyword "else" and its associated block is optional. Note that a block can contain either
a single action (chain), or an arbitrary complex script enclosed in curly braces, e.g.:

    if $programname == 'prog1' then {
        action(type="omfile" file="/var/log/prog1.log")
        if $msg contains 'test' then
            action(type="omfile" file="/var/log/prog1test.log")
        else
            action(type="omfile" file="/var/log/prog1notest.log")
    }

Other types of filtes can also be combined with the pure RainerScript ones. This makes
it particularly easy to migrate from early config files to RainerScript. Also, the traditional
syslog PRI-based filters are a good and easy to use addition. While they are legacy, we still
recommend there use where they are up to the job. We do NOT, however, recommend property-based
filters any longer. As an example, the following is perfectly valid:

    if $fromhost == 'host1' then {
        mail.* action(type="omfile" file="/var/log/host1/mail.log")
        *.err /var/log/host1/errlog # this is also still valid
        # 
        # more "old-style rules" ...
        #
    } else {
        mail.* action(type="omfile" file="/var/log/mail.log")
        *.err /var/log/errlog
        # 
        # more "old-style rules" ...
        #
    }

Right now, you need to specify numerical values if you would like to check for facilities 
and severity. These can be found  in [RFC 3164](http://www.ietf.org/rfc/rfc3164.txt)
If you don't like that, you can of course also use the textual property - just be sure to use the right one.  
As expression support is enhanced, this will change. For example, if you would like to filter on message
that have facility local0, start with "DEVNAME" and have either
"error1" or "error0" in their message content, you could use the following filter:

    if $syslogfacility-text == 'local0' and 
       $msg startswith 'DEVNAME'        and 
       ($msg contains 'error1' or $msg contains 'error0')
        then /var/log/somelog<br>

Please note that the above **must all be on one line**! And if you would like to store all
messages except those that contain "error1" or "error0", you just need
to add a "not":

    if  $syslogfacility-text == 'local0' and 
        $msg startswith 'DEVNAME' and not 
        not ($msg contains 'error1' or $msg contains 'error0') 
    then 
        /var/log/somelog<br>

If you would like to do case-insensitive comparisons, use
"contains_i" instead of "contains" and "startswith_i" instead of "startswith".

Regular expressions are supported via functions (see function list).

### Selectors ###

**Selectors are the traditional way of filtering syslog messages.** 
They have been kept in rsyslog with their original syntax, because it is well-known, highly 
effective and also needed for compatibility with stock syslogd configuration files. 
If you just need to filter based on priority and facility, you should do this with
selector lines. They are <b>not</b> second-class citizens
in rsyslog and offer the best performance for this job.

The selector field itself again consists of two parts, a
facility and a priority, separated by a period (".''). 
Both parts are
case insensitive and can also be specified as decimal numbers, but
don't do that, you have been warned. Both facilities and priorities are
described in syslog(3). The names mentioned below correspond to the
similar LOG_-values in /usr/include/syslog.h.

The facility is one of the following keywords:  auth, authpriv, cron, daemon, kern, lpr, 
mail, mark, news, security (same as auth), syslog, user, uucp and local0 through local7.

The keyword security should not
be used anymore and mark is only for internal use and therefore should
not be used in applications. Anyway, you may want to specify and
redirect these messages here. The facility specifies the subsystem that
produced the message, i.e. all mail programs log with the mail facility
(LOG_MAIL) if they log using syslog.

The priority is one of the following keywords, in ascending order:
debug, info, notice, warning, warn (same as warning), err, error (same as err), 
crit, alert, emerg, panic (same as emerg). 
The keywords error, warn and panic are deprecated and should not be used anymore. 
The priority defines the severity of the message.

The behavior of the original BSD syslogd is that all messages of the
specified priority and higher are logged according to the given action.
Rsyslogd behaves the same, but has some extensions.

In addition to the above mentioned names the rsyslogd(8) understands
the following extensions: An asterisk ("*'') stands for all facilities
or all priorities, depending on where it is used (before or after the
period). The keyword none stands for no priority of the given facility.

You can specify multiple facilities with the same priority pattern in
one statement using the comma (",'') operator. You may specify as much
facilities as you want. Remember that only the facility part from such
a statement is taken, a priority part would be skipped.

Multiple selectors may be specified for a single action using
the semicolon (";'') separator. Remember that each selector in the
selector field is capable to overwrite the preceding ones. Using this
behavior you can exclude some priorities from the pattern.

Rsyslogd has a syntax extension to the original BSD source,
that makes its use more intuitively. You may precede every priority
with an equals sign ("='') to specify only this single priority and
not any of the above. You may also (both is valid, too) precede the
priority with an exclamation mark ("!'') to ignore all that
priorities, either exact this one or this and any higher priority. If
you use both extensions than the exclamation mark must occur before the
equals sign, just use it intuitively.

### Property-Based Filters ###

Property-based filters are unique to rsyslogd. They allow to
filter on any property, like HOSTNAME, syslogtag and msg. A list of all
currently-supported properties can be found in the <a href="property_replacer.html">property replacer documentation</a>
(but keep in mind that only the properties, not the replacer is
supported). With this filter, each properties can be checked against a
specified value, using a specified compare operation.

A property-based filter must start with a colon in column 0.
This tells rsyslogd that it is the new filter type. The colon must be
followed by the property name, a comma, the name of the compare
operation to carry out, another comma and then the value to compare
against. This value must be quoted. There can be spaces and tabs
between the commas. Property names and compare operations are
case-sensitive, so "msg" works, while "MSG" is an invalid property
name. In brief, the syntax is as follows:

    :property, [!]compare-operation, "value"

The following **compare-operations** are currently supported:

* **contains**    
Checks if the string provided in value is contained in
the property. There must be an exact match, wildcards are not supported.


* **isempty**    
Checks if the property is empty. The value is discarded. This is
especially useful when working with normalized data, where some fields
may be populated based on normalization result.
Available since 6.6.2.


* **isequal**    
Compares the "value" string provided and the property contents.
These two values must be exactly equal to match. 
The difference to contains is that contains searches for the value anywhere
inside the property value, whereas all characters must be identical for isequal. 
As such, isequal is most useful for fields like syslogtag or
FROMHOST, where you probably know the exact contents.


* **startswith**    
Checks if the value is found exactly at the beginning
of the property value. For example, if you search for "val" with

    :msg, startswith, "val"

    it will be a match if msg contains "values are in this
message" but it won't match if the msg contains "There are values in
this message" (in the later case, contains would match). Please note
that "startswith" is by far faster than regular expressions. So
it makes very much sense (performance-wise) to use "startswith".

    Note: when processing syslog messages, please note that $msg usually
starts with a space. The reason for this is RFC3164. Please read the
<a href="http://www.rsyslog.com/log-normalization-and-the-leading-space/">detail
description</a> of what that means to you. In short, you need to make sure
that you include the first space if you use "startswith", otherwise you will
not get matches.



* **regex**    
Compares the property against the provided POSIX BRE regular expression.


* **ereregex**    
Compares the property against the provided POSIX ERE regular expression.


You can use the bang-character (!) immediately in front of a
compare-operation, the outcome of this operation is negated. For
example, if msg contains "This is an informative message", the
following sample would not match:

    :msg, contains, "error"

but this one matches:

    :msg, !contains, "error"    

Using negation can be useful if you would like to do some
generic processing but exclude some specific events. You can use the
discard action in conjunction with that. A sample would be:

    *.* /var/log/allmsgs-including-informational.log
    :msg, contains, "informational" ~
    *.* /var/log/allmsgs-but-informational.log

Do not overlook the red tilde in line 2! In this sample, all
messages are written to the file allmsgs-including-informational.log.
Then, all messages containing the string "informational" are discarded.
That means the config file lines below the "discard line" (number 2 in
our sample) will not be applied to this message. Then, all remaining
lines will also be written to the file allmsgs-but-informational.log.

**Value** is a quoted string. It supports some escape sequences:</p>

\" - the quote character (e.g. "String with \"Quotes\"")    
\\ - the backslash character (e.g. "C:\\tmp")

Escape sequences always start with a backslash. Additional
escape sequences might be added in the future. Backslash characters <b>must</b>
be escaped. Any other sequence then those outlined above is invalid and
may lead to unpredictable results.

<p>Probably, "msg" is the most prominent use case of property
based filters. It is the actual message text. If you would like to
filter based on some message content (e.g. the presence of a specific
code), this can be done easily by:</p>

    :msg, contains, "ID-4711"

This filter will match when the message contains the string
"ID-4711". Please note that the comparison is case-sensitive, so it
would not match if "id-4711" would be contained in the message.

    :msg, regex, "fatal .* error"

This filter uses a POSIX regular expression. It matches when the
string contains the words "fatal" and "error" with anything in between
(e.g. "fatal net error" and "fatal lib error" but not "fatal error" as
two spaces are required by the regular expression!).

Getting property-based filters right can sometimes be challenging. 
In order to help you do it with as minimal effort as
possible, rsyslogd spits out debug information for all property-based
filters during their evaluation. To enable this, run rsyslogd in
foreground and specify the "-d" option.

Boolean operations inside property based filters (like
'message contains "ID17" or message contains "ID18"') are currently not
supported (except for "not" as outlined above). Please note that while
it is possible to query facility and severity via property-based
filters, it is far more advisable to use classic selectors (see above)
for those cases.



[[input]]
== input() statement: a quick look

The new input() config statement is released. This concludes the major part of the new config 
format for v6 (v7 will also support an enhanced ruleset() statement). This article gives you 
some quick ideas of how the new format looks in practice.  Following is a small test 
rsyslog.conf with the old-style directives commented out and followed by the new style ones. 
Here it is:

    #$ModLoad imfile
    #$inputfilepollinterval 1

    module(
        load="imfile" 
        pollingInterval="1"
    )
>

    #input(type="imuxsock" )

    module(
        load="imuxsock" 
        syssock.use="off"
    )
    input(
        type="imuxsock" 
        socket="/home/rgerhards/testsock"
    )
>

    #$ModLoad imfile
    #$InputFileName /tmp/inputfile
    #$InputFileTag tag1:
    #$InputFileStateFile inputfile-state
    #$InputRunFileMonitor

    module(load="imfile")
    input( type="imfile" file="/tmp/inputfile" tag="tag1:" statefile="inputfile-state")
>

    #$ModLoad imtcp
    #$InputPTCPServerRun 13514
    module(load="imptcp")
    input(type="imptcp" port="13514")
>

    module(load="imtcp" keepalive="on")
    #$InputTCPServerSupportOctetCountedFraming off
    #$InputTCPServerInputName tcpname
    #$InputTCPServerRun 13515

    input(type="imtcp" port="13515" name="tcpname" supportOctetCountedFraming="off")
>

    #$UDPServerRun 13514
    #$UDPServerRun 13515

    input(type="imudp" port="13514")
    input(type="imudp" port="13515")
>



[[jsonparse]]
== Log Message Normalization Module

Module Name: mmjsonparse

Description:

This module provides support for parsing structured log messages that follow the CEE/lumberjack spec. The so-called "CEE cookie" is checked and, if present, the JSON-encoded structured message content is parsed. The properties are than available as original message properties.

Sample:

This activates the module and applies normalization to all messages:

    module(load="mmjsonparse")
    action(type="mmjsonparse")
    
The same in legacy format:

    $ModLoad mmjsonparse
    *.* :mmjsonparse:
    
    
    
### how to use mmjsonparse only for select messages ###

Rsyslog's mmjsonparse module permits to parse JSON base data (actually expecting CEE-format). 
This message modification module is implemented via the output plugin interface, which 
provides some nice flexibility in using it.  Most importantly, you can trigger parsing only 
for a select set of messages.

Note that the module checks for the presence of the cee cookie. Only if it is present, json 
parsing will happen. Otherwise, the message is left alone. As the cee cookie was specifically 
designed to signify the presence of JSON data, this is a sufficient check to make sure only 
valid data is processed.

However, you may want to avoid the (small) checking overhead for non-json messages (note, however, 
that the check is *really fast*, so using a filter just to spare it does not gain you too much). 
Another reason for using only a select set might be that you have different types of 
cee-based messages but want to parse (and specifically process just some of them).

With mmjsonparse being implemented via the output module interface, it can be used like a 
regular action. So you could for example do this:

    if ($programname == 'rsyslogd-pstats') then {
          action(type="mmjsonparse")
          action(type="omfwd" target="target.example.net" template="..." ...)
    }

As with any regular action, mmjsonparse will only be called when the filter evaluates to true. 
Note, however, that the modification mmjsonparse makes (most importantly creating the structured data) 
will be kept after the closing if-block. So any other action below that if (in the config file) will 
also be able to see it.

### CEE-enhanced syslog defined ###

CEE-enhanced syslog is an upcoming standard for expressing structured data inside syslog messages. 
It is a cross-platform effort that aims at making log analysis (and log processing in general) 
much more easy both for log producers and consumers. 

The idea was originally born as part of MITRE's CEE effort. It has been adopted by a larger set 
of logging stakeholders in an initiative that was named "project lumberjack". Under this project, 
cee-enhanced syslog, and a framework to make full use of it, is being openly advanced. 
It is hoped (and planned) that the outcome will flow back to the CEE standard.

In a nutshell cee-enhanced syslog is very simple and powerful: inside the syslog message, a 
special cookie ("@cee:") is followed by a JSON representation of the data. The cookie tells 
processors that the format is actually cee-enhanced. 

If you are interested in a more technical coverage, have a look at my 
[cee-enhanced syslog howto presentation[().


### JSON and rsyslog templates ###

Rsyslog already supports JSON parsing and formatting (for all cee properties). 
However, the way formatting currently is done is unsatisfactory to me. Right now, 
we just take the cee properties as they are and format them into JSON format. 
In this mode, we do not have any way to specify which fields to use and we also 
do not have a way to modify the field contents (e.g. pick substrings or do case 
conversions). Exactly these are the use cases rsyslog invented templates for.

One way to handle the situation is to have the user write the JSON code inside the 
template and just inject the data field where desired. This almost works (and I 
know Brian Knox tries to explore that route).  IT just works "almost" as there is 
currently no property replacer option to ensure proper JSON escaping. Adding this 
option is not hard. However, I don't feel this approach is the right route to take: 
making the admin craft the JSON string is error-prone and very user-unfriendly.

So I wonder what would be a good way to specify fields that shall go into a JSON format. 
As a limiting factor, the method should be possible within the limits of the current 
template system - otherwise it will probably take too long to implement it. 
The same question also arises for outputs like MongoDB: how best to specify the fields 
(and structure!) to be passed to the output module?

Of course, both questions are closely related. One approach would be to solve the
JSON encoding and say that to outputs like MongoDB JSON is passed. 
Unfortunately, this has strong performance implications. In a nutshell, it would mean 
formatting the data to JSON, and then re-parsing it inside the plugin. 
This process could be be somewhat simplified by passing the data structure 
(the underlaying tree) itself rather than the JSON encoding.  However, this would still 
mean, that a data structure specific for this use would need to be created. 
That obviously involves a lot of data-copying.
So it would probably be useful to have a capability to specify fields (and replacement 
options) that are just passed down to the module for its use (that would probably limit
the required amount of data copying, at least in common cases). Question again: what 
would be a decent syntax to specify this?

Suggestions are highly welcome. I need to find at least an interim solution urgently, 
as this is an important building block for the MongoDB driver and all work that will 
depend on it. So please provide feedback (note that I may try out a couple of things 
to finally settle on one - so any idea is highly welcome ;)).





[[jsonparser]]
== JSON Parser Module - mmjsonparse


#### Description ####

This module provides support for parsing structured log messages that follow the CEE/lumberjack spec. 
The so-called "CEE cookie" is checked and, if present, the JSON-encoded structured message content is parsed. 
The properties are than available as original message properties.

#### Sample ####

This activates the module and applies normalization to all messages:

    module(load="mmjsonparse")
    action(type="mmjsonparse")

The same in legacy format:

    $ModLoad mmjsonparse
    *.* :mmjsonparse:



[[jsonparsetip]]
== using mmjsonparse only for select messages

Rsyslog's mmjsonparse module permits to parse JSON base data (actually expecting CEE-format). 
This message modification module is implemented via the output plugin interface, which provides 
some nice flexibility in using it.  Most importantly, you can trigger parsing only for a select 
set of messages.

Note that the module checks for the presence of the cee cookie.  Only if it is present, 
json parsing will happen.  Otherwise, the message is left alone.  As the cee cookie was 
specifically designed to signify the presence of JSON data, this is a sufficient check to 
make sure only valid data is processed.

However, you may want to avoid the (small) checking overhead for non-json messages (note, however, 
that the check is *really fast*, so using a filter just to spare it does not gain you too much). 

Another reason for using only a select set might be that you have different types of cee-based 
messages but want to parse (and specifically process just some of them).

With mmjsonparse being implemented via the output module interface, it can be used like a regular action. 
So you could for example do this:

    if ($programname == 'rsyslogd-pstats') then {
        action(type="mmjsonparse")
        action(type="omfwd" target="target.example.net" template="..." ...)
    }

As with any regular action, mmjsonparse will only be called when the filter evaluates to true. Note, 
however, that the modification mmjsonparse makes (most importantly creating the structured data) will 
be kept after the closing if-block. So any other action below that if (in the config file) will also 
be able to see it.



[[jsonparsing]]
== parsing JSON-enhanced syslog

Strucuted logging is cool. A couple of month ago, I added support for log normalization and 
the 0.5 draft CEE standard to rsyslog. At last weeks Fedora Developer's Conference, there was 
a huge agreement that CEE-like JSON is a great way to enhance syslog logging. To follow up on 
this concept, I have integrated a JSON decoder into libee, so that it can now decode JSON with 
a single method call. It's a proof of concept, and for serious use performance optimization 
needs to be done. Besides that, it's already quite solid.

Also, I just added the mmjsonparse message modification module to rsyslog (available now in 
git master branch!). It checks if the message contains an "@JSON: " cookie and, if so, tries 
to parse the resulting string as JSON. If that succeeds, we obviously have a JSON-enhanced 
message and the individual name/value pairs are stored and can be used both in filters and 
output templates. This provides some really great opportunities when it comes to processing 
the structured data. Just think about RESTful interfaces and such!

Right now, everything is at proof of concept level, but works well enough for you to try it. 
I'll smoothen some edges but will release the versions rather soon. Probably the biggest drawback 
is that the JSON processor currently flattens the event, with structure being conveyed via 
field names. That means if you have a JSON object "SUPER" containing a number of fields "field1" 
to "fieldn", the current implementation will be a single level and the names are "SUPER.field1",... 
I did this in order to have a quick solution and one that fits into the existing framework. 
I'll work on creating real structure soon. It's not really hard, but I probably do some other PoCs first ;)

I considered several approaches, among them moving over to libcollection (part of ding-libs) or 
a pure JSON parser. The more I worked with the code, the more it turned out that libee already has 
a lot of the necessary plumbing and could simply been enhanced/modified under the hood. 

The big plus 
in that approach is that is immediately plugs in into rsyslog and the other solutions that already 
built on it. This even enables using the new functionality in the v6 context (I originally thought 
I'd need to move on to rsyslog v7 for the name-value pair changes). 

Now that I have written mmjsonparse, 
this really seems to work out. No engine change was required, and I expect little need for change even
for the final version. As such, I'll proceed in that direction. Actually, what I now use is kind of
a hybrid approch: I use a lot of philosophy of libcollection, which showed me the right route to take. 
Then, I use cJSON, which is a really nice JSON parser. 

In the proof of concept, I use both 
cJSON's object model and libee's own. I expect to merge them, actually tightly integrating cJSON.
The reason is that CEE has evolved quite a bit in the mean time, and many complex constructs are 
no longer required. As such, I can streamline the library as well, what not only reduces complexity 
but speeds up the whole process.

[[loggly]]
== loggly

source s_all {
    file ("/proc/kmsg" log_prefix("kernel: "));
    unix-stream ("/dev/log");
    internal();
    file("/mnt/log/apache2/error.log" follow_freq(1) flags(no-parse));
};
destination d_loggly {
    tcp("logs.loggly.com" port(14791));
};
filter f_loggly { 
    facility(authpriv); 
};
log {
    source(s_all); filter(f_loggly); destination(d_loggly);
};


[[misc]]
== How to write to a local socket?

Friday, August 27th, 2010 +
One member of the rsyslog comunity wrote:

I’d like to forward via a local UNIX domain socket, instead. I think  I understand how to configure the ‘imuxsock’ module so my unprivileged instance reads from a non-standard socket location. But I can’t figure out how to tell my root instance to forward via a local domain socket.

I didn’t figure out a completely RSyslog-native method, but another poster’s message pointed me toward ‘socat’ and ‘omprog’, which I have working, now. (It would be really nice if RSyslog could support this natively, though.)

In case anyone else wants to set this up, maybe this will save you some effort. I’m also interested in any comments/criticisms about this method, I’d love to hear suggestions for better ways to make this work.

Also, I rolled it all up into a Fedora/EL RPM spec, and I’ll send it on to anyone who’s interested–just ask.

Setup steps:

Install the ‘socat’ utility.
Build RSyslog with the `–enable-omprog` ./configure flag.
Create two separate RSyslog config files, one for the ‘root’ instance (writes to the socket) and a 
second for the ‘unprivileged’ instance (reads from the socket).
Rewrite your RSyslog init script to start two separate daemon instances, one using each config file
(and separate PID files, too).
Create the user ‘rsyslogd’ and the group ‘rsyslogd’.
Set permissions/ownerships as needed to allow the user ‘rsyslogd’ to write to the file ‘/var/log/rsyslog.log’
Create an executable script called '/usr/libexec/rsyslogd/omprog_socat' that contains the lines:

    #!/bin/bash
    /usr/bin/socat -t0 -T0 -lydaemon -d - UNIX-SENDTO:/dev/log
    
The ‘root’ instance config file should contain (modifying the output actions to taste):

    $ModLoad imklog
    $ModLoad omprog
    $Template FwdViaUNIXSocket,"<%pri%>%syslogtag%%msg%"
    $ActionOMProgBinary /usr/libexec/rsyslogd/omprog_socat
    *.* :omprog:;FwdViaUNIXSocket
    
The ‘unprivileged’ instance config file should contain (modifying the output actions to taste):

    $ModLoad imuxsock
    $PrivDropToUser rsyslogd
    $PrivDropToGroup rsyslogd
    *.* /var/log/rsyslog.log

The ‘root’ daemon can only accept input from the kernel message buffer, and nothing else 
(especially not the syslog socket (/dev/log) or any network sockets). The unprivileged user
will handle all of local and network log messages. To merge the kernel logs into the same 
data channel as everything else, here’s what happens:

[During the RSyslog daemons' startup]

A) At startup, the ‘root’ daemon’s ‘imklog’ module starts listening for kernel messages 
(via ‘/prog/kmsg’), and its ‘omprog’ module starts an instance of ‘socat’ (called via the 
‘omprog_socat’ wrapper), establishing a persistent one-way IO connection where ‘omprog’ 
pipes its output to the STDIN of ‘socat’.

(Note that this same ‘socat’ instance remains running throughout the life of the RSyslog daemon, 
handling everything ‘omprog’ outputs. Contrast this, efficiency-wise, against the built-in ‘subshell’ 
module [the '^/path/to/program' action], which runs a separate instance instance of the child program 
for each message.)

B) At startup, the ‘unprivileged’ daemon’s ‘imuxsock’ module opens the system logging socket 
(‘/dev/log’) and starts listening for incoming log messages from other programs.

[During normal operation]1) The kernel buffer produces a message string on ‘/proc/kmsg’.2) 
The ‘root’ RSyslog daemon reads the message from ‘/proc/kmsg’, assigning it the priority number 
of ‘kern.info’ and the string tag ‘kernel’.3) The ‘root’ daemon prepends the priority number and 
tag as a header to the message string, and then passes it to the ‘omprog’ module for output 
(via persistent pipe) to the running ‘socat’ instance.4) The ‘socat’ instance receives the 
header-framed message and sends it to the system logging socket (‘/dev/log’).

5) The ‘unprivileged’ RSyslog daemon reads the message from ‘/dev/log’, assigning it the priority 
and tag given in the message header, plus all of the other properties (timestamp, hostname, etc.) 
a message object should have.

6) The ‘unprivileged’ daemon formats the message and writes it to the output file.

The only real difference I can see in the forwarded messages is that the ‘source’ property is set 
to ‘imuxsock’ instead of ‘imklog’. I don’t think that’s a real problem, though, since the priority 
and tag are still distinct.


[[normalization]]
== Normalization Sample

[source]
----
# this is a config sample for log normalization, but can
# be used as a more complex general sample.
# It is based on a plain standard rsyslog.conf for Red Hat systems.
# 
# NOTE: Absolute path names for modules are used in this config
# so that we can run a different rsyslog version alongside the
# regular system-installed rsyslogd. Remove these path names
# for production environment.

#### MODULES ####

# we do not run imuxsock as we don't want to mess with the main system logger
#module(load="/home/rger/proj/rsyslog/plugins/imuxsock/.libs/imuxsock") # provides support for local system logging (e.g. via logger command)
#module(load="imklog")   # provides kernel logging support (previously done by rklogd)
module(load="/home/rger/proj/rsyslog/plugins/imudp/.libs/imudp")  # Provides UDP syslog reception
module(load="/home/rger/proj/rsyslog/plugins/imtcp/.libs/imtcp")
module(load="/home/rger/proj/rsyslog/plugins/mmjsonparse/.libs/mmjsonparse")
module(load="/home/rger/proj/rsyslog/plugins/mmnormalize/.libs/mmnormalize")

/* We assume to have all TCP logging (for simplicity)
 * Note that we use different ports to point different sources
 * to the right rule sets for normalization. While there are
 * other methods (e.g. based on tag or source), using multiple
 * ports is both the easiest as well as the fastest.
 */
input(type="imtcp" port="13514" Ruleset="WindowsRsyslog")
input(type="imtcp" port="13515" Ruleset="LinuxPlainText")
input(type="imtcp" port="13516" Ruleset="WindowsSnare")

#debug:
action(type="omfile" file="/home/rger/proj/rsyslog/logfile")

/* This ruleset handles structured logging.
 * It is the only one ever called for remote machines
 * but executed in addition to the standard action for
 * the local machine. The ultimate goal is to forward
 * to some Vendor's analysis tool (which digests a
 * structured log format, here we use Lumberjack).
 */
template(name="lumberjack" type="string" string="%$!all-json%\n")


/* the rsyslog Windows Agent uses native Lumberjack format
 * (better said: is configured to use it)
 */
ruleset(name="WindowsRsyslog") {
    action(type="mmjsonparse")
    if $parsesuccess == "OK" then {
        if $!id == 4634 then
            set $!usr!type = "logoff";
        else if $!id == 4624 then
            set $!usr!type = "logon";
        set $!usr!rcvdfrom = $!source;
        set $!usr!rcvdat = $timereported;
        set $!usr!user = $!TargetDomainName & "\\" & $!TargetUserName;
        call outwriter
    }
}

/* This handles clumsy snare format. Note that "#011" are
 * the escape sequences for tab chars used by snare.
 */
ruleset(name="WindowsSnare") {
    set $!usr!type = field($rawmsg, "#011", 6);
    if $!usr!type == 4634 then {
        set $!usr!type = "logoff";
        set $!doProces = 1;
    } else if $!usr!type == 4624 then {
        set $!usr!type = "logon";
        set $!doProces = 1;
    } else
        set $!doProces = 0;
    if $!doProces == 1 then {
        set $!usr!rcvdfrom = field($rawmsg, 32, 4);
        set $!usr!rcvdat = field($rawmsg, "#011", 5);
        /* we need to fix up the snare date */
        set $!usr!rcvdat = field($!usr!rcvdat, 32, 2) & " " &
                   field($!usr!rcvdat, 32, 3) & " " &
                   field($!usr!rcvdat, 32, 4);
        set $!usr!user = field($rawmsg, "#011", 8);
        call outwriter
    }
}

/* plain Linux log messages (here: ssh and sudo) need to be
 * parsed - we use mmnormalize for fast and efficient parsing
 * here.
 */
ruleset(name="LinuxPlainText") {
    action(type="mmnormalize"
               rulebase="/home/rger/proj/rsyslog/linux.rb" userawmsg="on")
    if $parsesuccess == "OK" and $!user != "" then {
        if $!type == "opened" then
            set $!usr!type = "logon";
        else if $!type == "closed" then
            set $!usr!type = "logoff";
        set $!usr!rcvdfrom = $!rcvdfrom;
        set $!usr!rcvdat = $!rcvdat;
        set $!usr!user = $!user;
        call outwriter
    }
}

/* with CSV, we the reader must receive information on the
 * field names via some other method (e.g. tool configuration,
 * prepending of a header to the written CSV-file). All of
 * this is highly dependant on the actual CSV dialect needed.
 * Below, we cover the basics.
 */
template(name="csv" type="list") {
    property(name="$!usr!rcvdat" format="csv")
    constant(value=",")
    property(name="$!usr!rcvdfrom" format="csv")
    constant(value=",")
    property(name="$!usr!user" format="csv")
    constant(value=",")
    property(name="$!usr!type" format="csv")
    constant(value="\n")
}

/* template for Lumberjack-style logging. Note that the extra
 * LF at the end is just for wrinting it to file - it MUST NOT
 * be included for messages intended to be sent to a remote system.
 * For the latter use case, the syslog header must also be prepended,
 * something we have also not done for simplicity (as we write to files).
 * Note that we use a JSON-shortcut: If a tree name is specified, JSON
 * for its whole subtree is generated. Thus, we only need to specify the
 * $!usr top node to get everytihing we need.
 */
template(name="cee" type="string" string="@cee: %$!usr%\n")


/* this ruleset simulates forwarding to the final destination */
ruleset(name="outwriter"){
    action(type="omfile"
               file="/home/rger/proj/rsyslog/logfile.csv" template="csv")
    action(type="omfile"
               file="/home/rger/proj/rsyslog/logfile.cee" template="cee")
}


/* below is just the usual "uninteresting" stuff...
 * Note that this goes into the default rule set. So 
 * local logging is handled "as usual" without the need
 * for any extra effort.
 */


#### GLOBAL DIRECTIVES ####

# Use default timestamp format
$ActionFileDefaultTemplate RSYSLOG_TraditionalFileFormat

# Include all config files in /etc/rsyslog.d/
# commented out not to interfere with the system rsyslogd
# (just for this test configuration!)
#$IncludeConfig /etc/rsyslog.d/*.conf


#### RULES ####

# Log all kernel messages to the console.
# Logging much else clutters up the screen.
#kern.*                                                 /dev/console

# Log anything (except mail) of level info or higher.
# Don't log private authentication messages!
*.info;mail.none;authpriv.none;cron.none                /var/log/messages

# The authpriv file has restricted access.
authpriv.*                                              /var/log/secure

# Log all the mail messages in one place.
mail.*                                                  /var/log/maillog


# Log cron stuff
cron.*                                                  /var/log/cron

# Everybody gets emergency messages
*.emerg                                                 :omusrmsg:*

# Save news errors of level crit and higher in a special file.
uucp,news.crit                                          /var/log/spooler

# Save boot messages also to boot.log
local7.*                                                /var/log/boot.log
----



[[omelasticsearch]]
== Elasticsearch Output Module

This module provides native support for logging to <a href="http://www.elasticsearch.org/">Elasticsearch</a>.</p>

**Action Parameters:**

* **server**    
Host name or IP address of the Elasticsearch server. Defaults to "localhost"</li>
* **serverport**    
HTTP port to connect to Elasticsearch. Defaults to 9200</li>
* **searchIndex**    
<a href="http://www.elasticsearch.org/guide/appendix/glossary.html#index">Elasticsearch index</a> to send your logs to. Defaults to "system"
* **dynSearchIndex** on off    
Whether the string provided for <strong>searchIndex</strong> should be taken as a <a href="http://www.rsyslog.com/doc/rsyslog_conf_templates.html">template</a>. Defaults to "off", which means the index name will be taken literally. Otherwise, it will look for a template with that name, and the resulting string will be the index name. For example, let's assume you define a template named "date-days" containing "%timereported:1:10:date-rfc3339%". Then, with dynSearchIndex="on", if you say searchIndex="date-days", each log will be sent to and index named after the first 10 characters of the timestamp, like "2013-03-22".

* **searchType**
<a href="http://www.elasticsearch.org/guide/appendix/glossary.html#type">Elasticsearch type</a> to send your index to. 
Defaults to "events"

* **dynSearchType** <on|**off**>
Like <strong>dynSearchIndex</strong>, it allows you to specify a <a href="http://www.rsyslog.com/doc/rsyslog_conf_templates.html">template</a> for <strong>searchType</strong>, instead of a static string.

* **asyncrepl** <on|**off>
By default, an indexing operation returns after 
all <a href="http://www.elasticsearch.org/guide/appendix/glossary.html#replica_shard">replica shards</a> 
have indexed the document. With asyncrepl="on" it will return after it was indexed on 
the <a href="http://www.elasticsearch.org/guide/appendix/glossary.html#primary_shard">primary shard</a> 
only - thus trading some consistency for speed.
* **timeout**
How long Elasticsearch will wait for a primary shard to be available for indexing your log before sending 
back an error. Defaults to "1m".
* **template**
This is the JSON document that will be indexed in Elasticsearch. 
The resulting string needs to be a valid JSON, otherwise Elasticsearch will return an error. Defaults to:

        <pre>$template JSONDefault, "{\"message\":\"%msg:::json%\",\"fromhost\":\"%HOSTNAME:::json%\",\"facility\":\"%syslogfacility-text%\",\"priority\":\"%syslogpriority-text%\",\"timereported\":\"%timereported:::date-rfc3339%\",\"timegenerated\":\"%timegenerated:::date-rfc3339%\"}"
</pre>

<p>Which will produce this sort of documents (pretty-printed here for readability):</p>


<pre>{
&nbsp;&nbsp;&nbsp; "message": " this is a test message",
&nbsp;&nbsp;&nbsp; "fromhost": "test-host",
&nbsp;&nbsp;&nbsp; "facility": "user",
&nbsp;&nbsp;&nbsp; "priority": "info",
&nbsp;&nbsp;&nbsp; "timereported": "2013-03-12T18:05:01.344864+02:00",
&nbsp;&nbsp;&nbsp; "timegenerated": "2013-03-12T18:05:01.344864+02:00"
}</pre>

* **bulkmode** <on|**off**>
The default "off" setting means logs are shipped one by one. Each in its own HTTP request, using the <a href="http://www.elasticsearch.org/guide/reference/api/index_.html">Index API</a>. Set it to "on" and it will use Elasticsearch's <a href="http://www.elasticsearch.org/guide/reference/api/bulk.html">Bulk API</a> to send multiple logs in the same request. The maximum number of logs sent in a single bulk request depends on your queue settings - usually limited by the <a href="http://www.rsyslog.com/doc/node35.html">dequeue batch size</a>. More information about queues can be found <a href="http://www.rsyslog.com/doc/node32.html">here</a>.</li>
            <li>
                <strong>parent</strong><br>
                Specifying a string here will index your logs with that string the parent ID of those logs. Please note that you need to define the <a href="http://www.elasticsearch.org/guide/reference/mapping/parent-field.html">parent field</a> in your <a href="http://www.elasticsearch.org/guide/reference/mapping/">mapping</a> for that to work. By default, logs are indexed without a parent.</li>
            <li>
                <strong>dynParent </strong>&lt;on/<strong>off</strong>&gt;<br>
                Using the same parent for all the logs sent in the same action is quite unlikely. So you'd probably want to turn this "on" and specify a <a href="http://www.rsyslog.com/doc/rsyslog_conf_templates.html">template</a> that will provide meaningful parent IDs for your logs.</li>
            <li>
                <strong>uid</strong><br>
                If you have basic HTTP authentication deployed (eg: through the <a href="https://github.com/Asquera/elasticsearch-http-basic">elasticsearch-basic plugin</a>), you can specify your user-name here.</li>
            <li>
                <strong>pwd</strong><br>
                Password for basic authentication.</li>
        </ul>
        <p>
            <b>Samples:</b></p>
        <p>
            The following sample does the following:</p>
        <ul>
            <li>
                loads the omelasticsearch module</li>
            <li>
                outputs all logs to Elasticsearch using the default settings</li>
        </ul>
        <pre>module(load="omelasticsearch")
*.*     action(type="omelasticsearch")</pre>
        <p>
            The following sample does the following:</p>
        <ul>
            <li>
                loads the omelasticsearch module</li>
            <li>
                defines a template that will make the JSON contain the following properties (more info about what properties you can use <a href="http://www.rsyslog.com/doc/property_replacer.html">here</a>):
                <ul>
                    <li>
                        RFC-3339 timestamp when the event was generated</li>
                    <li>
                        the message part of the event</li>
                    <li>
                        hostname of the system that generated the message</li>
                    <li>
                        severity of the event, as a string</li>
                    <li>
                        facility, as a string</li>
                    <li>
                        the tag of the event</li>
                </ul>
            </li>
            <li>
                outputs to Elasticsearch with the following settings
                <ul>
                    <li>
                        host name of the server is myserver.local</li>
                    <li>
                        port is 9200</li>
                    <li>
                        JSON docs will look as defined in the template above</li>
                    <li>
                        index will be "test-index"</li>
                    <li>
                        type will be "test-type"</li>
                    <li>
                        activate bulk mode. For that to work effectively, we use an in-memory queue that can hold up to 5000 events. The maximum bulk size will be 300</li>
                    <li>
                        retry indefinitely if the HTTP request failed (eg: if the target server is down)</li>
                </ul>
            </li>
        </ul>
        <pre>module(load="omelasticsearch")
template(name="testTemplate"
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; type="list"
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; option.json="on") {
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; constant(value="{")
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; constant(value="\"timestamp\":\"")&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; property(name="timereported" dateFormat="rfc3339")
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; constant(value="\",\"message\":\"")&nbsp;&nbsp;&nbsp;&nbsp; property(name="msg")
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; constant(value="\",\"host\":\"")&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; property(name="hostname")
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; constant(value="\",\"severity\":\"")&nbsp;&nbsp;&nbsp; property(name="syslogseverity-text")
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; constant(value="\",\"facility\":\"")&nbsp;&nbsp;&nbsp; property(name="syslogfacility-text")
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; constant(value="\",\"syslogtag\":\"")&nbsp;&nbsp; property(name="syslogtag")
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; constant(value="\"}")
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }
*.* action(type="omelasticsearch"
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; server="myserver.local"
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; serverport="9200"
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; template="testTemplate"
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; searchIndex="test-index"
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; searchType="test-type"
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; bulkmode="on"
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; queue.type="linkedlist"
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; queue.size="5000"
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; queue.dequeuebatchsize="300"
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; action.resumeretrycount="-1")</pre>



[[omrelp]]
== RELP Output Module (omrelp)

#### Description ####

This module supports sending syslog messages over the reliable RELP protocol.    
For RELP's advantages over plain tcp syslog, please see the documentation for imrelp (the server counterpart). 

#### Setup ####
Please note that librelp is required for imrelp (it provides the core relp protocol implementation).

#### Action Configuration Parameters ####

* **target** (mandatory)    
The target server to connect to.

* **template** (not mandatory, default "RSYSLOG_ForwardFormat")    
Defines the template to be used for the output.

* **timeout** (not mandatory, default 90)    
Timeout for relp sessions. If set too low, valid sessions may be considered dead and tried to recover.

* **windowSize** (not mandatory, default 0)    
This is an expert parameter. It permits to override the RELP window size being used by the client. 
Changing the window size has both an effect on performance as well as potential message duplication 
in failure case. A larger window size means more performance, but also potentially more duplicated 
messages - and vice versa. The default 0 means that librelp's default window size is being used, 
which is considered a compromise between goals reached.    
For your information: at the time of this 
writing, the librelp default window size is 128 messages, but this may change at any time.    
Note that there is no equivalent server parameter, as the client proposes and manages the window 
size in RELP protocol.

* **tls** (not mandatory, values "on","off", default "off")    
If set to "on", the RELP connection will be encrypted by TLS, so that the data is protected 
against observers. Please note that both the client and the server must have set TLS to 
either "on" or "off". Other combinations lead to unpredictable results.

* **tls.compression** (not mandatory, values "on","off", default "off")    
The controls if the TLS stream should be compressed (zipped). While this increases CPU use, 
the network bandwidth should be reduced. Note that typical text-based log records usually 
compress rather well.

* **tls.permittedPeer** peer    
Places access restrictions on this forwarder. Only peers which have been listed in this parameter 
may be connected to. This guards against rouge servers and man-in-the-middle attacks. The 
validation bases on the certficate the remote peer presents.
The peer parameter lists permitted certificate fingerprints. Note that it is an array parameter, 
so either a single or multiple fingerprints can be listed. When a non-permitted peer is connected to, 
the refusal is logged together with it's fingerprint. So if the administrator knows this was a 
valid request, he can simple add the fingerprint by copy and paste from the logfile to rsyslog.conf. 
It must be noted, though, that this situation should usually not happen after initial client setup 
and administrators should be alert in this case.    
Note that usually a single remote peer should be all that is ever needed. Support for multiple 
peers is primarily included in support of load balancing scenarios. If the connection goes to a 
specific server, only one specific certificate is ever expected (just like when connecting to a 
specific ssh server).    
To specify multiple fingerprints, just enclose them in braces like this:    
`tls.permittedPeer=["SHA1:...1", "SHA1:....2"]`     
To specify just a single peer, you can either specify the string directly or enclose it in braces.

* **tls.authMode** mode    
Sets the mode used for mutual authentication. Supported values are either "fingerprint" or "name".   
      
    Fingerprint mode basically is what SSH does. It does not require a full PKI to be present, instead 
self-signed certs can be used on all peers. Even if a CA certificate is given, the validity of the 
peer cert is NOT verified against it. Only the certificate fingerprint counts.    
    
    In "name" mode, certificate validation happens. Here, the matching is done against the certificate's 
subjectAltName and, as a fallback, the subject common name. If the certificate contains multiple names, 
a match on any one of these names is considered good and permits the peer to talk to rsyslog.

* **tls.prioritystring** (not mandatory, string)    
This parameter permits to specify the so-called "priority string" to GnuTLS. This string gives 
complete control over all crypto parameters, including compression setting. For this reason, 
when the prioritystring is specified, the "tls.compression" parameter has no effect and is ignored.     
Full information about how to construct a priority string can be found in the GnuTLS manual. 
At the time of this writing, this information was contained in section 6.10 of the GnuTLS manual.    
Note: this is an expert parameter. Do not use if you do not exactly know what you are doing.

#### Sample ####

The following sample sends all messages to the central server "centralserv" at port 2514 (note that that 
server must run imrelp on port 2514).

[source]
----
module(load="omrelp")
action(type="omrelp" target="centralserv" port="2514")
----


[[openquefile]]
== why disk-assisted queues keep a file open

From time to time, someone asks why rsyslog disk-assisted queues keep one file open 
until shutdown.  So it probably is time to elaborate a bit about it.

Let's start with explaining what can be seen: if a disk-assisted queue is configured, 
rsyslog will normally use the in-memory queue. It will open a disk queue only if there is good reason to do so (because it severely hurts performance). The prime reason to go to disk is when the in memory queue's configured size has been exhausted. Once this happens, rsyslog begins to create spool files, numbered consequtively. This should normally happen in cases where e.g. a remote target is temporarily not reachable or a database engine is responding extremely slow.  Once the situation has been cleared, rsyslog clears out the spool files. However, one spool file always is kept until you shut rsyslog down.

This is expected behaviour, and there is good reason for it. This is what happens technically:

A DA queue is actually "two queues in one": It is a regular in-memory queue, which has a (second) helper disk queue in case it neeeds it. As long as operations run smoothly, the disk queue is never used. When the system starts up, only the in-memory queue is started. Startup of the disk queue is deferred until it is actually needed (as in most cases it will never be needed).

When the in-memory queue runs out of space, it starts that Disk queue, which than allocates its queue file. In order to reclaim space, not a single file is written but a series of files, where old files are deleted when they are processed and new files are created on an as-needed basis. Initially, there is only one file, which is read and written. And if the queue is empty, this single file still exists, because it is the representation of a disk queue (like the in-memory mapping tables for memory queues always exist, which just cannot be seen by the user).

So what happens in the above case is that the disk queue is created, put into action (the part where it writes and deletes files) and is then becoming idle and empty. At that stage, it will keep its single queue file, which holds the queue's necessary mappings.

One may now ask "why not shut down the disk queue if no longer needed"? The short answer is that we anticpiate that it will be re-used and thus we do not make the effort to shut it down and restart when the need again arises. Let me elaborate: experience tells that when a system needs the disk queue once, it is highly likely to need it again in the future. The reason for disk queues to kick into action are often cyclic, like schedule restarts of remote systems or database engines (as part of a backup process, for example). So we assume if we used it once, we will most probably need it again and so keep it ready. This also helps reduce potential message loss during the switchover process to disk - in extreme cases this can happen if there is high traffic load and slim in-memory queues (remember that starting up a disk queue needs comparativley long).

The longer answer is that in the past rsyslog tried to shut down disk queues to "save" ressources. Experience told us that this "saving" often resulted in resource wasting. Also, the need to synchronize disk queue shutdown with the main queue was highly complex (just think about cases where we shut it down at the same time the in-memory queue actually begins to need it again!). This caused quite some overhead even when the disk queue was not used (again, this is the case most of the time - if properly sized). An indication of this effect probably is that we could remove roughly 20% of rsyslog's queue code when we removed the "disk queue shutdown" feature!

Bottom line: keeping the disk queue active once it has been activated is highly desirable and as such seeing a queue file around until shutdown is absolutely correct. Many users will even never see this spool file, because there are no exceptional circumstances that force the queue to go to disk. So they may be surprised if it happens every now and then.

Side-Note: if disk queue files are created without a traget going offline, one should consider increasing the in-memory queue size, as disk queues are obviouly much less efficient than memory queues.


[[queanalogy]]
== Turning Lanes and Rsyslog Queues - an Analogy

If there is a single object absolutely vital to understanding the way rsyslog works, this object is queues. Queues offer a variety of services, including support for multithreading. While there is elaborate in-depth documentation on the ins and outs of rsyslog queues, some of the concepts are hard to grasp even for experienced people. I think this is because rsyslog uses a very high layer of abstraction which includes things that look quite unnatural, like queues that do not actually queue...

With this document, I take a different approach: I will not describe every specific detail of queue operation but hope to be able to provide the core idea of how queues are used in rsyslog by using an analogy. I will compare the rsyslog data flow with real-life traffic flowing at an intersection.

But first let's set the stage for the rsyslog part. The graphic below describes the data flow inside rsyslog:

![rsyslog data flow](http://www.rsyslog.com/doc/dataflow.png "rsyslog data flow")

Note that there is a video tutorial available on the data flow. It is not perfect, but may aid in understanding this picture.

For our needs, the important fact to know is that messages enter rsyslog on "the left side" (for example, via UDP), are being preprocessed, put into the so-called main queue, taken off that queue, be filtered and be placed into one or several action queues (depending on filter results). They leave rsyslog on "the right side" where output modules (like the file or database writer) consume them.

So there are always two stages where a message (conceptually) is queued - first in the main queue and later on in n action specific queues (with n being the number of actions that the message in question needs to be processed by, what is being decided by the "Filter Engine"). As such, a message will be in at least two queues during its lifetime (with the exception of messages being discarded by the queue itself, but for the purpose of this document, we will ignore that possibility).

Also, it is vitally important to understand that each action has a queue sitting in front of it. If you have dug into the details of rsyslog configuration, you have probably seen that a queue mode can be set for each action. And the default queue mode is the so-called "direct mode", in which "the queue does not actually enqueue data". That sounds silly, but is not. It is an important abstraction that helps keep the code clean.

To understand this, we first need to look at who is the active component. In our data flow, the active part always sits to the left of the object. For example, the "Preprocessor" is being called by the inputs and calls itself into the main message queue. That is, the queue receiver is called, it is passive. One might think that the "Parser & Filter Engine" is an active component that actively pulls messages from the queue. This is wrong! Actually, it is the queue that has a pool of worker threads, and these workers pull data from the queue and then call the passively waiting Parser and Filter Engine with those messages. So the main message queue is the active part, the Parser and Filter Engine is passive.

Let's now try an analogy analogy for this part: Think about a TV show. The show is produced in some TV studio, from there sent (actively) to a radio tower. The radio tower passively receives from the studio and then actively sends out a signal, which is passively received by your TV set. In our simplified view, we have the following picture:

![rsyslog queues and TV analogy](http://www.rsyslog.com/doc/queue_analogy_tv.png "rsyslog queues and TV analogy")

The lower part of the picture lists the equivalent rsyslog entities, in an abstracted way. Every queue has a producer (in the above sample the input) and a consumer (in the above sample the Parser and Filter Engine). Their active and passive functions are equivalent to the TV entities that are listed on top of the rsyslog entity. For example, a rsyslog consumer can never actively initiate reception of a message in the same way a TV set cannot actively "initiate" a TV show - both can only "handle" (display or process) what is sent to them.

Now let's look at the action queues: here, the active part, the producer, is the Parser and Filter Engine. The passive part is the Action Processor. The later does any processing that is necessary to call the output plugin, in particular it processes the template to create the plugin calling parameters (either a string or vector of arguments). From the action queue's point of view, Action Processor and Output form a single entity. Again, the TV set analogy holds. The Output does not actively ask the queue for data, but rather passively waits until the queue itself pushes some data to it.

Armed with this knowledge, we can now look at the way action queue modes work. My analogy here is a junction, as shown below (note that the colors in the pictures below are not related to the colors in the pictures above!):



![cross](http://www.rsyslog.com/doc/direct_queue0.png)



This is a very simple real-life traffic case: one road joins another. We look at traffic on the straight road, here shown by blue and green arrows. Traffic in the opposing direction is shown in blue. Traffic flows without any delays as long as nobody takes turns. To be more precise, if the opposing traffic takes a (right) turn, traffic still continues to flow without delay. However, if a car in the red traffic flow intends to do a (left, then) turn, the situation changes:

![jjj](http://www.rsyslog.com/doc/direct_queue1.png)

The turning car is represented by the green arrow. It cannot turn unless there is a gap in the "blue traffic stream". And as this car blocks the roadway, the remaining traffic (now shown in red, which should indicate the block condition), must wait until the "green" car has made its turn. So a queue will build up on that lane, waiting for the turn to be completed. Note that in the examples below I do not care that much about the properties of the opposing traffic. That is, because its structure is not really important for what I intend to show. Think about the blue arrow as being a traffic stream that most of the time blocks left-turners, but from time to time has a gap that is sufficiently large for a left-turn to complete.

Our road network designers know that this may be unfortunate, and for more important roads and junctions, they came up with the concept of turning lanes:


![kk]http://www.rsyslog.com/doc/direct_queue2.png)




Now, the car taking the turn can wait in a special area, the turning lane. As such, the "straight" traffic is no longer blocked and can flow in parallel to the turning lane (indicated by a now-green-again arrow).

However, the turning lane offers only finite space. So if too many cars intend to take a left turn, and there is no gap in the "blue" traffic, we end up with this well-known situation:

![ll](http://www.rsyslog.com/doc/direct_queue3.png)

The turning lane is now filled up, resulting in a tailback of cars intending to left turn on the main driving lane. The end result is that "straight" traffic is again being blocked, just as in our initial problem case without the turning lane. In essence, the turning lane has provided some relief, but only for a limited amount of cars. Street system designers now try to weight cost vs. benefit and create (costly) turning lanes that are sufficiently large to prevent traffic jams in most, but not all cases.

Now let's dig a bit into the mathematical properties of turning lanes. We assume that cars all have the same length. So, units of cars, the length is alsways one (which is nice, as we don't need to care about that factor any longer ;)). A turning lane has finite capacity of n cars. As long as the number of cars wanting to take a turn is less than or eqal to n, "straigth traffic" is not blocked (or the other way round, traffic is blocked if at least n + 1 cars want to take a turn!). We can now find an optimal value for n: it is a function of the probability that a car wants to turn and the cost of the turning lane (as well as the probability there is a gap in the "blue" traffic, but we ignore this in our simple sample). If we start from some finite upper bound of n, we can decrease n to a point where it reaches zero. But let's first look at n = 1, in which case exactly one car can wait on the turning lane. More than one car, and the rest of the traffic is blocked. Our everyday logic indicates that this is actually the lowest boundary for n.

In an abstract view, however, n can be zero and that works nicely. There still can be n cars at any given time on the turning lane, it just happens that this means there can be no car at all on it. And, as usual, if we have at least n + 1 cars wanting to turn, the main traffic flow is blocked. True, but n + 1 = 0 + 1 = 1 so as soon as there is any car wanting to take a turn, the main traffic flow is blocked (remember, in all cases, I assume no sufficiently large gaps in the opposing traffic).

This is the situation our everyday perception calls "road without turning lane". In my math model, it is a "road with turning lane of size 0". The subtle difference is important: my math model guarantees that, in an abstract sense, there always is a turning lane, it may just be too short. But it exists, even though we don't see it. And now I can claim that even in my small home village, all roads have turning lanes, which is rather impressive, isn't it? ;)

And now we finally have arrived at rsyslog's queues! Rsyslog action queues exists for all actions just like all roads in my village have turning lanes! And as in this real-life sample, it may be hard to see the action queues for that reason. In rsyslog, the "direct" queue mode is the equivalent to the 0-sized turning lane. And actions queues are the equivalent to turning lanes in general, with our real-life n being the maximum queue size. The main traffic line (which sometimes is blocked) is the equivalent to the main message queue. And the periods without gaps in the opposing traffic are equivalent to execution time of an action. In a rough sketch, the rsyslog main and action queues look like in the following picture.



We need to read this picture from right to left (otherwise I would need to redo all the graphics ;)). In action 3, you see a 0-sized turning lane, aka an action queue in "direct" mode. All other queues are run in non-direct modes, but with different sizes greater than 0.

Let us first use our car analogy: Assume we are in a car on the main lane that wants to take turn into the "action 4" road. We pass action 1, where a number of cars wait in the turning lane and we pass action 2, which has a slightly smaller, but still not filled up turning lane. So we pass that without delay, too. Then we come to "action 3", which has no turning lane. Unfortunately, the car in front of us wants to turn left into that road, so it blocks the main lane. So, this time we need to wait. An observer standing on the sidewalk may see that while we need to wait, there are still some cars in the "action 4" turning lane. As such, even though no new cars can arrive on the main lane, cars still turn into the "action 4" lane. In other words, an observer standing in "action 4" road is unable to see that traffic on the main lane is blocked.

Now on to rsyslog: Other than in the real-world traffic example, messages in rsyslog can - at more or less the same time - "take turns" into several roads at once. This is done by duplicating the message if the road has a non-zero-sized "turning lane" - or in rsyslog terms a queue that is running in any non-direct mode. If so, a deep copy of the message object is made, that placed into the action queue and then the initial message proceeds on the "main lane". The action queue then pushes the duplicates through action processing. This is also the reason why a discard action inside a non-direct queue does not seem to have an effect. Actually, it discards the copy that was just created, but the original message object continues to flow.

In action 1, we have some entries in the action queue, as we have in action 2 (where the queue is slightly shorter). As we have seen, new messages pass action one and two almost instantaneously. However, when a messages reaches action 3, its flow is blocked. Now, message processing must wait for the action to complete. Processing flow in a direct mode queue is something like a U-turn:

message processing in an rsyslog action queue in direct mode

The message starts to execute the action and once this is done, processing flow continues. In a real-life analogy, this may be the route of a delivery man who needs to drop a parcel in a side street before he continues driving on the main route. As a side-note, think of what happens with the rest of the delivery route, at least for today, if the delivery truck has a serious accident in the side street. The rest of the parcels won't be delivered today, will they? This is exactly how the discard action works. It drops the message object inside the action and thus the message will no longer be available for further delivery - but as I said, only if the discard is done in a direct mode queue (I am stressing this example because it often causes a lot of confusion).

Back to the overall scenario. We have seen that messages need to wait for action 3 to complete. Does this necessarily mean that at the same time no messages can be processed in action 4? Well, it depends. As in the real-life scenario, action 4 will continue to receive traffic as long as its action queue ("turn lane") is not drained. In our drawing, it is not. So action 4 will be executed while messages still wait for action 3 to be completed.

Now look at the overall picture from a slightly different angle:

message processing in an rsyslog action queue in direct mode

The number of all connected green and red arrows is four - one each for action 1, 2 and 3 (this one is dotted as action 4 was a special case) and one for the "main lane" as well as action 3 (this one contains the sole red arrow). This number is the lower bound for the number of threads in rsyslog's output system ("right-hand part" of the main message queue)! Each of the connected arrows is a continuous thread and each "turn lane" is a place where processing is forked onto a new thread. Also, note that in action 3 the processing is carried out on the main thread, but not in the non-direct queue modes.

I have said this is "the lower bound for the number of threads...". This is with good reason: the main queue may have more than one worker thread (individual action queues currently do not support this, but could do in the future - there are good reasons for that, too but exploring why would finally take us away from what we intend to see). Note that you configure an upper bound for the number of main message queue worker threads. The actual number varies depending on a lot of operational variables, most importantly the number of messages inside the queue. The number t_m of actually running threads is within the integer-interval [0,confLimit] (with confLimit being the operator configured limit, which defaults to 5). Output plugins may have more than one thread created by themselves. It is quite unusual for an output plugin to create such threads and so I assume we do not have any of these. Then, the overall number of threads in rsyslog's filtering and output system is t_total = t_m + number of actions in non-direct modes. Add the number of inputs configured to that and you have the total number of threads running in rsyslog at a given time (assuming again that inputs utilize only one thread per plugin, a not-so-safe assumption).

A quick side-note: I gave the lower bound for t_m as zero, which is somewhat in contrast to what I wrote at the begin of the last paragraph. Zero is actually correct, because rsyslog stops all worker threads when there is no work to do. This is also true for the action queues. So the ultimate lower bound for a rsyslog output system without any work to carry out actually is zero. But this bound will never be reached when there is continuous flow of activity. And, if you are curios: if the number of workers is zero, the worker wakeup process is actually handled within the threading context of the "left-hand-side" (or producer) of the queue. After being started, the worker begins to play the active queue component again. All of this, of course, can be overridden with configuration directives.

When looking at the threading model, one can simply add n lanes to the main lane but otherwise retain the traffic analogy. This is a very good description of the actual process (think what this means to the "turning lanes"; hint: there still is only one per action!).

Let's try to do a warp-up: I have hopefully been able to show that in rsyslog, an action queue "sits in front of" each output plugin. Messages are received and flow, from input to output, over various stages and two level of queues to the outputs. Actions queues are always present, but may not easily be visible when in direct mode (where no actual queuing takes place). The "road junction with turning lane" analogy well describes the way - and intent - of the various queue levels in rsyslog.

On the output side, the queue is the active component, not the consumer. As such, the consumer cannot ask the queue for anything (like n number of messages) but rather is activated by the queue itself. As such, a queue somewhat resembles a "living thing" whereas the outputs are just tools that this "living thing" uses.

Note that I left out a couple of subtleties, especially when it comes to error handling and terminating a queue (you hopefully have now at least a rough idea why I say "terminating a queue" and not "terminating an action" - who is the "living thing"?). An action returns a status to the queue, but it is the queue that ultimately decides which messages can finally be considered processed and which not. Please note that the queue may even cancel an output right in the middle of its action. This happens, if configured, if an output needs more than a configured maximum processing time and is a guard condition to prevent slow outputs from deferring a rsyslog restart for too long. Especially in this case re-queuing and cleanup is not trivial. Also, note that I did not discuss disk-assisted queue modes. The basic rules apply, but there are some additional constraints, especially in regard to the threading model. Transitioning between actual disk-assisted mode and pure-in-memory-mode (which is done automatically when needed) is also far from trivial and a real joy for an implementer to work on ;).

If you have not done so before, it may be worth reading the rsyslog queue user's guide, which most importantly lists all the knobs you can turn to tweak queue operation.





[[queue]]
== Queues

Queues can be used both inside rulesets as well as for actions. 
As the underlying queue object is the same, a single set of property names is used to configure both use cases. 

For intuitive use, the keyword ``queue'' is used in front of all queue-related properties. 
For exmaple, "queue.size" specifies the maximum number of messages a queue may have. 

To configure an action queue, the queue settings can simply be specified inside the action object, as seen in the example below:

    action(type="omfile" 
           queue.size="10000" queue.type="linkedlist"
           file="/var/log/logfile")

This configures the action with an (asynchronous) linked-list based action queue which can hold a maximum of 10,000 messages. The queue properties can be used in similiar way to configure ruleset queues.

The following reference lists configuration parameters together with a brief description of their meaning. 
To fully understand queue modes and parameters, be sure to read the queue operation documentation.


<ul>
<li><a name="tex2html539" href="node33.html">queue.filename</a></li>
<li><a name="tex2html540" href="node34.html">queue.size</a></li>
<li><a name="tex2html541" href="node35.html">queue.dequeuebatchsize</a></li>
<li><a name="tex2html542" href="node36.html">queue.maxdiskspace</a></li>
<li><a name="tex2html543" href="node37.html">queue.highwatermark</a></li>
<li><a name="tex2html544" href="node38.html">queue.lowwatermark</a></li>
<li><a name="tex2html545" href="node39.html">queue.fulldelaymark</a></li>
<li><a name="tex2html546" href="node40.html">queue.discardmark</a></li>
<li><a name="tex2html547" href="node41.html">queue.discardseverity</a></li>
<li><a name="tex2html548" href="node42.html">queue.checkpointinterval</a></li>
<li><a name="tex2html549" href="node43.html">queue.syncqueuefiles</a></li>
<li><a name="tex2html550" href="node44.html">queue.type</a></li>
<li><a name="tex2html551" href="node45.html">queue.workerthreads</a></li>
<li><a name="tex2html552" href="node46.html">queue.timeoutshutdown</a></li>
<li><a name="tex2html553" href="node47.html">queue.timeoutactioncompletion</a></li>
<li><a name="tex2html554" href="node48.html">queue.timeoutenqueue</a></li>
<li><a name="tex2html555" href="node49.html">queue.timeoutworkerthreadshutdown</a></li>
<li><a name="tex2html556" href="node50.html">queue.workerthreadminimummessages</a></li>
<li><a name="tex2html557" href="node51.html">queue.maxfilesize</a></li>
<li><a name="tex2html558" href="node52.html">queue.saveonshutdown</a></li>
<li><a name="tex2html559" href="node53.html">queue.dequeueslowdown</a></li>
<li><a name="tex2html560" href="node54.html">queue.dequeuetimebegin</a></li>
<li><a name="tex2html561" href="node55.html">queue.dequeuetimeend</a></li>
</ul>


### Understanding Queues

Rsyslog uses queues whenever two activities need to be loosely coupled. With a queue, one part of the system "produces" something while another part "consumes" this something. The "something" is most often syslog messages, but queues may also be used for other purposes.

This document provides a good insight into technical details, operation modes and implications. In addition to it, an rsyslog queue concepts overview document exists which tries to explain queues with the help of some analogies. This may probably be a better place to start reading about queues. I assume that once you have understood that document, the material here will be much easier to grasp and look much more natural.

The most prominent example is the main message queue. Whenever rsyslog receives a message (e.g. locally, via UDP, TCP or in whatever else way), it places these messages into the main message queue. Later, it is dequeued by the rule processor, which then evaluates which actions are to be carried out. In front of each action, there is also a queue, which potentially de-couples the filter processing from the actual action (e.g. writing to file, database or forwarding to another host).


#### Where are Queues Used?

Currently, queues are used for the main message queue and for the actions.

There is a single main message queue inside rsyslog. Each input module delivers messages to it. The main message queue worker filters messages based on rules specified in rsyslog.conf and dispatches them to the individual action queues. Once a message is in an action queue, it is deleted from the main message queue.

There are multiple action queues, one for each configured action. By default, these queues operate in direct (non-queueing) mode. Action queues are fully configurable and thus can be changed to whatever is best for the given use case.

Future versions of rsyslog will most probably utilize queues at other places, too.

Wherever "<object>"  is used in the config file statements, substitute "<object>" with either "MainMsg" or "Action". The former will set main message queue parameters, the later parameters for the next action that will be created. Action queue parameters can not be modified once the action has been specified. For example, to tell the main message queue to save its content on shutdown, use $MainMsgQueueSaveOnShutdown on".

If the same parameter is specified multiple times before a queue is created, the last one specified takes precedence. The main message queue is created after parsing the config file and all of its potential includes. An action queue is created each time an action selector is specified. Action queue parameters are reset to default after an action queue has been created (to provide a clean environment for the next action).

Not all queues necessarily support the full set of queue configuration parameters, because not all are applicable. For example, in current output module design, actions do not support multi-threading. Consequently, the number of worker threads is fixed to one for action queues and can not be changed.

#### Queue Modes

Rsyslog supports different queue modes, some with submodes. Each of them has specific advantages and disadvantages. Selecting the right queue mode is quite important when tuning rsyslogd. The queue mode (aka "type") is set via the "$<object>QueueType" config directive.

### Direct Queues ###
Direct queues are non-queuing queues. A queue in direct mode does neither queue nor buffer any of the queue elements but rather passes the element directly (and immediately) from the producer to the consumer. This sounds strange, but there is a good reason for this queue type.

Direct mode queues allow to use queues generically, even in places where queuing is not always desired. A good example is the queue in front of output actions. While it makes perfect sense to buffer forwarding actions or database writes, it makes only limited sense to build up a queue in front of simple local file writes. Yet, rsyslog still has a queue in front of every action. So for file writes, the queue mode can simply be set to "direct", in which case no queuing happens.

Please note that a direct queue also is the only queue type that passes back the execution return code (success/failure) from the consumer to the producer. This, for example, is needed for the backup action logic. Consequently, backup actions require the to-be-checked action to use a "direct" mode queue.

To create a direct queue, use the "$<object>QueueType Direct" config directive.

### Disk Queues ###
Disk queues use disk drives for buffering. The important fact is that the always use the disk and do not buffer anything in memory. Thus, the queue is ultra-reliable, but by far the slowest mode. For regular use cases, this queue mode is not recommended. It is useful if log data is so important that it must not be lost, even in extreme cases.

When a disk queue is written, it is done in chunks. Each chunk receives its individual file. Files are named with a prefix (set via the "$<object>QueueFilename" config directive) and followed by a 7-digit number (starting at one and incremented for each file). Chunks are 10mb by default, a different size can be set via the"$<object>QueueMaxFileSize" config directive. Note that the size limit is not a sharp one: rsyslog always writes one complete queue entry, even if it violates the size limit. So chunks are actually a little but (usually less than 1k) larger then the configured size. Each chunk also has a different size for the same reason. If you observe different chunk sizes, you can relax: this is not a problem.

Writing in chunks is used so that processed data can quickly be deleted and is free for other uses - while at the same time keeping no artificial upper limit on disk space used. If a disk quota is set (instructions further below), be sure that the quota/chunk size allows at least two chunks to be written. Rsyslog currently does not check that and will fail miserably if a single chunk is over the quota.

Creating new chunks costs performance but provides quicker ability to free disk space. The 10mb default is considered a good compromise between these two. However, it may make sense to adapt these settings to local policies. For example, if a disk queue is written on a dedicated 200gb disk, it may make sense to use a 2gb (or even larger) chunk size.

Please note, however, that the disk queue by default does not update its housekeeping structures every time it writes to disk. This is for performance reasons. In the event of failure, data will still be lost (except when manually is mangled with the file structures). However, disk queues can be set to write bookkeeping information on checkpoints (every n records), so that this can be made ultra-reliable, too. If the checkpoint interval is set to one, no data can be lost, but the queue is exceptionally slow.

Each queue can be placed on a different disk for best performance and/or isolation. This is currently selected by specifying different $WorkDirectory config directives before the queue creation statement.

To create a disk queue, use the "$<object>QueueType Disk" config directive. Checkpoint intervals can be specified via "$<object>QueueCheckpointInterval", with 0 meaning no checkpoints. Note that disk-based queues can be made very reliable by issuing a (f)sync after each write operation. Starting with version 4.3.2, this can be requested via "<object>QueueSyncQueueFiles on/off with the default being off. Activating this option has a performance penalty, so it should not be turned on without reason.

### In-Memory Queues ###
In-memory queue mode is what most people have on their mind when they think about computing queues. Here, the enqueued data elements are held in memory. Consequently, in-memory queues are very fast. But of course, they do not survive any program or operating system abort (what usually is tolerable and unlikely). Be sure to use an UPS if you use in-memory mode and your log data is important to you. Note that even in-memory queues may hold data for an infinite amount of time when e.g. an output destination system is down and there is no reason to move the data out of memory (lying around in memory for an extended period of time is NOT a reason). Pure in-memory queues can't even store queue elements anywhere else than in core memory.

There exist two different in-memory queue modes: LinkedList and FixedArray. Both are quite similar from the user's point of view, but utilize different algorithms.

A FixedArray queue uses a fixed, pre-allocated array that holds pointers to queue elements. The majority of space is taken up by the actual user data elements, to which the pointers in the array point. The pointer array itself is comparatively small. However, it has a certain memory footprint even if the queue is empty. As there is no need to dynamically allocate any housekeeping structures, FixedArray offers the best run time performance (uses the least CPU cycle). FixedArray is best if there is a relatively low number of queue elements expected and performance is desired. It is the default mode for the main message queue (with a limit of 10,000 elements).

A LinkedList queue is quite the opposite. All housekeeping structures are dynamically allocated (in a linked list, as its name implies). This requires somewhat more runtime processing overhead, but ensures that memory is only allocated in cases where it is needed. LinkedList queues are especially well-suited for queues where only occasionally a than-high number of elements need to be queued. A use case may be occasional message burst. Memory permitting, it could be limited to e.g. 200,000 elements which would take up only memory if in use. A FixedArray queue may have a too large static memory footprint in such cases.

In general, it is advised to use LinkedList mode if in doubt. The processing overhead compared to FixedArray is low and may be outweigh by the reduction in memory use. Paging in most-often-unused pointer array pages can be much slower than dynamically allocating them.

To create an in-memory queue, use the "$<object>QueueType LinkedList" or  "$<object>QueueType FixedArray" config directive.

### Disk-Assisted Memory Queues ###
If a disk queue name is defined for in-memory queues (via $<object>QueueFileName), they automatically become "disk-assisted" (DA). In that mode, data is written to disk (and read back) on an as-needed basis.

Actually, the regular memory queue (called the "primary queue") and a disk queue (called the "DA queue") work in tandem in this mode. Most importantly, the disk queue is activated if the primary queue is full or needs to be persisted on shutdown. Disk-assisted queues combine the advantages of pure memory queues with those of  pure disk queues. Under normal operations, they are very fast and messages will never touch the disk. But if there is need to, an unlimited amount of messages can be buffered (actually limited by free disk space only) and data can be persisted between rsyslogd runs.

With a DA-queue, both disk-specific and in-memory specific configuration parameters can be set. From the user's point of view, think of a DA queue like a "super-queue" which does all within a single queue [from the code perspective, there is some specific handling for this case, so it is actually much like a single object].

DA queues are typically used to de-couple potentially long-running and unreliable actions (to make them reliable). For example, it is recommended to use a disk-assisted linked list in-memory queue in front of each database and "send via tcp" action. Doing so makes these actions reliable and de-couples their potential low execution speed from the rest of your rules (e.g. the local file writes). There is a howto on massive database inserts which nicely describes this use case. It may even be a good read if you do not intend to use databases.

With DA queues, we do not simply write out everything to disk and then run as a disk queue once the in-memory queue is full. A much smarter algorithm is used, which involves a "high watermark" and a "low watermark". Both specify numbers of queued items. If the queue size reaches high watermark elements, the queue begins to write data elements to disk. It does so until it reaches the low water mark elements. At this point, it stops writing until either high water mark is reached again or the on-disk queue becomes empty, in which case the queue reverts back to in-memory mode, only. While holding at the low watermark, new elements are actually enqueued in memory. They are eventually written to disk, but only if the high water mark is ever reached again. If it isn't, these items never touch the disk. So even when a queue runs disk-assisted, there is in-memory data present (this is a big difference to pure disk queues!).

This algorithm prevents unnecessary disk writes, but also leaves some additional buffer space for message bursts. Remember that creating disk files and writing to them is a lengthy operation. It is too lengthy to e.g. block receiving UDP messages. Doing so would result in message loss. Thus, the queue initiates DA mode, but still is able to receive messages and enqueue them - as long as the maximum queue size is not reached. The number of elements between the high water mark and the maximum queue size serves as this "emergency buffer". Size it according to your needs, if traffic is very bursty you will probably need a large buffer here. Keep in mind, though, that under normal operations these queue elements will probably never be used. Setting the high water mark too low will cause disk-assistance to be turned on more often than actually needed.

The water marks can be set via the "$<object>QueueHighWatermark" and  "$<object>QueueHighWatermark" configuration file directives. Note that these are actual numbers, not precentages. Be sure they make sense (also in respect to "$<object>QueueSize"), as rsyslodg does currently not perform any checks on the numbers provided. It is easy to screw up the system here (yes, a feature enhancement request is filed ;)).

## Limiting the Queue Size

All queues, including disk queues, have a limit of the number of elements they can enqueue. This is set via the "$<object>QueueSize" config parameter. Note that the size is specified in number of enqueued elements, not their actual memory size. Memory size limits can not be set. A conservative assumption is that a single syslog messages takes up 512 bytes on average (in-memory, NOT on the wire, this *is* a difference).

Disk assisted queues are special in that they do not have any size limit. The enqueue an unlimited amount of elements. To prevent running out of space, disk and disk-assisted queues can be size-limited via the "$<object>QueueMaxDiskSpace" configuration parameter. If it is not set, the limit is only available free space (and reaching this limit is currently not very gracefully handled, so avoid running into it!). If a limit is set, the queue can not grow larger than it. Note, however, that the limit is approximate. The engine always writes complete records. As such, it is possible that slightly more than the set limit is used (usually less than 1k, given the average message size). Keeping strictly on the limit would be a performance hurt, and thus the design decision was to favour performance. If you don't like that policy, simply specify a slightly lower limit (e.g. 999,999K instead of 1G).

In general, it is a good idea to limit the pysical disk space even if you dedicate a whole disk to rsyslog. That way, you prevent it from running out of space (future version will have an auto-size-limit logic, that then kicks in in such situations).

## Worker Thread Pools

Each queue (except in "direct" mode) has an associated pool of worker threads. Worker threads carry out the action to be performed on the data elements enqueued. As an actual sample, the main message queue's worker task is to apply filter logic to each incoming message and enqueue them to the relevant output queues (actions).

Worker threads are started and stopped on an as-needed basis. On a system without activity, there may be no worker at all running. One is automatically started when a message comes in. Similarily, additional workers are started if the queue grows above a specific size. The "$<object>QueueWorkerThreadMinimumMessages"  config parameter controls worker startup. If it is set to the minimum number of elements that must be enqueued in order to justify a new worker startup. For example, let's assume it is set to 100. As long as no more than 100 messages are in the queue, a single worker will be used. When more than 100 messages arrive, a new worker thread is automatically started. Similarily, a third worker will be started when there are at least 300 messages, a forth when reaching 400 and so on.

It, however, does not make sense to have too many worker threads running in parall. Thus, the upper limit ca be set via "$<object>QueueWorkerThreads". If it, for example, is set to four, no more than four workers will ever be started, no matter how many elements are enqueued.

Worker threads that have been started are kept running until an inactivity timeout happens. The timeout can be set via "$<object>QueueWorkerTimeoutThreadShutdown" and is specified in milliseconds. If you do not like to keep the workers running, simply set it to 0, which means immediate timeout and thus immediate shutdown. But consider that creating threads involves some overhead, and this is why we keep them running. If you would like to never shutdown any worker threads, specify -1 for this parameter.

## Discarding Messages

If the queue reaches the so called "discard watermark" (a number of queued elements), less important messages can automatically be discarded. This is in an effort to save queue space for more important messages, which you even less like to loose. Please note that whenever there are more than "discard watermark" messages, both newly incoming as well as already enqueued low-priority messages are discarded. The algorithm discards messages newly coming in and those at the front of the queue.

The discard watermark is a last resort setting. It should be set sufficiently high, but low enough to allow for large message burst. Please note that it take effect immediately and thus shows effect promptly - but that doesn't help if the burst mainly consist of high-priority messages...

The discard watermark is set via the "$<object>QueueDiscardMark" directive. The priority of messages to be discarded is set via "$<object>QueueDiscardSeverity". This directive accepts both the usual textual severity as well as a numerical one. To understand it, you must be aware of the numerical severity values. They are defined in RFC 3164:

        Numerical         Severity
          Code

           0       Emergency: system is unusable
           1       Alert: action must be taken immediately
           2       Critical: critical conditions
           3       Error: error conditions
           4       Warning: warning conditions
           5       Notice: normal but significant condition
           6       Informational: informational messages
           7       Debug: debug-level messages
           
Anything of the specified severity and (numerically) above it is discarded. To turn message discarding off, simply specify the discard watermark to be higher than the queue size. An alternative is to specify the numerical value 8 as DiscardSeverity. This is also the default setting to prevent unintentional message loss. So if you would like to use message discarding, you need to set" $<object>QueueDiscardSeverity" to an actual value.

An interesting application is with disk-assisted queues: if the discard watermark is set lower than the high watermark, message discarding will start before the queue becomes disk-assisted. This may be a good thing if you would like to switch to disk-assisted mode only in cases where it is absolutely unavoidable and you prefer to discard less important messages first.

## Filled-Up Queues

If the queue has either reached its configured maximum number of entries or disk space, it is finally full. If so, rsyslogd throttles the data element submitter. If that, for example, is a reliable input (TCP, local log socket), that will slow down the message originator which is a good resolution for this scenario.

During throtteling, a disk-assisted queue continues to write to disk and messages are also discarded based on severity as well as regular dequeuing and processing continues. So chances are good the situation will be resolved by simply throttling. Note, though, that throtteling is highly undesirable for unreliable sources, like UDP message reception. So it is not a good thing to run into throtteling mode at all.

We can not hold processing infinitely, not even when throtteling. For example, throtteling the local log socket too long would cause the system at whole come to a standstill. To prevent this, rsyslogd times out after a configured period ("$<object>QueueTimeoutEnqueue", specified in milliseconds) if no space becomes available. As a last resort, it then discards the newly arrived message.

If you do not like throtteling, set the timeout to 0 - the message will then immediately be discarded. If you use a high timeout, be sure you know what you do. If a high main message queue enqueue timeout is set, it can lead to something like a complete hang of the system. The same problem does not apply to action queues.

## Rate Limiting

Rate limiting provides a way to prevent rsyslogd from processing things too fast. It can, for example, prevent overruning a receiver system.

Currently, there are only limited rate-limiting features available. The "$<object>QueueDequeueSlowdown"  directive allows to specify how long (in microseconds) dequeueing should be delayed. While simple, it still is powerful. For example, using a DequeueSlowdown delay of 1,000 microseconds on a UDP send action ensures that no more than 1,000 messages can be sent within a second (actually less, as there is also some time needed for the processing itself).

Processing Timeframes
Queues can be set to dequeue (process) messages only during certain timeframes. This is useful if you, for example, would like to transfer the bulk of messages only during off-peak hours, e.g. when you have only limited bandwidth on the network path the the central server.

Currently, only a single timeframe is supported and, even worse, it can only be specified by the hour. It is not hard to extend rsyslog's capabilities in this regard - it was just not requested so far. So if you need more fine-grained control, let us know and we'll probably implement it. There are two configuration directives, both should be used together or results are unpredictable:" $<object>QueueDequeueTimeBegin <hour>" and "$<object>QueueDequeueTimeEnd <hour>". The hour parameter must be specified in 24-hour format (so 10pm is 22). A use case for this parameter can be found in the rsyslog wiki.

## Performance

The locking involved with maintaining the queue has a potentially large performance impact. How large this is, and if it exists at all, depends much on the configuration and actual use case. However, the queue is able to work on so-called "batches" when dequeueing data elements. With batches, multiple data elements are dequeued at once (with a single locking call). The queue dequeues all available elements up to a configured upper limit (<object>DequeueBatchSize <number>). It is important to note that the actual upper limit is dictated by availability. The queue engine will never wait for a batch to fill. So even if a high upper limit is configured, batches may consist of fewer elements, even just one, if there are no more elements waiting in the queue.

Batching can improve performance considerably. Note, however, that it affects the order in which messages are passed to the queue worker threads, as each worker now receive as batch of messages. Also, the larger the batch size and the higher the maximum number of permitted worker threads, the more main memory is needed. For a busy server, large batch sizes (around 1,000 or even more elements) may be useful. Please note that with batching, the main memory must hold BatchSize * NumOfWorkers objects in memory (worst-case scenario), even if running in disk-only mode. So if you use the default 5 workers at the main message queue and set the batch size to 1,000, you need to be prepared that the main message queue holds up to 5,000 messages in main memory in addition to the configured queue size limits!

The queue object's default maximum batch size is eight, but there exists different defaults for the actual parts of rsyslog processing that utilize queues. So you need to check these object's defaults.

Terminating Queues
Terminating a process sounds easy, but can be complex. Terminating a running queue is in fact the most complex operation a queue object can perform. You don't see that from a user's point of view, but its quite hard work for the developer to do everything in the right order.

The complexity arises when the queue has still data enqueued when it finishes. Rsyslog tries to preserve as much of it as possible. As a first measure, there is a regular queue time out ("$<object>QueueTimeoutShutdown", specified in milliseconds): the queue workers are given that time period to finish processing the queue.

If after that period there is still data in the queue, workers are instructed to finish the current data element and then terminate. This essentially means any other data is lost. There is another timeout ("$<object>QueueTimeoutActionCompletion", also specified in milliseconds) that specifies how long the workers have to finish the current element. If that timeout expires, any remaining workers are cancelled and the queue is brought down.

If you do not like to lose data on shutdown, the "$<object>QueueSaveOnShutdown" parameter can be set to "on". This requires either a disk or disk-assisted queue. If set, rsyslogd ensures that any queue elements are saved to disk before it terminates. This includes data elements there were begun being processed by workers that needed to be cancelled due to too-long processing. For a large queue, this operation may be lengthy. No timeout applies to a required shutdown save.


[[queueparam]]
== General Queue Parameters

Queues need to be configured in the action or ruleset it should affect. 
If nothing is configured, default values will be used. Thus, the default ruleset has 
only the default main queue. Specific Action queues are not set up by default.

* **queue.filename**  name

* **queue.size**  number

* **queue.dequeuebatchsize**  number    
    default 16
* **queue.maxdiskspace**  number    

* **queue.highwatermark**  number    
    default 8000
* **queue.lowwatermark**  number    
    default 2000
* **queue.fulldelaymark**  number    

* **queue.lightdelaymark**  number     

* **queue.discardmark**  number    
    default 9750
* **queue.discardseverity**  number    
    numerical severity default 8 (nothing discarded)
* **queue.checkpointinterval**  number    

* **queue.syncqueuefiles**  on/off    

* **queue.type**  [FixedArray/LinkedList/Direct/Disk]    

* **queue.workerthreads**  number    
    number of worker threads, default 1, recommended 1
* **queue.timeoutshutdown**  number    
    number is timeout in ms, default 0 (indefinite)
* **queue.timeoutactioncompletion**  number    
    number is timeout in ms, default 1000, 0 means immediate!
* **queue.timeoutenqueue**  number    
    number is timeout in ms, default 2000, 0 means indefinite
* **queue.timeoutworkerthreadshutdown**  number    
    number is timeout in ms, default 60000 (1 minute)
* **queue.workerthreadminimummessages**  number    
    default 100
* **queue.maxfilesize**  size_nbr    
    default 1m
* **queue.saveonshutdown**   on/off    

* **queue.dequeueslowdown**  number    
    number is timeout in microseconds, default 0 (no delay). Simple rate-limiting.
* **queue.dequeuetimebegin**  number    

* **queue.dequeuetimeend**  number    

**Sample:**

The following is a sample of a TCP forwarding action with its own queue.

[source]
----
    module (load="builtin:omfwd")
    *.* action(
            type="omfwd" 
            target="192.168.2.11"
            port="10514"
            protocol="tcp"
            queue.filename="forwarding"
            queue.size="1000000"
            queue.type="LinkedList"
        )
----



[[queueparams]]
== Queue Parameters


### queue.filename

Specifes the base name to be used for queue files.

> Available Since: 6.3.3    
> Format: word    
> Default: none    
> Mandatory: yes (for disk-based queues)    
     
Disk-based queues create a set of files for queue content. The value set via queue.filename 
acts as the basename to be used for filename creation. For actual log data, a number is 
appended to the file name. There is also a so-called "queue information" (qi) file created, 
which holds administrative information about the queue status. This file is named with the 
base name plus ".qi" as suffix.


### queue.size

Specifes the maximum number of (in-core) messages a queue can hold.

> Available Since: 6.3.3    
> Format: size    
> Default: 10,000 for ruleset queues, 1,000 for action queues    
> Mandatory: no    
     
This setting affects the in-memory queue size. Disk based queues may hold more data inside 
the queue, but not in main memory but on disk. The size is specified in number of messages. 
The representation of a typical syslog message object should require less than 1K, but excessively large messages may also result in excessively large objects. Note that not all message types may utilize the full queue. This depends on other queue parameters like the watermark settings. Most importantly, a small amount (seven percent) is reserved for messages with high loss potential (like UDP-received messages) and will not be utilized by messages with lower loss potential (like TCP-received messages).

Warning: do not set the size to extremely small values (like less than 500 messages) unless you know exactly what you do (and why!). This could interfere with other internal settings like watermarks and batch sizes. It is possible to specify very small values in order to support power users who customize the other settings accordingly. Usually there is no need to do that. Queues take only up memory when messages are stored in them. So reducing queue sizes does not reduce memory usage, except in cases where queues are actually full. The default settings permit small message bursts to be buffered without message loss.



### queue.dequeuebatchsize

Specifies how many messages can be dequeued at once.

> Available Since: 6.3.3    
> Format: number    
> Default:   
> Mandatory: no
     
Specifies the batch size for dequeue operations. This setting affects performance. As a rule of thumb, larger batch sizes (up to a environment-induced upper limit) provide better performance. For the average system, there usually should be no need to adjust batch sizes as the defaults are sufficient.


### queue.maxdiskspace

Specifies maximum amount of disk space a queue may use.

> Available Since: 6.3.3    
> Format: size    
> Default: unlimited    
> Mandatory: no
     
This setting permits to limit the maximum amount of disk space the queue data files will use. Note that actual disk allocation may be slightly larger due to block allocation. Also, no partial messages are written to queue, so writing a message is completed even if that means going slightly above the limit. Note that, contrary to queue.size, the size is specified in bytes and not messages. It is recommended to limit queue disk allocation, as otherwise the filesystem free space may be exhausted if the queue needs to grow very large.
If the size limit is hit, messages are discarded until sufficient messages have been dequeued and queue files been deleted


### queue.highwatermark

Specifies ...

Available Since: 6.3.3    
Format: number    
Default:    
Mandatory: no

### queue.lowwatermark

Specifies ... 

Available Since: 6.3.3    
Format: number    
Default:    
Mandatory: no

### queue.fulldelaymark

Specifies .

Available Since: 6.3.3    
Format: number    
Default:    
Mandatory: no


### queue.discardmark

Specifies

Available Since:    6.3.3
Format: number
Default:     
Mandatory:  no


### queue.discardseverity

Specifies

Available Since:    6.3.3
Format: severity
Default:     
Mandatory:  no

### queue.checkpointinterval

Specifies

Available Since:    6.3.3
Format: number
Default:     
Mandatory:  no


### queue.syncqueuefiles

Specifies

Available Since:    6.3.3
Format: binary
Default:     
Mandatory:  no

### queue.type

Specifies

Available Since:    6.3.3
Format: queue type
Default: LinkedList for ruleset queues, Direct for action queues
Mandatory:  no


### queue.workerthreads

Specifies

Available Since:    6.3.3
Format: number
Default:     
Mandatory:  no

### queue.timeoutshutdown

Specifies

Available Since:    6.3.3
Format: number
Default:     
Mandatory:  no


### queue.timeoutactioncompletion

Specifies

Available Since:    6.3.3
Format: number
Default:     
Mandatory:  no


### queue.timeoutenqueue

Specifies

Available Since:    6.3.3
Format: number
Default:     
Mandatory:  no


### queue.timeoutworkerthreadshutdown

Specifies

Available Since:    6.3.3
Format: number
Default:     
Mandatory:  no

### queue.workerthreadminimummessages

Specifies

Available Since:    6.3.3
Format: number
Default:     
Mandatory:  no


### queue.maxfilesize

Specifies

Available Since:    6.3.3
Format: size
Default:     
Mandatory:  no


### queue.saveonshutdown

Specifies

Available Since:    6.3.3
Format: binary
Default:    no
Mandatory:  no

### queue.dequeueslowdown

Specifies

Available Since:    6.3.3
Format: number
Default:     
Mandatory:  no

### queue.dequeuetimebegin

Specifies

Available Since:    6.3.3
Format: number
Default:     
Mandatory:  no

### queue.dequeuetimeend

Specifies

Available Since:    6.3.3
Format: number
Default:     
Mandatory:  no


[[queuesample]]
== Queues and omelasticsearch

The following text gives some explained configuration snippets about how you can use 
omelasticsearch to get your logs to Elasticsearch, and how to use queues in the process 
for enhancing performance and reliability.

First of all, some references about queues:
* general info: http://www.rsyslog.com/doc/queues.html
* v6-specific: http://www.rsyslog.com/doc/node32.html

But in short, queues are good for chaching logs when the recipient is not as fast as the 
sender. Or when it's down. Or, when you simply want to process them faster, because 
rsyslog can make batches of messages from the queue and process them in bulk.

You get a "MainMsgQueue", where rsyslog puts the received messages in order to be processed. 
And for each action, you can define a queue.

Queues can be in memory, on disk, or a smart combination of both, named Disk-Assisted. 
I don't like pure disk queues because they're slow, so I'll concentrate on the other two.

Elasticsearch is a distributed search engine build on top of Lucene. It's nice to collect 
your logs there in order to search them. At the time of this writing, omelasticsearch 
(rsyslog plugin for Elasticsearch) can be found in the latest beta - v6.3.12. A HOWTO on how 
to get it working can be found here: 
http://wiki.rsyslog.com/index.php/HOWTO:_rsyslog_%2B_elasticsearch


### Main message queue ###

With 6.3.12, you need to specify Main message queue options in the old, pre-v6 format. 
Here's an example of a disk-assisted queue here:

    # location for storing queue files on disk
    $WorkDirectory /var/lib/elasticsearch/rsyslog_queues
 
    main_queue {

        # file name template, also enables disk mode for the memory queue
        queue.filename="incoming_queue"
 
        # allocate memory dynamically for the queue. Better for handling spikes
        queue.type="LinkedList"
 
        # when to start discarding messages
        queue.discardMark=2000000
 
        # limit of messages in the memory queue. When this is reached, it starts to write to disk
        queue.highWaterMark=1000000
 
        # memory queue size at which it stops writing to the disk
        queue.lowWaterMark=800000
 
        # maximum disk space used for the disk part of the queue
        queue.maxfilesize="5g"
 
        # how many messages (messages, not bytes!) to hold in memory
        $queue.size=8000000
 
        # don't throttle receiving messages when the queue gets full
        queue.timeoutenqueue 0
 
        # save the queue contents when stopping rsyslog
        queue.saveonshutdown=on
    }

You can also use an in-memory queue here. The advantages are that it will be consistently good 
in terms of performance, and it will fail fast - things that you normally want in a distributed,
scalable system, where reliability isn't very important. The downside is that you can keep a 
limited number of messages in that queue, less than you normally can with a disk-assisted queue. 
Example config snippet:

    main_queue {

        # allocate memory dynamically for the queue. Better for handling spikes
        queue.type="LinkedList"
 
        # how many messages (messages, not bytes!) to hold in memory
        $queue.size=8000000
 
        # don't throttle receiving messages when the queue gets full
        queue.timeoutenqueue=0
    }


### Action queue ###

For each action you can define a queue. And you can do that with the new format. 
Here's a config snippet that you might find useful for using disk assisted queues 
with omelasticsearch:

    template(name="miniSchema"  type="list" option.json="on") {

        #- open the curly brackets,
        constant(value=”{")

        #- add the timestamp field surrounded with quotes
        #- add the colon which separates field from value
        #- open the quotes for the timestamp itself
        constant(value="\”timestamp\”:\”")
        #- add the timestamp from the log,
        # format it in RFC-3339, so that ES detects it by default
        property(name=”timereported” dateFormat=”rfc3339″ outname="timestamp")

        #- close the quotes for timestamp,
        #- add a comma, then the syslogtag field in the same manner
        constant(value="\",\”syslogtag\”:\”")




        constant(value="{\"message\":\"")
        property(name=%msg:::json%\",
        \"host\":\"%HOSTNAME:::json%\",
        \"severity\":\"
        %syslogseverity%
        \",\"date\":\"
        %timereported:1:19:date-rfc3339%
        .
        %timereported:1:3:date-subseconds%
        \",\"tag\":\"
        %syslogtag:::json%
        \"}"
        constant(value=”\n”) #we’ll separate logs with a newline
    }

    $template srchidx,"%timereported:1:10:date-rfc3339%"

    *.* action(
            type="omelasticsearch" 
                template="miniSchema"
                searchIndex="srchidx"
                dynSearchIndex="on" 
                searchType="logs"
                server="localhost"
                serverport="9200"
                bulkmode="on" 
                queue.dequeuebatchsize="200"
                queue.type="linkedlist"
                queue.timeoutenqueue="0"
                queue.filename="dbq"
                queue.highwatermark="500000"
                queue.lowwatermark="400000"
                queue.discardmark="5000000"
                queue.timeoutenqueue="0"
                queue.maxdiskspace="5g"
                queue.size="2000000"
                queue.saveonshutdown="on"
                action.resumeretrycount="-1")

Now let's take things one line at a time:

    $template miniSchema,"{
        \"message\":\"
        %msg:::json%
        \",\"host\":\"
        %HOSTNAME:::json%
        \",\"severity\":\"
        %syslogseverity%
        \",\"date\":\"
        %timereported:1:19:date-rfc3339%
        .
        %timereported:1:3:date-subseconds%
        \",\"tag\":\"
        %syslogtag:::json%
        \"}"

This is to describe how your log line will end up as a document in Elasticsearch. 
For example, at the beginning, we have \"message\":\"%msg:::json%\". This means that
"message" will be the name of the field, and the value will be whatever the syslog %msg% 
property contains. Putting %msg:::json% there is to tell rsyslog to make that value 
a proper JSON field, by escaping quotes for example. Some fields don't need such escaping,
so you can save your CPU some time by omitting that, like it's done here
with \"severity\":\"%syslogseverity%\".

Note about quotes: in general, you need to have quotes to describe the template, like this: 

    $template TemplateName,"template contents go here"

JSON fields and values are delimited by double-quotes, so you need to escape them with a 
backslash. If you need to check out more about rsyslog templates, here are two useful links:
* general info about templates: http://www.rsyslog.com/doc/rsyslog_conf_templates.html
* what sort of variables you can put there: http://www.rsyslog.com/doc/property_replacer.html

Finally, you can use the full power of templates when describind your JSON. For example, you 
might have noticed the hack with this one: 

    \"date\":\"%timereported:1:19:date-rfc3339%.%timereported:1:3:date-subseconds%\"

It's because I always want my timestamp with miliseconds, and I want 000 for those who don't
report sub-second timestamps. There might be a better way to do this (please fill in here if
you got a nicer approach), but it works for me.

    $template srchidx,"%timereported:1:10:date-rfc3339%"

This is a template that we'll use to make one index of logs per day. You can choose to keep
all your logs in one index, and remove old ones by 
using TTL: http://www.elasticsearch.org/guide/reference/mapping/ttl-field.html

But in general it's recommended to roll your indices based on time. Depending on how long 
you want to keep your logs, you might want to choose another interval (eg: one per month)
to avoid having too many indices. But other than that, you will be able to change your number
of shards and replicas, and other index setting pretty easily, just because a new index will
be created every now and then. Most importantly, you can backup, optimize, remove and restore
"old" indices quite easily. Here are a couple of scripts that do all 
that (except for deleting): https://gist.github.com/3180985

    *.*     action(type="omelasticsearch"
                template="miniSchema"

Here we make all logs go through omelasticsearch. We use the template defined earlier to 
make the JSON document. You don't *need* to use such a template, you can rely on the default 
one which looks like this: 

    $template JSONDefault, "{
                             \"message\":\"
                             %msg:::json%
                             \",\"fromhost\":\"
                             %HOSTNAME:::json%
                             \",\"facility\":\"
                             %syslogfacility-text%
                             \",\"priority\":\"
                             %syslogpriority-text%\",\"timereported\":\"
                             %timereported:::date-rfc3339%\",\"timegenerated\":\"
                             %timegenerated:::date-rfc3339%\"}"

                searchIndex="srchidx"
                dynSearchIndex="on" 
                searchType="logs"

Here we use the "srchidx" template defined earlier to make index names like "2012-07-25". In order to tell rsyslog to use the template with that name instead of just the given string, you need to specify dynSearchIndex="on". By contrast, we leave it to the default "off" for dynSearchType, because here we want to name our type simply "logs".

                server="localhost"
                serverport="9200"

Here we define the Elasticsearch node where we send the logs (host and port). In case the node goes down, you can specify a failover node, much like you do with any other action, as described here: http://wiki.rsyslog.com/index.php/FailoverSyslogServer

But stay tuned! In future versions this might be done automatically, since a list of Elasticsearch nodes can be retrieved at startup, if multicasting works in your network. Take a look here for details: http://www.gossamer-threads.com/lists/rsyslog/users/7161

                bulkmode="on" 
                queue.dequeuebatchsize="200"

By default, rsyslog inserts your logs to Elasticsearch one by one. You can make things much faster by using a queue and inserting them in bulk. queue.dequeuebatchsize is the maximum number of elements in the bulk. "Maximum" means that, if there isn't much load, rsyslog will not wait for the queue to get to that size in order to do a bulk insert. Which is good. More info about bulk inserts in Elasticsearch can be found here: http://www.elasticsearch.org/guide/reference/api/bulk.html

                queue.type="linkedlist"
                queue.timeoutenqueue="0"
                queue.filename="dbq"
                queue.highwatermark="500000"
                queue.lowwatermark="400000"
                queue.discardmark="5000000"
                queue.timeoutenqueue="0"
                queue.maxdiskspace="5g"
                queue.size="2000000"
                queue.saveonshutdown="on"

These are the queue settings we have looked at in the Main Message Queue section, only in the new, v6 configuration format. Each action can have it's own queue settings.

                action.resumeretrycount="-1")

This enables rsyslog to retry indefinitely if your Elasticsearch node is down, which will build up a queue during that time. When Elasticsearch goes back up, it will start inserting from the queue.

You don't have to worry what happens if a message gets malformed in a way (eg: not a proper JSON). Elasticsearch gives an error in this case, and the message is discarded from the queue.

### Some messages are already JSON ###

Let's take a more complex scenario: some applications can put JSONs in some of their syslog messages. It would be awesome (eg: faster and more precise searches, better statistics using facets) to use that JSON and insert it in Elasticsearch as it is.

Now let's assume that the logging application will identify those JSON messages in a way. For example, tag them by making JSON messages look like this: 'TAG {"name": "Radu", "class": "paladin", "level": 9001}'

Let's also assume that we want to take the JSON from there, and add the timestamp for when the message was reported, and insert the resulting JSON in a given Elasticsearch index. We might also want to put logs in separate types based on hostnames. For example, logs from webserver01, webserver02, etc will go to webserver_logs and those from db01, db02, etc will go to db_logs.

We should be able to achieve all this with the following:

    $template getType,"%hostname:R,ERE,0,DFLT:[A-Za-z-]+--end%_logs"

    $template myJSONs,"{\"date\":\"%timereported:1:19:date-rfc3339%\", %msg:7:$::"

    if $msg startswith ' TAG' then action(type="omelasticsearch"
                                    template="myJSONs"
                                    searchIndex="testindex"
                                    searchType="getType"
                                    dynSearchType="on"
                                    server="localhost"
                                    serverport="9200"
                                    bulkmode="on"
                                    queue.dequeuebatchsize="100"
                                    queue.type="linkedlist"
                                    queue.size="200000"
                                    queue.timeoutenqueue="0"
                                    action.resumeretrycount="-1")

Let's take them one by one:

    $template getType,"%hostname:R,ERE,0,DFLT:[A-Za-z-]+--end%_logs"

This will take hostnames like myserver01 or my-server23 and generate strings like 
myserver_logs or my-server_logs. Which is what we want for our Elasticsearch types in this scenario.

    $template myJSONs,"{\"date\":\"%timereported:1:19:date-rfc3339%\", %msg:7:$::"

This will take a syslog message that goes like 

    'TAG {"name": "Radu", "class": "paladin", "level": 9001}'

but only from the 7th character until the end `(%msg:7:$::)`. This will be added to the first part, which is our reported timestamp. The end result will be the the proper, date-containing JSON that we really want: '{"date": "2012-07-26T13:59:36", "name": "Radu", "class": "paladin", "level": 9001}'.

    if $msg startswith ' TAG' then 
        action(
            type="omelasticsearch"
            template="myJSONs"
            searchIndex="testindex"

This will take all messages that start with "TAG", use the defined template to make them a proper JSON and insert them in the specified index. Note the leading space in the condition: ' TAG'. It's there because in normal conditions the %msg% variable gets a leading space. Some details about why that happens can be found here: http://www.rsyslog.com/log-normalization-and-the-leading-space/

                                    searchType="getType"
                                    dynSearchType="on"
                                    server="localhost"
                                    serverport="9200"

Here we define our Elasticsearch server and port, and also the dynamic type (eg: generic-server-name_logs) that we defined earlier.

                                    bulkmode="on"
                                    queue.dequeuebatchsize="100"

You probably want bulk indexing here as well, so we configure this again.

                                    queue.type="linkedlist"
                                    queue.size="200000"
                                    queue.timeoutenqueue="0"
                                    action.resumeretrycount="-1")

Here we configure an in-memory queue, of type "linkedlist" that will hold up to 200K messages. 
If the queue gets full it will not throttle, and it will retry indefinitely if the Elasticsearch
server is unreachable.



### outname Support for omelasticsearch ###

**Question**    
I'm trying out rsyslog 7.2.2 with omelasticsearch, and I'm struggling a bit
with the new template format. What I got working is something like this:

    template(name="myTemplate" type="list" option.json="on") {
        constant(value="{")
        constant(value="\"myMessage\":\"")       property(name="msg")
        constant(value="\",\"myTimestamp\":\"")  property(name="timereported" dateFormat="rfc3339")
        constant(value="\"}")
    }


But from reading the documentation I understand that I could use the
"outname" option to make it look less ugly. Something like this:

    template(name="myTemplate" type="list" option.json="on") {
        property(name="msg" outname="message")
        property(name="timereported" outname="timestamp" dateFormat="rfc3339")
    }

Which doesn't work, it seems to work as if "outname" just isn't there.

Are there any nicer ways to define my template? How does "outname" work? Is
it not supported for omelasticsearch?

**Answer**    
just a quick note: I need to look some more in-depth, will do asap.

I think outname does not affect string generation. 
It's primarily for plugins that are structure-aware, and omelasticsearch has not been upgraded 
to that new interface.  But I am not 100% sure (too much changed in the past weeks ;)).


[[queuetips]]
== Sending messages with tags larger than 32 characters

Friday, October 21st, 2011    
The relevant syslog RFCs 3164 and 5424 limit the syslog tag to 32 characters max. 
Messages with larger tag length are malformed and may be discarded by receivers. 
Anyhow, some folks sometimes need to send tags longer than permitted.

To do so, a new template must be created and used when sending. 
The simplest way is to start with the standard forwarding template. 
The standard templates are hardcoded inside rsyslog. Thus they do not show up 
in your configuration file (but you can obtain them from the source, of course). 
In 5.8.6, the forwarding template is defined as follows:

[source]
----
template (name="ForwardFormat" type="string" string="<%PRI%>%TIMESTAMP:::date-rfc3339% %HOSTNAME%
%syslogtag:1:32%%msg:::sp-if-no-1st-sp%%msg%")
----

NOTE: all templates are on one line in rsyslog.conf. They are broken here for readability.

This template is RFC-compliant. Now look at the part in red. It specifies the tag. Note that, 
via the property replacer, it is restricted to 32 characters (from position 1 to position 32 inclusive).
This is what you need to change. To remove the limit … just remove it ;-) This leads to a template like this:

template (name="LongTagForwardFormat" type="string" string="<%PRI%>%TIMESTAMP:::date-rfc3339% %HOSTNAME%
%syslogtag%%msg:::sp-if-no-1st-sp%%msg%")
Note that I have renamed the template in order to avoid conflicts with build-in templates. 
As it is a custom template, it is not hardcoded, so you need to actually configure it 
in your rsyslog.conf. Then, you need to use that template if you want to send messages 
to a remote host. This can be done via the usual way. 
Let’s assume you use legacy plain TCP syslog. Then the line looks as follows:

action(type="omfwd" 
Target="server.example.net"
Port="10514"
Protocol="tcp"
Template="LongTagForwardFormat"
)
This will bind the forwarding action to the newly defined template. Now tags of any size will be forwarded. Please keep in mind that receivers may have problems with large tags and may truncate them or drop the whole message. So check twice that the receiver handles long tags well.

Rsyslog supports tags to a build-defined maximum. The current (5.8.6) default is 511 characters, but this may be different if you install from a package, use a newer version of rsyslog or use sources obtained from someone else. So double-check.

Tags: config snippet, interoperability, rsyslog, syslog, tag
Posted in Basic Configuration, Guides for rsyslog, The recipies | No Comments »

Using the syslog receiver module
Tuesday, March 29th, 2011
We want to use rsyslog in its general purpose. We want to receive syslog. In rsyslog, we have two possibilities to achieve that.

Things to think about
First of all, we will determine, which way of syslog reception we want to use. We can receive syslog via UDP or TCP. The config statements are each a bit different in both cases.

In most cases, UDP syslog should be fully sufficient and performing well. But, you should be aware, that on large message bursts messages can be dropped. That is not the case with TCP syslog, since the sender and receiver communicate about the arrival of network packets. That makes TCP syslog more suitable for environments where log messages may not be lost or that must ensure PCI compliance (like banks).

Config Statements

    module(load="imudp") # needs to be done just once
    input(type="imudp" port="514")
    
and

module(load="imtcp") # needs to be done just once
input(type="imtcp" port="514")
How it works
The configuration part for the syslog server is pretty simple basically. Though, there are some parameters that can be set for both modules. But this is not necessary in most cases.

In general, first the module needs to be loaded. This is done via the directive module().

module(load="imudp / imtcp")
The module must be loaded, because the directives and functions rely on it. Just by using the right commands, rsyslog will not know where to get the functionality code from.

And next is the command that runs the syslog server itself is called input().

input(type="imudp" port="514")
input(type="imtcp" port="514")
Basically, the command says to run a syslog server on a specific port. Depending on the command, you can easily determine the UDP and the TCP server.

You can of course use both types of syslog server at the same time, too. You just need to load both modules for that and configure the server command to listen to specific ports. Then you can receive both UDP and TCP syslog.

Important

In general, we suggest to use TCP syslog. It is way more reliable than UDP syslog and still pretty fast. The main reason is, that UDP might suffer of message loss. This happens when the syslog server must receive large bursts of messages. If the system buffer for UDP is full, all other messages will be dropped. With TCP, this will not happen. But sometimes it might be good to have a UDP server configured as well. That is, because some devices (like routers) are not able to send TCP syslog by design. In that case, you would need both syslog server types to have everything covered. If you need both syslog server types configured, please make sure they run on proper ports. By default UDP syslog is received on port 514. TCP syslog needs a different port because often the RPC service is using this port as well.

Tags: config snippet, rsyslog, syslog, TCP, UDP
Posted in Basic Configuration | 1 Comment »


[[relpaction]]
== Actions with directives

This snippet will show, how Action directives need to be applied to work properly. 
We will show it with the RELP output module. RELP should ensure a safe and loss-free 
transmission between two machines. But if not configured properly, messages may get lost anyway. 

This is mainly meant for any client side configuration.

First of all you have to enable the RELP module.

To load the module use this:

    module(name=omrelp)

To make sure, messages will not get dropped in the event the receiver is not available, 
we basically  need the following directives.  
Additionaly, the queued messages should get saved to the harddrive if the client service needs to shut down. 
It is followed by a forwarding action via RELP to our remote server.

    $ActionQueueType            LinkedList  # use asynchronous processing
    $ActionQueueFileName        srvrfwd     # set file name, also enables disk mode
    $ActionResumeRetryCount     -1          # infinite retries on insert failure
    $ActionQueueSaveOnShutdown  on          # save in-memory data if rsyslog shuts down
    *.* :omrelp:192.168.152.2:20514    
<br/>

    action(
        type                   "omrelp" 
        template               "RSYSLOG_ForwardFormat"
        target                 "192.168.152.2:20514"
        port                   "2514"
        action.resumretrycount -1                     # infinite retries on insert failure
        queue.type             "LinkedList"           # use asynchronous processing
        queue.filename         "srvrfwd"              # set file name, also enables disk mode
        queue.savedonshutdown  "on"                   # save in-memory data if rsyslog shuts down
    )

**Attention**: The directives are only valid for the next configured action!     
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; So you have to set the directives each time you use a new action.

Here is an example with two actions.

    #first action
    $ActionQueueType LinkedList # use asynchronous processing
    $ActionQueueFileName srvrfwd # set file name, also enables disk mode
    $ActionResumeRetryCount -1 # infinite retries on insert failure
    $ActionQueueSaveOnShutdown on # save in-memory data if rsyslog shuts down
    :syslogtag, isequal, “app1″ :omrelp:192.168.152.2:20514
>

    #second action
    $ActionQueueType LinkedList # use asynchronous processing
    $ActionQueueFileName srvrfwd # set file name, also enables disk mode
    $ActionResumeRetryCount -1 # infinite retries on insert failure
    $ActionQueueSaveOnShutdown on # save in-memory data if rsyslog shuts down
    :syslogtag, isequal, “app2″ :omrelp:192.168.152.3:20514

As you can see, we have the whole block of directives mulitple times. But this time, we filter the message 
for the syslogtag and have the diffenrently tagged messages sent to different receivers. 
Now if the receiver is not available, sending the messages will be retried until it is back up again.

If the local rsyslog needs to shut down, all queued messages get written to disk without being lost.


[[replacer]]
== The Property Replacer

The property replacer is a core component in rsyslogd's output system. A syslog message has a number of 
well-defined properties (see below). Each of this properties can be accessed and manipulated by the 
property replacer. With it, it is easy to use only part of a property value or manipulate the value, 
e.g. by converting all characters to lower case.

### Accessing Properties ###

Syslog message properties are used inside templates. They are accessed by putting them between percent signs. 
Properties can be modified by the property replacer. The full syntax is as follows:

    %propname:fromChar:toChar:options:fieldname%

### Available Properties ###

* **propname** is the name of the property to access. It is case-insensitive 
    (prior to 3.17.0, they were case-senstive).
    Currently supported are:

* **msg**  the MSG part of the message (aka "the message" ;))

* **rawmsg**  the message excactly as it was received from the socket. Should be useful for debugging.

* **hostname**  hostname from the message

* **source**  alias for HOSTNAME

* **fromhost**  hostname of the system the message was received from (in a relay chain, this is the system immediately in 
    front of us and not necessarily the original sender). This is a DNS-resolved name, except if that is not possible or 
    DNS resolution has been disabled.

* **fromhost-ip**  The same as fromhost, but alsways as an IP address. Local inputs (like imklog) use 127.0.0.1 in this property.

* **syslogtag**  TAG from the message

* **programname**  the "static" part of the tag, as defined by BSD syslogd. 
For example, when TAG is "named[12345]", programname is "named".

* **pri**  PRI part of the message - undecoded (single value)

* **pri-text**  the PRI part of the message in textual form (e.g. "syslog.info")

* **iut**  the monitorware InfoUnitType - used when talking to a MonitorWare backend (also for phpLogCon)

* **syslogfacility**  the facility from the message - in numerical form

* **syslogfacility-text**  the facility from the message - in text form

* **syslogseverity**  severity from the message - in numerical form

* **syslogseverity-text**  severity from the message - in text form

* **syslogpriority**  an alias for syslogseverity - included for historical reasons 
    (be careful: it still is the severity, not PRI!)

* **syslogpriority-text**  an alias for syslogseverity-text

* **timegenerated**  timestamp when the message was RECEIVED. Always in high resolution

* **timereported**  timestamp from the message. Resolution depends on what was provided in the message 
    (in most cases, only seconds)

* **timestamp**  alias for timereported

* **protocol-version**  The contents of the PROTCOL-VERSION field from IETF draft draft-ietf-syslog-protcol

* **structured-data**  The contents of the STRUCTURED-DATA field from IETF draft draft-ietf-syslog-protocol

* **app-name**  The contents of the APP-NAME field from IETF draft draft-ietf-syslog-protocol

* **procid**  The contents of the PROCID field from IETF draft draft-ietf-syslog-protocol

* **msgid**  The contents of the MSGID field from IETF draft draft-ietf-syslog-protocol

* **parsesuccess**  This returns the status of the last called higher level parser, like mmjsonparse. 
A higher level parser parses the actual message for additional structured data and maintains an extra 
property table while doing so (this is often referred to as "cee data" because the idea was originally 
rooted in the cee effort, only (but has been extended since then). Note that higher level parsers must 
explicitely support (and set) this property. So, depending on the parser, it may not be set correctly. 
If the parser properly supports it, the value "OK" means that parsing was successfull, while "FAIL" 
means the parser could not successfully obtain any data. Failure state is not necessarily an error. 
For example, it may simple indicate that the cee-enhanced syslog parser (mmjsonparse) did not detect 
cee-enhanced format, what can be totally valid. Using this property, further processing of the message
can be directed based on this parsing outcome. If no parser has been called at the time this property 
is accessed, it will contain "FAIL". 
This property is available since version 6.3.8.

* **inputname**  The name of the input module that generated the message (e.g. "imuxsock", "imudp"). 
Note that not all modules necessarily provide this property. If not provided, it is an empty string.
Also note that the input module may provide any value of its liking. Most importantly, it is not 
necessarily the module input name. Internal sources can also provide inputnames. Currently, "rsyslogd"
is defined as inputname for messages internally generated by rsyslogd, for example startup and shutdown 
and error messages. This property is considered useful when trying to filter messages based on where 
they originated - e.g. locally generated messages ("rsyslogd", "imuxsock", "imklog")  should go to a 
different place than messages generated somewhere.

* **$bom**  The UTF-8 encoded Unicode byte-order mask (BOM). This may be useful in templates for RFC5424 support, 
    when the character set is know to be Unicode.

* **$uptime**  system-uptime in seconds (as reported by operating system).

* **$now**  The current date stamp in the format YYYY-MM-DD

* **$year**  The current year (4-digit)

* **$month**  The current month (2-digit)

* **$day**  The current day of the month (2-digit)

* **$hour**  The current hour in military (24 hour) time (2-digit)

* **$hhour**  The current half hour we are in. From minute 0 to 29, this is always 0 while from 30 to 59 it is always 1.

* **$qhour**  The current quarter hour we are in. Much like $HHOUR, but values range from 0 to 3 (for the four quater hours that are in each hour)

* **$minute**  The current minute (2-digit)

* **$myhostname**  The name of the current host as it knows itself (probably useful for filtering in a generic way)

* **$!<name>**  This is the "bridge" to syslog message normalization (via mmnormalize): 
name is a name defined inside the normalization rule. It has the value selected by the 
rule or none if no rule with this field did match. You can also use these properties to 
specify JSON fields from the * **CEE-enhanced** syslog message, once you parse it with mmjsonparse

* **$!all-json**  This is the JSON part of the CEE-enhanced syslog message, which can be parsed with mmjsonparse

Properties starting with a $-sign are so-called system properties. These do NOT stem from the message but are rather internally-generated.

#### Legacy Character Positions ####

FromChar and toChar are used to build substrings. They specify the offset within the string that should be copied. Offset counting starts at 1, so if you need to obtain the first 2 characters of the message text, you can use this syntax: "%msg:1:2%". If you do not whish to specify from and to, but you want to specify options, you still need to include the colons. For example, if you would like to convert the full message text to lower case, use "%msg:::lowercase%". If you would like to extract from a position until the end of the string, you can place a dollar-sign ("$") in toChar (e.g. %msg:10:$%, which will extract from position 10 to the end of the string).

There is also support for regular expressions. To use them, you need to place a "R" into FromChar. This tells rsyslog that a regular expression instead of position-based extraction is desired. The actual regular expression must then be provided in toChar. The regular expression must be followed by the string "--end". It denotes the end of the regular expression and will not become part of it. If you are using regular expressions, the property replacer will return the part of the property text that matches the regular expression. An example for a property replacer sequence with a regular expression is: "%msg:R:.*Sev:. \(.*\) \[.*--end%"

It is possible to specify some parametes after the "R". These are comma-separated. They are:

    R,<regexp-type>,<submatch>,<nomatch>,<match-number>

* **regexp-type** is either "BRE" for Posix basic regular expressions or "ERE" for extended ones. 
    The string must be given in upper case. The default is "BRE" to be consistent with earlier versions 
    of rsyslog that did not support ERE. 
* **The submatch** identifies the submatch to be used with the result. A single digit is supported. 
    Match 0 is the full match, while 1 to 9 are the acutal submatches.
* **The match-number** identifies which match to use, if the expression occurs more than once inside the string.
    The first match is number 0, the second 1 and so on.  Up to 10 matches (up to number 9) are supported.     
    Note that it would be more natural to have the match-number in front of submatch, but this would 
    break backward-compatibility. So the match-number must be specified after "nomatch".
* **nomatch** specifies what should be used in case no match is found.

The following is a sample of an ERE expression that takes the first submatch from the message string and replaces the expression with the full field if no match is found:

    %msg:R,ERE,1,FIELD:for (vlan[0-9]*):--end%

and this takes the first submatch of the second match of said expression:

    %msg:R,ERE,1,FIELD,1:for (vlan[0-9]*):--end%

Please note: there is also a **rsyslog regular expression checker/generator** online tool available. With that tool, you can check your regular expressions and also generate a valid property replacer sequence. Usage of this tool is recommended. Depending on the version offered, the tool may not cover all subleties that can be done with the property replacer. It concentrates on the most often used cases. So it is still useful to hand-craft expressions for demanding environments.

Also, extraction can be done based on so-called **"fields"**.  To do so, place a "F" into FromChar. A **field** in its current definition is anything that is delimited by a delimiter character.  The delimiter by default is TAB (US-ASCII value 9). However, if can be changed to any other US-ASCII character by specifying a comma and the decimal US-ASCII value of the delimiter immediately after the "F". For example, to use comma (",") as a delimiter, use this field specifier: "F,44".  If your syslog data is delimited, this is a quicker way to extract than via regular expressions (actually, a *much* quicker way). Field counting starts at 1. Field zero is accepted, but will always lead to a "field not found" error. The same happens if a field number higher than the number of fields in the property is requested. The field number must be placed in the "ToChar" parameter. An example where the 3rd field (delimited by TAB) from the msg property is extracted is as follows: "%msg:F:3%". The same example with semicolon as delimiter is "%msg:F,59:3%".

Please note that the special characters "F" and "R" are case-sensitive. Only upper case works, lower case will return an error. There are no white spaces permitted inside the sequence (that will lead to error messages and will NOT provide the intended result).

Each occurence of the field delimiter starts a new field. However, if you add a plus sign ("+") after the field delimiter, multiple delimiters, one immediately after the others, are treated as separate fields. This can be useful in cases where the syslog message contains such sequences. A frequent case may be with code that is written as follows:

    int n, m;
    ...
    syslog(LOG_ERR, "%d test %6d", n, m);

This will result into things like this in syslog messages:

    "1 test      2", "1 test     23", "1 test  234567"

As you can see, the fields are delimited by space characters, but their exact number is unknown.
They can properly be extracted as follows:

    "%msg:F,32:2%" to "%msg:F,32+:2%".

This feature is modeled after perl compatible regular expressions.

### Property Options ###

property options are case-insensitive. They are available as of version 6.5.0. Currently, 
the following options are defined:

* **Name** New format. Name of the template / property / constant.

* **Outname**  This field permits to specify a field name for structured-data emitting property replacer options. 
    It is most useful to set, for example, the name for JSON-based fields (like used in ommngodb). 
    For text-based modules, it is simply ignored. If not specified, the original property name is used, with the 
    exception of properties starting with "$!", where that prefix is removed. Note that unnamaned constants are 
    NOT forwarded to output modules that expect structure (like ommnogodb). To pass constants, an outname must be set.

* **CaseConversion**  New format. Additional values below.

* **upper**  convert property to lowercase only

* **lower**  convert property text to uppercase only

* **DateFormat**  New format, additional parameter is needed. See below.

* **mysql**  format as mysql date

* **pgsql**  format as pgsql date

* **rfc3164**  format as RFC 3164 date

* **rfc3164-buggyday**  similar to date-rfc3164, but emulates a common coding error: 
    RFC 3164 demands that a space is written for 

* **single-digit days. With this option, a zero is written instead. 
    This format seems to be used by syslog-ng and the date-rfc3164-buggyday option can be used in 
    migration scenarios where otherwise lots of scripts would need to be adjusted. 
    It is recommended not to use this option when forwarding to remote hosts - they may treat the 
    date as invalid (especially when parsing strictly according to RFC 3164).

* **rfc3339**  format as RFC 3339 date

* **unixtimestamp**  format as unix timestamp (seconds since epoch)

* **subseconds**  just the subseconds of a timestamp (always 0 for a low precision timestamp)

* **pos-end-relative**  the from and to position is relative to the end of the string instead of 
    the usual start of string. (available since rsyslog v7.3.10)

* **ControlCharacters**  Option values for how to process control characters

* **escape**  replace control characters (ASCII value 127 and values less then 32) with an escape sequence. 
    The sequnce is "#<charval>" where charval is the 3-digit decimal value of the control character. 
    For example, a tabulator would be replaced by "#009".
    Note: using this option requires that $EscapeControlCharactersOnReceive is set to off.

* **space**  replace control characters by spaces
    Note: using this option requires that $EscapeControlCharactersOnReceive is set to off.

* **drop**  drop control characters - the resulting string will neither contain control characters, 
    escape sequences nor any other replacement character like space.
    Note: using this option requires that $EscapeControlCharactersOnReceive is set to off.

* **SecurePath**  Option values for securing path templates.

* **drop**  Drops slashes inside the field (e.g. "a/b" becomes "ab"). 
    Useful for secure pathname generation (with dynafiles).

* **replace**  Replace slashes inside the field by an underscore. (e.g. "a/b" becomes "a_b"). 
    Useful for secure pathname generation (with dynafiles).

* **Format**  Option values for the general output format.

* **json**  encode the value so that it can be used inside a JSON field. 
    This means that several characters (according to the JSON spec) are being escaped, 
    for example US-ASCII LF is replaced by "\n". The json option cannot be used together 
    with either jsonf or csv options.

* **jsonf**  (available in 6.3.9+) This signifies that the property should be expressed as a json field. 
    That means not only the property is written, but rather a complete json field in the format
    "fieldname"="value" where "filedname" is the assigend field name (or the property name if none 
    was assigned) and value is the end result of property replacer operation. Note that value supports 
    all property replacer options, like substrings, case converson and the like. 
    Values are properly json-escaped. However, field names are (currently) not. 
    It is expected that proper field names are configured. 
    The jsonf option cannot be used together with either json or csv options.

* **csv**  formats the resulting field (after all modifications) in CSV format as specified in RFC 4180. 
    Rsyslog will always use double quotes. Note that in order to have full CSV-formatted text, you need 
    to define a proper template. An example is this one:    
    `$template csvline,"%syslogtag:::csv%,%msg:::csv%"`    
    Most importantly, you need to provide the commas between the fields inside the template. 
    The csv option cannot be used together with either json or jsonf options. 
    This feature was introduced in rsyslog 4.1.6.

* **droplastlf**  The last LF in the message (if any), is dropped. Especially useful for PIX.

* **spifno1stsp**  This option looks scary and should probably not be used by a user. 
    For any field given, it returns either a single space character or no character at all. 
    Field content is never returned. A space is returned if (and only if) the first character of 
    the field's content is NOT a space. This option is kind of a hack to solve a problem rooted 
    in RFC 3164: 3164 specifies no delimiter between the syslog tag sequence and the actual message text. 
    Almost all implementation in fact delemit the two by a space. 
    As of RFC 3164, this space is part of the message text itself. This leads to a problem when building
    the message (e.g. when writing to disk or forwarding).  
    Should a delimiting space be included if the message does not start with one? If not, the tag is 
    immediately followed by another non-space character, which can lead some log parsers to misinterpret 
    what is the tag and what the message. The problem finally surfaced when the klog module was 
    restructured and the tag correctly written. It exists with other message sources, too. The solution 
    was the introduction of this special property replacer option. Now, the default template can contain 
    a conditional space, which exists only if the message does not start with one. 
    While this does not solve all issues, it should work good enough in the far majority of all cases. 
    If you read this text and have no idea of what it is talking about - relax: this is a good indication 
    you will never need this option. Simply forget about it ;)

* **New character position**  In addition to the above mentioned Character Positions in the legacy format, 
    positions can be determined by specifying the correct options for the properties. 
    Again, this is mostly for using the list format.

* **position.From**  Character position in the property to start from.

* **position.To**  Character position that determines the end for extraction. 
    If the value is "$" then the end of the string will be used.

* **field.Number**  The number of the field, which should be used for the search operation with Regex.

* **field.Delimiter**  The Character that should delimit a field. 
    Example: ",".  Everything in a property until this character is considered a field.

* **regex.Expression**  Value to be compared to property.

* **regex.Type**  Values BRE or ERE

* **regex.NoMatchMode**  DFLT, BLANK, ZERO, FIELD

* **regex.Match**  Match to use.

* **regex.Submatch**  Submatch to use. Values 0-9 whereas 0 = All


### Legacy Property Options ###

property options are case-insensitive. 
Currently, the following options are defined:

This feature was introduced in rsyslog 4.6.2 and v4 versions above and 5.5.3 and all versions above.

* **uppercase**  convert property to lowercase only

* **lowercase**  convert property text to uppercase only

* **json**  encode the value so that it can be used inside a JSON field. 
    This means that several characters (according to the JSON spec) are being escaped, 
    for example US-ASCII LF is replaced by "\n". The json option cannot be used together 
    with either jsonf or csv options.

* **jsonf**  (available in 6.3.9+) This signifies that the property should be expressed as a json field. 
    That means not only the property is written, but rather a complete json field in the format
    "fieldname"="value" where "filedname" is the assigend field name (or the property name if 
    none was assigned) and value is the end result of property replacer operation. 
    Note that value supports all property replacer options, like substrings, case converson and the like. 
    Values are properly json-escaped. However, field names are (currently) not. It is expected that
    proper field names are configured. 
    The jsonf option cannot be used together with either json or csv options.

* **csv**  formats the resulting field (after all modifications) in CSV format as specified in RFC 4180. 
    Rsyslog will always use double quotes. Note that in order to have full CSV-formatted text, you need 
    to define a proper template. An example is this one:    
    `$template csvline,"%syslogtag:::csv%,%msg:::csv%"`    
    Most importantly, you need to provide the commas between the fields inside the template. 
    The csv option cannot be used together with either json or jsonf options. 
    This feature was introduced in rsyslog 4.1.6.

* **drop-last-lf**  The last LF in the message (if any), is dropped. Especially useful for PIX.

* **date-mysql**  format as mysql date

* **date-rfc3164**  format as RFC 3164 date

* **date-rfc3164-buggyday**  similar to date-rfc3164, but emulates a common coding error: 
    RFC 3164 demands that a space is written for single-digit days. With this option, 
    a zero is written instead. This format seems to be used by syslog-ng and the 
    date-rfc3164-buggyday option can be used in migration scenarios where otherwise lots 
    of scripts would need to be adjusted. It is recommended not to use this option when 
    forwarding to remote hosts - they may treat the date as invalid (especially when 
    parsing strictly according to RFC 3164).

* **date-rfc3339**  format as RFC 3339 date

* **date-unixtimestamp**  format as unix timestamp (seconds since epoch)

* **date-subseconds**  just the subseconds of a timestamp (always 0 for a low precision timestamp)

* **escape-cc**  replace control characters (ASCII value 127 and values less then 32) with an escape sequence. 
    The sequnce is "#<charval>" where charval is the 3-digit decimal value of the control character. 
    For example, a tabulator would be replaced by "#009".
    Note: using this option requires that $EscapeControlCharactersOnReceive is set to off.

* **space-cc**  replace control characters by spaces
    Note: using this option requires that $EscapeControlCharactersOnReceive is set to off.

* **drop-cc**  drop control characters - the resulting string will neither contain control characters, 
    escape sequences nor any other replacement character like space.
    Note: using this option requires that $EscapeControlCharactersOnReceive is set to off.

* **sp-if-no-1st-sp**  This option looks scary and should probably not be used by a user. 
    For any field given, it returns either a single space character or no character at all. 
    Field content is never returned. A space is returned if (and only if) the first character 
    of the field's content is NOT a space. 
    This option is kind of a hack to solve a problem rooted in RFC 3164: 3164 specifies no delimiter 
    between the syslog tag sequence and the actual message text.
    Almost all implementation in fact delemit the two by a space. 
    As of RFC 3164, this space is part of the message text itself. This leads to a problem when building 
    the message (e.g. when writing to disk or forwarding). 
    Should a delimiting space be included if the message does not start with one? If not, the tag is 
    immediately followed by another non-space character, which can lead some log parsers to misinterpret 
    what is the tag and what the message. The problem finally surfaced when the klog module was restructured 
    and the tag correctly written. It exists with other message sources, too. The solution was the 
    introduction of this special property replacer option. Now, the default template can contain a 
    conditional space, which exists only if the message does not start with one. While this does not solve 
    all issues, it should work good enough in the far majority of all cases. 
    If you read this text and have no idea of what it is talking about - relax: this is a good indication 
    you will never need this option. Simply forget about it ;)

* **secpath-drop**  Drops slashes inside the field (e.g. "a/b" becomes "ab"). 
    Useful for secure pathname generation (with dynafiles).

* **secpath-replace**  Replace slashes inside the field by an underscore.  (e.g. "a/b" becomes "a_b"). 
    Useful for secure pathname generation (with dynafiles).

* **mandatory-field**  In templates that are used for building field lists (in particular, ommongodb), 
    include this field, even if it is empty (or NULL). If not set, the field will be removed from 
    the output field set if empty. The latter is the default case.
    To use multiple options, simply place them one after each other with a comma delmimiting them. 
    For example "escape-cc,sp-if-no-1st-sp". If you use conflicting options together, the last one 
    will override the previous one. For example, using "escape-cc,drop-cc" will use drop-cc and 
    "drop-cc,escape-cc" will use escape-cc mode.

### Fieldname ###

This field permits to specify a field name for structured-data emitting property replacer options. 
It was initially introduced to support the "jsonf" option, for which it provides the capability to 
set an alternative field name. If it is not specified, it defaults to the property name.

### Further Links ###

* Article on "Recording the Priority of Syslog Messages" (describes use of templates to record severity
and facility of a message)

* Configuration file format, this is where you actually use the property replacer.

[[rsyslog]]
== Rsyslog Config File

[source]
----
/* rsyslog configuration file (for Red Hat-based systems)
 * note that most of this config file uses old-style format,
 * because it is well-known AND quite suitable for simple cases
 * like we have with the default config. For more advanced 
 * things, RainerScript configuration is suggested.
 *
 * For more information see /usr/share/doc/rsyslog-*/rsyslog_conf.html
 * or latest version online at http://www.rsyslog.com/doc/rsyslog_conf.html 
 * If you experience problems, see http://www.rsyslog.com/doc/troubleshoot.html
 */

#### MODULES ####

module(load="imuxsock") # provides support for local system logging (e.g. via logger command)
module(load="imklog")   # provides kernel logging support (previously done by rklogd)
#module(load"immark")  # provides --MARK-- message capability

# Provides UDP syslog reception
# for parameters see http://www.rsyslog.com/doc/imudp.html
#module(load="imudp") # needs to be done just once
#input(type="imudp" port="514")

# Provides TCP syslog reception
# for parameters see http://www.rsyslog.com/doc/imtcp.html
#module(load="imtcp") # needs to be done just once
#input(type="imtcp" port="514")


#### GLOBAL DIRECTIVES ####

# Use default timestamp format
$ActionFileDefaultTemplate RSYSLOG_TraditionalFileFormat

# File syncing capability is disabled by default. This feature is usually not required,
# not useful and an extreme performance hit
#$ActionFileEnableSync on

# Include all config files in /etc/rsyslog.d/
$IncludeConfig /etc/rsyslog.d/*.conf


#### RULES ####

# Log all kernel messages to the console.
# Logging much else clutters up the screen.
#kern.*                                                 /dev/console

# Log anything (except mail) of level info or higher.
# Don't log private authentication messages!
*.info;mail.none;authpriv.none;cron.none                /var/log/messages

# The authpriv file has restricted access.
authpriv.*                                              /var/log/secure

# Log all the mail messages in one place.
mail.*                                                  /var/log/maillog


# Log cron stuff
cron.*                                                  /var/log/cron

# Everybody gets emergency messages
*.emerg                                                 :omusrmsg:*

# Save news errors of level crit and higher in a special file.
uucp,news.crit                                          /var/log/spooler

# Save boot messages also to boot.log
local7.*                                                /var/log/boot.log


# ### begin forwarding rule ###
# The statement between the begin ... end define a SINGLE forwarding
# rule. They belong together, do NOT split them. If you create multiple
# forwarding rules, duplicate the whole block!
# Remote Logging (we use TCP for reliable delivery)
#
# An on-disk queue is created for this action. If the remote host is
# down, messages are spooled to disk and sent when it is up again.
#$WorkDirectory /var/lib/rsyslog # where to place spool files
#$ActionQueueFileName fwdRule1 # unique name prefix for spool files
#$ActionQueueMaxDiskSpace 1g   # 1gb space limit (use as much as possible)
#$ActionQueueSaveOnShutdown on # save messages to disk on shutdown
#$ActionQueueType LinkedList   # run asynchronously
#$ActionResumeRetryCount -1    # infinite retries if host is down
# remote host is: name/ip:port, e.g. 192.168.0.1:514, port optional
#*.* @@remote-host:514
# ### end of the forwarding rule ###
----


[[rsyslogque]]
== Main Queue

This object is available since 7.5.3. It permits to specify parameters for the main message queue. 
Note that only queue-parameters are permitted for this config object. 
This permits to set the same options like in ruleset and action queues. 
A special statement is needed for the main queue, because it is a different 
object and cannot be configured via any other object.

Note that when the main_queue() object is configured, the legacy `$MainMsgQ...` statements are ignored.
    
## A Simple Example ##

[source]
----
    main_queue(    
        queue.size="100000"      # how many messages (messages, not bytes!) to hold in memory
        queue.type="LinkedList"  # allocate memory dynamically for the queue. Better for handling spikes
    )
----   

[source]
----
    # location for storing queue files on disk
    $WorkDirectory /var/lib/elasticsearch/rsyslog_queues

    main_queue(
        # file name template, also enables disk mode for the memory queue
        queue.fileName incoming_queue

        # allocate memory dynamically for the queue. Better for handling spikes
        Queue.Type LinkedList

        # when to start discarding messages
        queue.DiscardMark 2000000

        # limit of messages in the memory queue. When this is reached, it starts to write to disk
        queue.HighWaterMark 1000000

        # memory queue size at which it stops writing to the disk
        queue.LowWaterMark 800000

        # maximum disk space used for the disk part of the queue
        queue.MaxDiskSpace 5g

        # how many messages (messages, not bytes!) to hold in memory
        queue.Size 8000000

        # don't throttle receiving messages when the queue gets full
        queue.TimeoutEnqueue 0

        # save the queue contents when stopping rsyslog
        queue.SaveOnShutdown on
    )
----
    
# Action Queues #
In v6+, queue parameters are set directly within the action:

    *.* action(type="omfile"
               queue.filename="nfsq"
               queue.type="linkedlist"
               file="/var/log/log1")    
> 

    *.* action(type="omfile"
               queue.filename="diskq"
               queue.type="linkedlist"
               file="/var/log/log2")

>  

    *.* action(type="omfile"
               file="/var/log/log3") 
>    


    $template miniSchema,"{\"message\":\"%msg:::json%\",\"host\":\"%HOSTNAME:::json%\",\"severity\":\"%syslogseverity%\",\"date\":\"%timereported:1:19:date-rfc3339%.%timereported:1:3:date-subseconds%\",\"tag\":\"%syslogtag:::json%\"}"
    $template srchidx,"%timereported:1:10:date-rfc3339%"
    *.*     action(type="omelasticsearch" 
                   template="miniSchema"
                   searchIndex="srchidx"
                   dynSearchIndex="on" 
                   searchType="logs"
                   server="localhost"
                   serverport="9200"
                   bulkmode="on" 
                   queue.dequeuebatchsize="200"
                   queue.type="linkedlist"
                   queue.timeoutenqueue="0"
                   queue.filename="dbq"
                   queue.highwatermark="500000"
                   queue.lowwatermark="400000"
                   queue.discardmark="5000000"
                   queue.timeoutenqueue="0"
                   queue.maxdiskspace="5g"
                   queue.size="2000000"
                   queue.saveonshutdown="on"
                   action.resumeretrycount="-1")


    
# Queue Parameters #

Queues need to be configured in the action or ruleset it should affect. 
If nothing is configured, default values will be used. Thus, the default ruleset has 
only the default main queue. Specific Action queues are not set up by default.

*  **queue.filename** name

*  **queue.size** number

*  **queue.dequeuebatchsize** number    
default 16

*  **queue.maxdiskspace** number

*  **queue.highwatermark** number    
default 8000

*  **queue.lowwatermark** number    
default 2000

*  **queue.fulldelaymark** number

*  **queue.lightdelaymark** number

*  **queue.discardmark** number    
default 9750

*  **queue.discardseverity** number    
numerical severity! default 8 (nothing discarded)

*  **queue.checkpointinterval** number

*  **queue.syncqueuefiles** on/off

*  **queue.type** [FixedArray/LinkedList/Direct/Disk]

*  **queue.workerthreads** number    
number of worker threads, default 1, recommended 1

*  **queue.timeoutshutdown** number     
number is timeout in ms,   
default 0 (indefinite)

*  **queue.timeoutactioncompletion** number    
number is timeout in ms (1000ms is 1sec!),    
default 1000, 0 means immediate!

*  **queue.timeoutenqueue** number    
number is timeout in ms (1000ms is 1sec!),    
default 2000, 0 means indefinite

*  **queue.timeoutworkerthreadshutdown** number    
number is timeout in ms,    
default 60000 (1 minute)

*  **queue.workerthreadminimummessages** number    
default 100

*  **queue.maxfilesize** size_nbr    
default 1m

*  **queue.saveonshutdown** on/off
 
*  **queue.dequeueslowdown** number    
number is timeout in microseconds,    
default 0 (no delay). Simple rate-limiting!

*  **queue.dequeuetimebegin** number    

*  **queue.dequeuetimeend** number    
    

###Sample###
The following is a sample of a TCP forwarding action with its own queue.

    Module (load="builtin:omfwd")
    *.* action(Type="omfwd" 
               Target="192.168.2.11"
               Port="10514"
               Protocol="tcp" 
      
               queue.filename="forwarding"
               queue.size="1000000"
               queue.type="LinkedList"
        )

This is the end



[[storeandforward]]
== Storing and forwarding remote messages

Friday, April 1st, 2011

In this scenario, we want to store remote sent messages into a specific local file and 
forward the received messages to another syslog server. Local messages should still be 
locally stored.

### Things to think about ###

How should this work out? Basically, we need a syslog listener for TCP and one for UDP, the local logging service and two rulesets, one for the local logging and one for the remote logging.

TCP recpetion is not a build-in capability. You need to load the imtcp plugin in order to enable it. This needs to be done only once in rsyslog.conf. Do it right at the top.

Note that the server port address specified in $InputTCPServerRun must match the port address that the clients send messages to.

### Config Statements ###

~~~
# Modules
$ModLoad imtcp
$ModLoad imudp
$ModLoad imuxsock
$ModLoad imklog

# Templates
# log every host in its own directory
$template RemoteHost,"/var/syslog/hosts/%HOSTNAME%/%$YEAR%/%$MONTH%/%$DAY%/syslog.log"

### Rulesets
# Local Logging
$RuleSet local
kern.*                                                 /var/log/messages
*.info;mail.none;authpriv.none;cron.none                /var/log/messages
authpriv.*                                              /var/log/secure
mail.*                                                  -/var/log/maillog
cron.*                                                  /var/log/cron
*.emerg                                                 *
uucp,news.crit                                          /var/log/spooler
local7.*                                                /var/log/boot.log
# use the local RuleSet as default if not specified otherwise
$DefaultRuleset local

# Remote Logging
$RuleSet remote
*.* ?RemoteHost
# Send messages we receive to Gremlin
*.* @@W.X.Y.Z:514

### Listeners
# bind ruleset to tcp listener
$InputTCPServerBindRuleset remote
# and activate it:
$InputTCPServerRun 10514

$InputUDPServerBindRuleset remote
$UDPServerRun 514
~~~

### How it works ###

The configuration basically works in 4 parts. First, we load all the modules (imtcp, imudp, imuxsock, imklog). Then we specify the templates for creating files. The we create the rulesets which we can use for the different receivers. And last we set the listeners.

The rulesets are somewhat interesting to look at. The ruleset “local” will be set as the default ruleset. That means, that it will be used by any listener if it is not specified otherwise. Further, this ruleset uses the default log paths vor various facilities and severities.

The ruleset “remote” on the other hand takes care of the local logging and forwarding of all log messages that are received either via UDP or TCP. First, all the messages will be stored in a local file. The filename will be generated with the help of the template at the beginning of our configuration (in our example a rather complex folder structure will be used). After logging into the file, all the messages will be forwarded to another syslog server via TCP.

In the last part of the configuration we set the syslog listeners. We first bind the listener to the ruleset “remote”, then we give it the directive to run the listener with the port to use. In our case we use 10514 for TCP and 514 for UDP.

### Important ###
There are some tricks in this configuration. Since we are actively using the rulesets, 
we must specify those rulesets before being able to bind them to a listener. 
That means, the order in the configuration is somewhat different than usual. 
Usually we would put the listener commands on top of the configuration right after the modules. 
Now we need to specify the rulesets first, then set the listeners (including the bind command). 
This is due to the current configuration design of rsyslog. 
To bind a listener to a ruleset, the ruleset object must at least be present before the 
listener is created. And that is why we need this kind of order for our configuration.



[[structuredlog]]
== Structured Logging with Rsyslog and Elasticsearch

http://blog.sematext.com/2013/05/28/structured-logging-with-rsyslog-and-elasticsearch/[Article Source]

When your applications generate a lot of logs, you’d probably want to make some sense of them by 
searching and/or statistics.  Here’s when structured logging comes in handy, and I would 
like to share some thoughts and configuration examples of how you could use a popular 
syslog  daemon like rsyslog to handle both structured and unstructured logs.  Then I’m going
to look at how you can take those logs, format them in JSON, and index them with 
Elasticsearch – for some fast and easy searching and statistics.  


### On structured logging ###

If we take an unstructured log message, like:

    Joe bought 2 apples

And compare it with a similar one in JSON, like:

    {“name”: “Joe”, “action”: “bought”, “item”: “apples”, “quantity”: 2}

We can immediately spot a couple of advantages and disadvantages of structured logging:

*  If we index these logs, it will be faster and more precise to search for “apples” in the 
“item” field, rather than in the whole document. 

*  At the same time, the structured log will take up more space than the unstructured one.

*  But in most use-cases there will be more applications that would log the same subset of fields. 
So if you want to search for the same user across those applications, it’s nice to be able to pinpoint 
the “name” field everywhere.  

*  And when you add statistics, like who’s the user buying most of our apples, 
that’s when structured logging really becomes useful.

*  Finally, it helps to have a structure when it comes to maintenance. If a new version of the application 
adds a new field, and your log becomes:    
`Joe bought 2 red apples`    
it might break some log-parsing, while structured logs rarely suffer from the same problem.

Enter **CEE** and **Lumberjack**: structured logging within syslog

With syslog, as defined by RFC3164, there is already a structure in the sense that there’s
* a priority value (severity*8 + facility), 
* a header (timestamp and hostname) and 
* a message.

But this usually isn’t the structure we’re looking for.

CEE and Lumberjack are efforts to introduce structured logging to syslog in a backwards-compatible way. 
The process is quite simple: in the message part of the log, one would start with a cookie string `@cee:`, 
followed by an optional space and then a JSON or XML.  From this point on I will talk about JSON, 
since it’s the format that both rsyslog and Elasticsearch prefer.  Here’s a sample CEE-enhanced syslog message:

    @cee: {“foo”: “bar”}

This makes it quite easy to use CEE-enhanced syslog with existing syslog libraries, although there 
are specific libraries like liblumberlog, which make it even easier.  They’ve also defined a list of 
standard fields, and applications should use those fields where they’re applicable – so that you get
the same field names for all applications.  But the schema is free, so you can add custom fields at will.


### CEE-enhanced syslog with rsyslog ##

rsyslog has a module named mmjsonparse for handling CEE-enhanced syslog messages. It checks for the 
“CEE cookie” at the beginning of the message, and then tries to parse the following JSON.  If all is 
well, the fields from that JSON are loaded and you can then use them in templates to extract whatever
information seems important. Fields from your JSON can be accessed like this: $!field-name. 

An example of how they can be used is shown here.

To get started, you need to have at least rsyslog version 6.6.0, and I’d recommend using version 7 
or higher. If you don’t already have that, check out Adiscon’s repositories for RHEL/CentOS and Ubuntu.

Also, mmjsonparse is not enabled by default. If you use the repositories, install the 
rsyslog-mmjsonparse package. If you compile rsyslog from sources, specify `–enable-mmjsonparse` when 
you run the configure script.  In order for that to work you’d probably have to install libjson and 
liblognorm first, depending on your operating system.

For a proof of concept, we can take this config:

    # load needed modules
    module(load=”imuxsock”)        # provides support for local system logging
    module(load=”imklog”)          # provides kernel logging support
    module(load=”mmjsonparse”)     # for parsing CEE-enhanced syslog messages

    # try to parse structured logs
    *.* :mmjsonparse:

    # define a template to print field “foo”
    template(name=”justFoo” type=”list”) {
        property(name=”$!foo”)
        constant(value=”\n”)       # we’ll separate logs with a newline
    }

    # and now let’s write the contents of field “foo” in a file
    *.* action(
            type=”omfile”
            template=”justFoo”
            file=”/var/log/foo”
        )

To see things, better, you can start rsyslog in foreground and in debug mode:

    rsyslogd -dn

And in another terminal, you can send a structured log, then see the value in your file:

    > logger ‘@cee: {“foo”:”bar”}’
    > cat /var/log/foo
    bar

If we send an unstructured log, or an invalid JSON, nothing will be added

    > logger ‘test’
    > logger ‘@cee: test2′
    > cat /var/log/foo
    bar

But you can see in the debug output of rsyslog why:

    mmjsonparse: no JSON cookie: ‘test’
    [...]
    mmjsonparse: toParse: ‘ test2′
    mmjsonparse: Error parsing JSON ‘ test2′: boolean expected


### Indexing logs in Elasticsearch ###

To index our logs in Elasticsearch, we will use an output module of rsyslog called omelasticsearch. 
Like mmjsonparse, it’s not compiled by default, so you will have to add the –enable-elasticsearch 
parameter to the configure script to get it built when you run make. If you use the repositories, 
you can simply install the rsyslog-elasticsearch package.

omelasticsearch expects a valid JSON from your template, to send it via HTTP to Elasticsearch. 
You can select individual fields, like we did in the previous scenario, but you can also select the 
JSON part of the message via the `$!all-json` property. That would produce the message part of the log, 
without the “CEE cookie”.

The configuration below should be good for inserting the syslog message to an Elasticsearch instance 
running on localhost:9200, under the index “system” and type “events“. These are the default options, 
and you can take a look at this tutorial if you need some info on changing them.

    # load needed modules
    module(load=”imuxsock”)        # provides support for local system logging
    module(load=”imklog”)          # provides kernel logging support
    module(load=”mmjsonparse”)     # for parsing CEE-enhanced syslog messages
    module(load=”omelasticsearch”) # for indexing to Elasticsearch

    # try to parse a structured log
    *.* :mmjsonparse:

    # define a template to print all fields of the message
    template(name=”messageToES” type=”list”) {
        property(name=”$!all-json”)
    }

    # write the JSON message to the local ES node
    *.* action(
            type=”omelasticsearch”
            template=”messageToES”
    )

After restarting rsyslog, you can see your JSON will be indexed:

    > logger ‘@cee: {“foo”: “bar”, “foo2″: “bar2″}’
    > curl -XPOST localhost:9200/system/events/_search?q=foo2:bar2 2>/dev/null | sed s/.*_source//
    ” : { “foo”: “bar”, “foo2″: “bar2″ }}]}}

As for unstructured logs, $!all-json will produce a JSON with a field named “msg”, 
having the message as a value:

    > logger test
    > curl -XPOST localhost:9200/system/events/_search?q=test 2>/dev/null | sed s/.*_source//
    ” : { “msg”: “test” }}]}}

It’s “msg” because that’s rsyslog’s property name for the syslog message.


### Including other properties ###

But the message isn’t the only interesting property. I would assume most would want to 
index other information, like the timestamp, severity, or host which generated that message.

To do that, one needs to play with templates and properties. In the future it might be made easier, 
but at the time of this writing (rsyslog 7.2.3), you need to manually craft a valid JSON to pass it 
to omelasticsearch.  For example, if we want to add the timestamp and the syslogtag, a working 
template might look like this:

    template(name=”customTemplate” type=”list”) {
        #- open the curly brackets,
        #- add the timestamp field surrounded with quotes
        #- add the colon which separates field from value
        #- open the quotes for the timestamp itself
        constant(value=”{\”timestamp\”:\”")

        #- add the timestamp from the log,
        # format it in RFC-3339, so that ES detects it by default
        property(name=”timereported” dateFormat=”rfc3339″)

        #- close the quotes for timestamp,
        #- add a comma, then the syslogtag field in the same manner
        constant(value=”\”,\”syslogtag\”:\”")

        #- now the syslogtag field itself
        # and format=”json” will ensure special characters
        # are escaped so they won’t break our JSON
        property(name=”syslogtag” format=”json”)

        #- close the quotes for syslogtag
        #- add a comma
        #- then add our JSON-formatted syslog message,
        # but start from the 2nd position to omit the left
        # curly bracket
        constant(value=”\”,”)

        property(name=”$!all-json” position.from=”2″)
    }


### Summary ###

If you’re interested in searching or analyzing lots of logs, structured logging might help. 
And you can do it with the existing syslog libraries, via CEE-enhanced syslog.  

If you use a newer version of rsyslog, you can parse these logs with mmjsonparse and index
them in Elasticsearch with omelasticsearch.  

If you want to use Logsene, it will consume your structured logs as described in this post.


[[template]]
== Templates

http://www.rsyslog.com/doc/rsyslog_conf_templates.html[Article's Source]

Templates are a key feature of rsyslog. They allow to specify any format a user might want. They 
are also used for dynamic file name generation. Every output in rsyslog uses templates - this holds 
true for files, user messages and so on. The database writer expects its template to be a proper 
SQL statement - so this is highly customizable too. You might ask how does all of this work when no
templates at all are specified. Good question ;) The answer is simple, though. Templates compatible
with the stock syslogd formats are hardcoded into rsyslogd. So if no template is specified, we use 
one of these hardcoded templates. Search for "template_" in syslogd.c and you will find the 
hardcoded ones.

Templates are specified by template() statements. They can also be specified via $Template legacy 
statements. Note that these are scheduled for removal in later versions of rsyslog, so it is 
probably a good idea to avoid them for new uses.

### The template() statement ###

The template() statement is used to define templates. Note that it is a static statement, that means
all templates are defined when rsyslog reads the config file. As such, templates are not affected by
if-statements or config nesting.

The basic structure of the template statement is as follows: 

    template(parameters) 

In addition to this simpler syntax, list templates (to be described below) support an 
extended syntax: 

    template(parameters) {    
        list-descriptions    
    }

Each template has a parameter name, which specifies the templates name, and a parameter type, which
specifies the template type. The name parameter must be unique, and behaviour is unpredictable if 
it is not. The type parameter specifies different template types. Different types simply enable 
different ways to specify the template content. The template type does not affect what an (output) 
plugin can do with it. So use the type that best fits your needs (from a config writing point of 
view!). 

The following types are available:

* list
* subtree
* string
* plugin

The various types are described below.

#### list ####

In this case, the template is generated by a list of constant and variable statements. These follow
the template spec in curly braces. This type is also primarily meant for use with structure-aware 
outputs, like ommongodb. However, it also works perfectly with text-based outputs. We recommend to 
use this mode if more complex property substitutions needs to be done. In that case, the list-based
template syntax is much clearer than the simple string-based one.

The list template contains the template header (with type="list") and is followed by constant and 
property statements, given in curly braces to signify the template statement they belong to. As the
name says, constant statements describe constant text and property describes property access. There
are many options to property, described further below. Most of these options are used to extract 
only partial property contents or to modify the text obtained (like to change its case to upper or 
lower case, only).

To grasp the idea, an actual sample is: 

    template(name="tpl1" type="list") {
        constant(value="Syslog MSG is: '")
        property(name="msg")
        constant(value="', ")
        property(name="timereported" dateFormat="rfc3339" caseConversion="lower")
        constant(value="\n")
    }

This sample is probably primarily targeted at the usual file-based output.

##### constant statement #####

This provides a way to specify constant text. The text is used literally. It is primarily intended 
for text-based output, so that some constant text can be included. For example, if a complex 
template is build for file output, one usually needs to finish it by a newline, which can be 
introduced by a constant statement. 

Here is an actual sample of that use case from the rsylsog testbench: 

    template(name="outfmt" type="list") {
        property(name="$!usr!msgnum")
        constant(value="\n")
    }

The following escape sequences are recogniced inside the constant text:

* **\\** - single backslash

* **\n** - LF

* **\ooo** - (three octal digits) - represents character with this numerical value 
    (e.g. \101 equals "A"). Note that three octal digits must be given (in contrast to some 
    languagues, where between one and three are valid). While we support octal notation, 
    we recommend to use hex notation as this is better known.

* **\xhh** - (where h is a hex digit) - represents character with this numerical value
    (e.g. \x41 equals "A"). Note that two hexadecimal digits must be given (in contrast to 
    some languagues where one or two are valid).

* **...** some others ... list needs to be extended

Note: if an unsupported character follows a backslash, this is treated as an error. Behaviour is 
unpredictable in this case.

To aid usage of the same template both for text-based outputs and structured ones, constant text 
without an "outname" parameter will be ignored when creating the name/value tree for structured 
outputs. So if you want to supply some constant text e.g. to mongodb, you must include an outname, 
as can be seen here: 

    template(name="outfmt" type="list") {
        property(name="$!usr!msgnum")
        constant(value="\n" outname="IWantThisInMyDB")
    }

The "constant" statement supports the following parameters:

* **value** - the constant value to use
* **outname** - output field name (for structured outputs)


##### property statement #####

This statement is used to include property text. It can access all properties. Also, options permit
to specify picking only part of a property or modifying it. It supports the following parameters:

* **name** - the name of the property to access

* **outname** - output field name (for structured outputs)

* **dateformat** - date format to use (only for date-related properties)

* **caseconversion** - permits to convert case of the text. supported values 
    are "lower" and "upper"

* **controlcharacters** - specifies how to handle control characters. Supported values 
    are "escape", which escapes them, "space", which replaces them by a single space, and "drop", 
    which simply removes them from the string.

* **securepath** - used for creating pathnames suitable for use in dynafile templates

* **format** - specifiy format on a field basis. Supported values are "csv", 
    for use when csv-data is generated, "json", which formats proper json content 
    (but without a field header) and "jsonf", which formats as a complete json field.

* **position.from** - obtain substring starting from this position (1 is the first position)

* **position.to** - obtain substring up to this position

* **position.relativeToEnd** - the from and to position is relative to the end 
    of the string instead of the usual start of string. (available since rsyslog v7.3.10)

* **field.number** - obtain this field match

* **field.delimiter** - decimal value of delimiter character for field extraction

* **regex.expression** - expression to use

* **regex.type** - either ERE or BRE

* **regex.nomatchmode** - what to do if we have no match

* **regex.match** - match to use

* **regex.submatch** - submatch to use

* **droplastlf** - drop a trailing LF, if it is present

* **mandatory** - signifies a field as mandatory. If set to "on", this field will
    always be present in data passed to structured outputs, even if it is empty. 
    If "off" (the default) empty fields will not be passed to structured outputs. 
    This is especially useful for outputs that support dynamic schemas (like ommongodb).

* **spifno1stsp** - expert options for RFC3164 template processing

#### subtree ####

Available since rsyslog 7.1.4

In this case, the template is generated based on a complete (CEE) subtree. This type of template is
most useful for outputs that know how to process hierarchical structure, like ommongodb. With that 
type, the parameter subtree must be specified, which tells which subtree to use. For example 
`template(name="tpl1" type="subtree" subtree="$!")` includes all CEE data, while 
`template(name="tpl2" type="subtree" subtree="$!usr!tpl2")` includes only the subtree starting at 
$!usr!tpl2. The core idea when using this type of template is that the actual data is prefabricated
via set and unset script statements, and the resulting strucuture is then used inside the template.
This method MUST be used if a complete subtree needs to be placed directly into the object's root. 
With all other template types, only subcontainers can be generated. Note that subtree type can also
be used with text-based outputs, like omfile. HOWEVER, you do not have any capability to specify 
constant text, and as such cannot include line breaks. As a consequence, using this template type 
for text outputs is usually only useful for debugging or very special cases (e.g. where the text is
interpreted by a JSON parser later on).

##### Use case #####

A typical use case is to first create a custom subtree and then include it into the template, like 
in this small example: 

    set $!usr!tpl2!msg = $msg; 
    set $!usr!tpl2!dataflow = field($msg, 58, 2); 
    template(name="tpl2" type="subtree" subtree="$!usr!tpl2")

Here, we assume that $msg contains various fields, and the data from a field is to be extracted and 
stored - together with the message - as field content.

#### string ####

This closely resembles the legacy template statement. It has a mandatory parameter string, which 
holds the template string to be applied. A template string is a mix of constant text and replacement
variables (see property replacer). These variables are taken from message or other dynamic content 
when the final string to be passed to a plugin is generated. String-based templates are a great way
to specify textual content, especially if no complex manipulation to properties is necessary. 
Full details on how to specify template text can be found below.

##### Config example #####

    template(
        name="tpl3"
        type="string"
        string="%TIMESTAMP:::date-rfc3339% %HOSTNAME% \
                %syslogtag%%msg:::sp-if-no-1st-sp%%msg:::drop-last-lf%\n"
    )

#### plugin ####

In this case, the template is generated by a plugin (which is then called a "strgen" or 
"string generator").  The format is fix as it is coded. While this is inflexible, it provides 
superior performance, and is often used for that reason (not that "regular" templates are slow - 
but in very demanding environments that "last bit" can make a difference). Refer to the plugin's 
documentation for further details. For this type, the paramter plugin must be specified and must 
contain the name of the plugin as it identifies itself. Note that the plugin must be loaded prior
to being used inside a template.

##### Config example #####

    template(
        name="tpl4" 
        type="plugin" 
        plugin="mystrgen"
    )


##### options #####
The <options> part is optional. It carries options influencing the template as whole and is 
part of the template parameters. See details below. Be sure NOT to mistake template options
with property options - the latter ones are processed by the property replacer and apply to 
a SINGLE property, only (and not the whole template).

Template options are case-insensitive. Currently defined are:

* **option.sql** - format the string suitable for a SQL statement in MySQL format. 
    This will replace single quotes ("'") and the backslash character by their 
    backslash-escaped counterpart ("\'" and "\\") inside each field. Please note that
    in MySQL configuration, the NO_BACKSLASH_ESCAPES mode must be turned off for this 
    format to work (this is the default).

* **option.stdsql** - format the string suitable for a SQL statement that is to be
    sent to a standards-compliant sql server. This will replace single quotes ("'") by
    two single quotes ("''") inside each field. You must use stdsql together with MySQL
    if in MySQL configuration the NO_BACKSLASH_ESCAPES is turned on.

* **option.json** - format the string suitable for a json statement. 
    This will replace single quotes ("'") by two single quotes ("''") inside each field.

At no time, multiple template option should be used. This can cause unpredictable behaviour and is 
against all logic.

Either the sql or stdsql  option must be specified when a template is used for writing to a 
database, otherwise injection might occur. Please note that due to the unfortunate fact that 
several vendors have violated the sql standard and introduced their own escape methods, it is 
impossible to have a single option doing all the work.  So you yourself must make sure you are 
using the right format. If you choose the wrong one, you are still vulnerable to sql injection.

Please note that the database writer *checks* that the sql option is present in the template. 
If it is not present, the write database action is disabled. This is to guard you against accidental
forgetting it and then becoming vulnerable to SQL injection. The sql option can also be useful with 
files - especially if you want to import them into a database on another machine for performance 
reasons. However, do NOT use it if you do not have a real need for it - among others, it takes some 
toll on the processing time. Not much, but on a really busy system you might notice it ;)

The default template for the write to database action has the sql option set. As we currently 
support only MySQL and the sql option matches the default MySQL configuration, this is a good 
choice. However, if you have turned on NO_BACKSLASH_ESCAPES in your MySQL config, you need to 
supply a template with the stdsql option. Otherwise you will become vulnerable to SQL injection. 

To escape:

    % = \%
    \ = \\ --> '\' is used to escape (as in C)
>  

    template (
        name="TraditionalFormat" 
        type="string" 
        string="%timegenerated% %HOSTNAME% %syslogtag%%msg%\n"
    )


### Examples ###

#### Standard Template for Writing to Files ####

    template(name="FileFormat" 
             type="list") {
        property(name="timestamp" dateFormat="rfc3339")
        constant(value=" ")
        property(name="hostname")
        constant(value=" ")
        property(name="syslogtag")
        constant(value=" ")
        property(name="msg" spifno1stsp="on" )
        property(name="msg" droplastlf="on" )
        constant(value="\n")
    }

The equivalent string template looks like this: 

    template(
        name="FileFormat" 
        type="string"
        string= "%TIMESTAMP% %HOSTNAME% %syslogtag%%msg:::sp-if-no-1st-sp%%msg:::drop-last-lf%\n"
    )

Note that the template string itself must be on a single line.

#### Standard Template for Forwarding to a Remote Host (RFC3164 mode) ####

    template(name="ForwardFormat" type="list") {
        constant(value="<")
        property(name="PRI")
    constant(value="<")
    property(name="timestamp" dateFormat="rfc3339")
    constant(value=" ")
    property(name="hostname")
    constant(value=" ")
    property(name="syslogtag" position.from="1" position.to="32")
    constant(value=" ")
        property(name="msg" spifno1stsp="on" )
    }

The equivalent string template looks like this: 

    template(
        name="forwardFormat" 
        type="string"
        string="<%PRI%>%TIMESTAMP:::date-rfc3339% %HOSTNAME% \
                %syslogtag:1:32%%msg:::sp-if-no-1st-sp%%msg%"
    )

Note that the template string itself must be on a single line.

#### Standard Template for write to the MySQL database ####

    template(name="StdSQLformat" type="list" option.sql="on") {
        constant(value="insert into SystemEvents (Message, Facility, FromHost, Priority, \
                                                  DeviceReportedTime, ReceivedAt, InfoUnitID, \
                                                  SysLogTag)")
        constant(value=" values ('")
        property(name="msg")
        constant(value="', ")
        property(name="syslogfacility")
        constant(value=", '")
        property(name="hostname")
        constant(value="', ")
        property(name="syslogpriority")
        constant(value=", '")
        property(name="timereported" dateFormat="mysql")
        constant(value="', '")
        property(name="timegenerated" dateFormat="mysql")
        constant(value="', ")
        property(name="iut")
        constant(value=", '")
        property(name="syslogtag")
        constant(value="')")
    }

The equivalent string template looks like this: 

    template(
        name="stdSQLformat" 
        type="string" 
        option.sql="on"
        string="insert into SystemEvents (Message, Facility, FromHost, Priority, \
                                          DeviceReportedTime, ReceivedAt, InfoUnitID, SysLogTag) \
                                          values ('%msg%', %syslogfacility%, '%HOSTNAME%', \
                                                   %syslogpriority%, '%timereported:::date-mysql%', \
                                                   '%timegenerated:::date-mysql%', %iut%, \
                                                   '%syslogtag%')"
    )

Note that the template string itself must be on a single line.


### legacy format ###

In pre v6-versions of rsyslog, you need to use the $template statement to configure templates. 
They provide the equivalent to string- and plugin-based templates. The legacy syntax continous
to work in v7, however we recommend to avoid legacy format for newly written config files. 
Legacy and current config statements can coexist within the same config file.

The general format is 

    $template name,param[,options]

where "name" is the template name and "param" is a single parameter that specifies template content. 
The optional "options" part is used to set template options.

#### string ####

The parameter is the same string that with the current-style format you specify in the string 
parameter, for example: 

    $template strtpl,"PRI: %pri%, MSG: %msg%\n"

Note that list templates are not available in legacy format, so you need to use complex property 
replacer constructs to do complex things.


#### plugin ####

This is equivalent to the "plugin"-type template directive. Here, the parameter is the plugin name,
with an equal sign prepended. An example is: 

    $template plugintpl,=myplugin


### Reserved Template Names ###

Template names beginning with "RSYSLOG_" are reserved for rsyslog use. Do NOT use them if, otherwise 
you may receive a conflict in the future (and quite unpredictable behaviour). 
There is a small set of pre-defined templates that you can use without the need to define it:

* **RSYSLOG_TraditionalFileFormat** - the "old style" default log file format with low-precision 
    timestamps

* **RSYSLOG_FileFormat** - a modern-style logfile format similar to TraditionalFileFormat, 
    both with high-precision timestamps and timezone information

* **RSYSLOG_TraditionalForwardFormat** - the traditional forwarding format with low-precision 
    timestamps. Most useful if you send messages to other syslogd's or rsyslogd below version 3.12.5.

* **RSYSLOG_SysklogdFileFormat** - sysklogd compatible log file format. 
    If used with options: $SpaceLFOnReceive on; $EscapeControlCharactersOnReceive off; 
    $DropTrailingLFOnReception off, the log format will conform to sysklogd log format.

* **RSYSLOG_ForwardFormat** - a new high-precision forwarding format very similar to the 
    traditional one, but with high-precision timestamps and timezone information. 
    Recommended to be used when sending messages to rsyslog 3.12.5 or above.

* **RSYSLOG_SyslogProtocol23Format** - the format specified in IETF's internet-draft 
    ietf-syslog-protocol-23, which is assumed to be come the new syslog standard RFC. 
    This format includes several improvements. The rsyslog message parser understands 
    this format, so you can use it together with all relatively recent versions of rsyslog. 
    Other syslogd's may get hopelessly confused if receiving that format, so check before 
    you use it. Note that the format is unlikely to change when the final RFC comes out, 
    but this may happen.

* **RSYSLOG_DebugFormat** - a special format used for troubleshooting property problems. 
    This format is meant to be written to a log file. 
    Do not use for production or remote forwarding.

The following is legacy documentation soon to be integrated.

Starting with 5.5.6, there are actually two different types of templates:

* string based
* string-generator module based

String-generator module based templates have been introduced in 5.5.6. They permit a string 
generator, actually a C "program", the generate a format. Obviously, it is more work required to 
code such a generator, but the reward is speed improvement. If you do not need the ultimate 
throughput, you can forget about string generators (so most people never need to know what 
they are). You may just be interested in learning that for the most important default formats, 
rsyslog already contains highly optimized string generators and these are called without any need 
to configure anything. But if you have written (or purchased) a string generator module, you need 
to know how to call it. Each such module has a name, which you need to know (look it up in the 
module doc or ask the developer). Let's assume that "mystrgen" is the module name. Then you can 
define a template for that strgen in the following way:

    template(
        name="MyTemplateName" 
        type="plugin" 
        string="mystrgen"
    )

#### Legacy example ####

    $template MyTemplateName,=mystrgen

(Of course, you must have first loaded the module via $ModLoad).
The important part is the equal sign in the legacy format: it tells the rsyslog config parser that
no string follows but a strgen module name.

There are no additional parameters but the module name supported. This is because there is 
no way to customize anything inside such a "template" other than by modifying the code of 
the string generator.

So for most use cases, string-generator module based templates are not the route to take. Usually, 
we use string based templates instead. This is what the rest of the documentation now talks about.

A template consists of a template directive, a name, the actual template text and optional options. 
A sample is:

    template(
        name="MyTemplateName" 
        type="string" 
        string="Example: Text %property% some more text\n" 
        options
    )

#### Legacy example ####

    $template MyTemplateName,"\7Text %property% some more text\n",<options>

The "template" (legacy: $template) is the template directive. It tells rsyslog that this line 
contains a template. "MyTemplateName" is the template name. All other config lines refer to 
this name. The text within "string" is the actual template text. The backslash is an escape 
character, much as it is in C. It does all these "cool" things. For example, \7 rings the bell 
(this is an ASCII value), \n is a new line. C programmers and perl coders have the advantage of 
knowing this, but the set in rsyslog is a bit restricted currently.

All text in the template is used literally, except for things within percent signs. These are 
properties and allow you access to the contents of the syslog message. Properties are accessed via 
the property replacer (nice name, huh) and it can do cool things, too. For example, it can pick a 
substring or do date-specific formatting. More on this is below, on some lines of the 
property replacer.


Properties can be accessed by the **property replacer** (see there for details).

Templates can be used in the form of a list as well. This has been introduced with 6.5.0 The list 
consists of two parts which are either a constant or a property. The constants are taking the part
of "text" that you usually enter in string-based templates. The properties stay variable, as they 
are a substitute for different values of a certain type. This type of template is extremely useful 
for complicated cases, as it helps you to easily keep an overview over the template. Though, it has 
the disadvantage of needing more effort to create it.


#### Config example ####

    template(name="MyTemplate" 
             type="list" 
             option.json="off") { 
        constant(value="Test: ") 
        property(name="msg" outname="mymessage") 
        constant(value=" --!!!-- ") 
        property(name="timereported" dateFormat="rfc3339" caseConversion="lower") 
        constant(value="\n") 
    }

First, the general template option will be defined. The values of the template itself get defined 
in the curly brackets. As it can be seen, we have constants and properties in exchange. 
Whereas constants will be filled with a value and probably some options, properties do direct to 
a property and the options that could be needed additional format definitions.

We suggest to use separate lines for all constants and properties. This helps to keep a good 
overview over the different parts of the template. Though, writing it in a single line will work, 
it is much harder to debug if anything goes wrong with the template.

Please note that templates can also be used to generate selector lines with dynamic file names. 
For example, if you would like to split syslog messages from different hosts to different files 
(one per host), you can define the following template:

    template (
        name="DynFile" 
        type="string" 
        string="/var/log/system-%HOSTNAME%.log"
    )


#### Legacy example: ####

    $template DynFile,"/var/log/system-%HOSTNAME%.log"

This template can then be used when defining an output selector line. It will result in something 
like "/var/log/system-localhost.log"

#### Legacy String-based Template Samples ####

This section provides some default templates in legacy format, as used in rsyslog previous to 
version 6. Note that this format is still supported, so there is no hard need to upgrade existing 
configurations. However, it is strongly recommended that the legacy constructs are not used when 
crafting new templates. Note that each $Template statement is on a single line, but probably broken
accross several lines for display purposes by your browsers. Lines are separated by empty lines. 
Keep in mind, that line breaks are important in legacy format.

    $template FileFormat,"%TIMESTAMP:::date-rfc3339% %HOSTNAME% \
                          %syslogtag%%msg:::sp-if-no-1st-sp%%msg:::drop-last-lf%\n" 

    $template TraditionalFileFormat,"%TIMESTAMP% %HOSTNAME% %syslogtag%\
                                     %msg:::sp-if-no-1st-sp%%msg:::drop-last-lf%\n" 

    $template ForwardFormat,"<%PRI%>%TIMESTAMP:::date-rfc3339% %HOSTNAME% \
                             %syslogtag:1:32%%msg:::sp-if-no-1st-sp%%msg%" 

    $template TraditionalForwardFormat,"<%PRI%>%TIMESTAMP% %HOSTNAME% \
                                        %syslogtag:1:32%%msg:::sp-if-no-1st-sp%%msg%" 

    $template StdSQLFormat,"insert into SystemEvents (Message, Facility, FromHost, Priority, \
                                                      DeviceReportedTime, ReceivedAt, InfoUnitID, \
                                                      SysLogTag) \
                                                      values ('%msg%', %syslogfacility%, \
                                                              '%HOSTNAME%', %syslogpriority%, \
                                                              '%timereported:::date-mysql%', \
                                                              '%timegenerated:::date-mysql%', \
                                                               %iut%, '%syslogtag%')",\
                            SQL



[[templatebind]]
== How to bind a template

This little FAQ describe how to bind a template.
First with the new template format “list” and then with the old “legacy” format.

First off all you have to define a template for example for specify output.

Here is an example template in the list format:

    template(
        name=”FileFormat”
        type=”list”
    ) 
    {
        property(name=”timestamp” dateFormat=”rfc3339″)
        constant(value=” “)
        property(name=”hostname”)
        constant(value=” “)
        property(name=”syslogtag”)
        constant(value=” “)
        property(name=”msg” spifno1stsp=”on” )
        property(name=”msg” droplastlf=”on” )
        constant(value=”\n”)
    }

Then you have to bind the template to an action. The Syntax to bind a template is:

    Action;name-of-template

Here is an example action with a example-template:

    *.* action(type=”omfile” file=”/var/log/all-messages.log”);Name-of-your-template

In the configuration it should looks like this:

    template(name=”FileFormat” type=”list”) {
        property(name=”timestamp” dateFormat=”rfc3339″)
        constant(value=” “)
        property(name=”hostname”)
        constant(value=” “)
        property(name=”syslogtag”)
        constant(value=” “)
        property(name=”msg” spifno1stsp=”on” )
        property(name=”msg” droplastlf=”on” )
        constant(value=”\n”)
    }
    action(
        type=”omfile” 
        file=”/var/log/all-msgs.log”
    );FileFormat
    “

Here is an example for the legacy format

Here is an example template in the legacy format:

    $template ExampleFormat,”%timereported:::date-rfc3339% %HOSTNAME% %msg%”

Here is an example action with a example-template:

    *.* /var/log/all-messages.log;Your-Template-Name

In the Configuration it looks like this:

    “$template ExampleFormat,”%timereported:::date-rfc3339% %HOSTNAME% %msg%”
    *.* /var/log/all-messages.log;ExampleFormat”



[[textinput]]
== Using the Text File Input Module

Friday, March 25th, 2011

Log files should be processed by rsyslog. Here is some information on how the file monitor works. 
This will only describe setting up the Text File Input Module. Further configuration like processing 
rules or output methods will not be described.

Things to think about

The configuration given here should be placed on top of the rsyslog.conf file.

Config Statements

    module(load="imfile" PollingInterval="10")
    # needs to be done just once. PollingInterval is a module directive and is only 
    # set once when loading the module
    # File 1
    input(
        type="imfile" 
        File="/path/to/file1" 
        Tag="tag1" 
        StateFile="/var/spool/rsyslog/statefile1" 
        Severity="error" 
        Facility="local7"
    )
    # File 2
    input(
        type="imfile" File="/path/to/file2" 
        Tag="tag2" 
        StateFile="/var/spool/rsyslog/statefile2")
        # ... and so on ...
        #
        
How it works

The configuration for using the Text File Input Module is very extensive. At the beginning of your 
rsyslog configuration file, you always load the modules. There you need to load the module for 
Text File Input as well. Like all other modules, this has to be made just once. Please note that 
the directive PollingInterval is a module directive which needs to be set when loading the module.

    module(load="imfile" PollingInterval="10")

Next up comes the input and its parameters. We configure a input of a certain type and then set the 
parameters to be used by this input. This is basically the same principle for all inputs:

    # File 1
    input(type="imfile" File="/path/to/file1" 
        Tag="tag1" 
        StateFile="/var/spool/rsyslog/statefile1" 
        Severity="error" 
        Facility="local7"
    )
        
File specifies, the path and name of the text file that should be monitored. The file name must 
be absolute.

Tag will set a tag in front of each message pulled from the file. If you want a colon after the 
tag you must set it as well, it will not be added automatically.

StateFile will create a file where rsyslog keeps track of the position it currently is in a file. 
You only need to set the filename. This file always is created in the rsyslog working directory 
(configurable via $WorkDirectory). This file is important so rsyslog will not pull messages from 
the beginning of the file when being restarted.

Severity will give all log messages of a file the same severity. This is optional. 
By default all mesages will be set to “notice”.

Facility gives alle log messages of a file the same facility. Again, this is optional. 
By default all messages will be set to “local0″.

These statements are needed for monitoring a file. There are other statements described in the doc, 
which you might want to use. If you want to monitor another file the statements must be repeated.

Since the files cannot be monitored in genuine real time (which generates too much processing effort) 
you need to set a polling interval:

PollingInterval 10

This is a module setting and it defines the interval in which the log files will be polled. 
By default this value is set to 10 seconds. If you want this to get more near realtime, you 
can decrease the value, though this is not suggested due to increasing processing load. 
Setting this to 0 is supported, but not suggested. Rsyslog will continue reading the file 
as long as there are unprocessed messages in it. The interval only takes effect once rsyslog 
reaches the end of the file.

Important

The StateFile needs to be unique for every file that is monitored. If not, strange things could happen.


[[tls]]
== gtls Network Stream Driver

This [network stream driver](http://www.rsyslog.com/doc/netstream.html) implements a TLS protected transport via the [GnuTLS library](http://www.gnu.org/software/gnutls/).

**Supported Driver Modes**
* 0 - unencrypted transmission (just like <a href="ns_ptcp.html">ptcp</a> driver)</li>
* 1 - TLS-protected operation</li>

Note: mode 0 does not provide any benefit over the ptcp driver. This
mode exists for technical reasons, but should not be used. It may be
removed in the future.

<span style="font-weight: bold;">Supported Authentication Modes</span><br>
<ul>
<li><span style="font-weight: bold;">anon</span>
- anonymous authentication as
described in IETF's draft-ietf-syslog-transport-tls-12 Internet draft</li>
<li><span style="font-weight: bold;">x509/fingerprint</span>
- certificate fingerprint authentication as
described in IETF's draft-ietf-syslog-transport-tls-12 Internet draft</li>
<li><span style="font-weight: bold;">x509/certvalid</span>
- certificate validation only</li>
<li><span style="font-weight: bold;">x509/name</span>
- certificate validation and subject name authentication as
described in IETF's draft-ietf-syslog-transport-tls-12 Internet draft
</li>
</ul>

Note: "anon" does not permit to authenticate the remote peer. As such,
this mode is vulnerable to man in the middle attacks as well as
unauthorized access. It is recommended NOT to use this mode.

x509/certvalid is a nonstandard mode. It validates the remote
peers certificate, but does not check the subject name. This is 
weak authentication that may be useful in scenarios where multiple
devices are deployed and it is sufficient proof of authenticity when
their certificates are signed by the CA the server trusts. This is
better than anon authentication, but still not recommended.

**Known Problems**

Even in x509/fingerprint mode, both the client and sever
certificate currently must be signed by the same root CA. This is an
artifact of the underlying GnuTLS library and the way we use it. It is
expected that we can resolve this issue in the future.


# Network Stream Drivers #

Network stream drivers are a layer between various parts of rsyslogd (e.g. the imtcp module) 
and the transport layer. They provide sequenced delivery, authentication and confidentiality 
to the upper layers. Drivers implement different capabilities.

Users need to know about netstream drivers because they need to configure the proper driver, 
and proper driver properties, to achieve desired results (e.g. a TLS-protected syslog transmission).

The following drivers exist:
* **ptcp** - the plain tcp network transport (no security)
* **gtls** - a secure TLS transport implemented via the GnuTLS library


# ptcp Network Stream Driver #

This network stream driver implement a plain tcp transport without security properties.

Supported Driver Modes

* **0** - unencrypted trasmission

**Supported Authentication Modes**

* **anon** - no authentication

[[understandingques]]
== Understanding rsyslog Queues

Rsyslog uses queues whenever two activities need to be loosely coupled. 
With a queue, one part of the system "produces" something while another part "consumes" this something. 
The "something" is most often syslog messages, but queues may also be used for other purposes.

This document provides a good insight into technical details, operation modes and implications. 
In addition to it, an rsyslog queue concepts overview document exists which tries to explain 
queues with the help of some analogies. This may probably be a better place to start reading 
about queues. I assume that once you have understood that document, the material here will be 
much easier to grasp and look much more natural.

The most prominent example is the main message queue. 
Whenever rsyslog receives a message (e.g. locally, via UDP, TCP or in whatever else way), 
it places these messages into the main message queue. Later, it is dequeued by the 
rule processor, which then evaluates which actions are to be carried out. 
In front of each action, there is also a queue, which potentially de-couples the filter
processing from the actual action (e.g. writing to file, database or forwarding to another host).

### Where are Queues Used?

Currently, queues are used for the main message queue and for the actions.

There is a single main message queue inside rsyslog. Each input module delivers messages to it. The main message queue worker filters messages based on rules specified in rsyslog.conf and dispatches them to the individual action queues. Once a message is in an action queue, it is deleted from the main message queue.

There are multiple action queues, one for each configured action. By default, these queues operate in direct (non-queueing) mode. Action queues are fully configurable and thus can be changed to whatever is best for the given use case.

Future versions of rsyslog will most probably utilize queues at other places, too.

Wherever "<object>"  is used in the config file statements, substitute "<object>" with either "MainMsg" or "Action". The former will set main message queue parameters, the later parameters for the next action that will be created. Action queue parameters can not be modified once the action has been specified. For example, to tell the main message queue to save its content on shutdown, use $MainMsgQueueSaveOnShutdown on".

If the same parameter is specified multiple times before a queue is created, the last one specified takes precedence. The main message queue is created after parsing the config file and all of its potential includes. An action queue is created each time an action selector is specified. Action queue parameters are reset to default after an action queue has been created (to provide a clean environment for the next action).

Not all queues necessarily support the full set of queue configuration parameters, because not all are applicable. For example, in current output module design, actions do not support multi-threading. Consequently, the number of worker threads is fixed to one for action queues and can not be changed.

### Queue Modes

Rsyslog supports different queue modes, some with submodes. Each of them has specific advantages and disadvantages. Selecting the right queue mode is quite important when tuning rsyslogd. The queue mode (aka "type") is set via the "$<object>QueueType" config directive.

### Direct Queues ###
Direct queues are non-queuing queues. A queue in direct mode does neither queue nor buffer any of the queue elements but rather passes the element directly (and immediately) from the producer to the consumer. This sounds strange, but there is a good reason for this queue type.

Direct mode queues allow to use queues generically, even in places where queuing is not always desired. A good example is the queue in front of output actions. While it makes perfect sense to buffer forwarding actions or database writes, it makes only limited sense to build up a queue in front of simple local file writes. Yet, rsyslog still has a queue in front of every action. So for file writes, the queue mode can simply be set to "direct", in which case no queuing happens.

Please note that a direct queue also is the only queue type that passes back the execution return code (success/failure) from the consumer to the producer. This, for example, is needed for the backup action logic. Consequently, backup actions require the to-be-checked action to use a "direct" mode queue.

To create a direct queue, use the "$<object>QueueType Direct" config directive.

### Disk Queues ###
Disk queues use disk drives for buffering. The important fact is that the always use the disk and do not buffer anything in memory. Thus, the queue is ultra-reliable, but by far the slowest mode. For regular use cases, this queue mode is not recommended. It is useful if log data is so important that it must not be lost, even in extreme cases.

When a disk queue is written, it is done in chunks. Each chunk receives its individual file. Files are named with a prefix (set via the "$<object>QueueFilename" config directive) and followed by a 7-digit number (starting at one and incremented for each file). Chunks are 10mb by default, a different size can be set via the"$<object>QueueMaxFileSize" config directive. Note that the size limit is not a sharp one: rsyslog always writes one complete queue entry, even if it violates the size limit. So chunks are actually a little but (usually less than 1k) larger then the configured size. Each chunk also has a different size for the same reason. If you observe different chunk sizes, you can relax: this is not a problem.

Writing in chunks is used so that processed data can quickly be deleted and is free for other uses - while at the same time keeping no artificial upper limit on disk space used. If a disk quota is set (instructions further below), be sure that the quota/chunk size allows at least two chunks to be written. Rsyslog currently does not check that and will fail miserably if a single chunk is over the quota.

Creating new chunks costs performance but provides quicker ability to free disk space. The 10mb default is considered a good compromise between these two. However, it may make sense to adapt these settings to local policies. For example, if a disk queue is written on a dedicated 200gb disk, it may make sense to use a 2gb (or even larger) chunk size.

Please note, however, that the disk queue by default does not update its housekeeping structures every time it writes to disk. This is for performance reasons. In the event of failure, data will still be lost (except when manually is mangled with the file structures). However, disk queues can be set to write bookkeeping information on checkpoints (every n records), so that this can be made ultra-reliable, too. If the checkpoint interval is set to one, no data can be lost, but the queue is exceptionally slow.

Each queue can be placed on a different disk for best performance and/or isolation. This is currently selected by specifying different $WorkDirectory config directives before the queue creation statement.

To create a disk queue, use the "$<object>QueueType Disk" config directive. Checkpoint intervals can be specified via "$<object>QueueCheckpointInterval", with 0 meaning no checkpoints. Note that disk-based queues can be made very reliable by issuing a (f)sync after each write operation. Starting with version 4.3.2, this can be requested via "<object>QueueSyncQueueFiles on/off with the default being off. Activating this option has a performance penalty, so it should not be turned on without reason.

### In-Memory Queues ###
In-memory queue mode is what most people have on their mind when they think about computing queues. Here, the enqueued data elements are held in memory. Consequently, in-memory queues are very fast. But of course, they do not survive any program or operating system abort (what usually is tolerable and unlikely). Be sure to use an UPS if you use in-memory mode and your log data is important to you. Note that even in-memory queues may hold data for an infinite amount of time when e.g. an output destination system is down and there is no reason to move the data out of memory (lying around in memory for an extended period of time is NOT a reason). Pure in-memory queues can't even store queue elements anywhere else than in core memory.

There exist two different in-memory queue modes: LinkedList and FixedArray. Both are quite similar from the user's point of view, but utilize different algorithms.

A FixedArray queue uses a fixed, pre-allocated array that holds pointers to queue elements. The majority of space is taken up by the actual user data elements, to which the pointers in the array point. The pointer array itself is comparatively small. However, it has a certain memory footprint even if the queue is empty. As there is no need to dynamically allocate any housekeeping structures, FixedArray offers the best run time performance (uses the least CPU cycle). FixedArray is best if there is a relatively low number of queue elements expected and performance is desired. It is the default mode for the main message queue (with a limit of 10,000 elements).

A LinkedList queue is quite the opposite. All housekeeping structures are dynamically allocated (in a linked list, as its name implies). This requires somewhat more runtime processing overhead, but ensures that memory is only allocated in cases where it is needed. LinkedList queues are especially well-suited for queues where only occasionally a than-high number of elements need to be queued. A use case may be occasional message burst. Memory permitting, it could be limited to e.g. 200,000 elements which would take up only memory if in use. A FixedArray queue may have a too large static memory footprint in such cases.

In general, it is advised to use LinkedList mode if in doubt. The processing overhead compared to FixedArray is low and may be outweigh by the reduction in memory use. Paging in most-often-unused pointer array pages can be much slower than dynamically allocating them.

To create an in-memory queue, use the "$<object>QueueType LinkedList" or  "$<object>QueueType FixedArray" config directive.

### Disk-Assisted Memory Queues ###
If a disk queue name is defined for in-memory queues (via $<object>QueueFileName), they automatically become "disk-assisted" (DA). In that mode, data is written to disk (and read back) on an as-needed basis.

Actually, the regular memory queue (called the "primary queue") and a disk queue (called the "DA queue") work in tandem in this mode. Most importantly, the disk queue is activated if the primary queue is full or needs to be persisted on shutdown. Disk-assisted queues combine the advantages of pure memory queues with those of  pure disk queues. Under normal operations, they are very fast and messages will never touch the disk. But if there is need to, an unlimited amount of messages can be buffered (actually limited by free disk space only) and data can be persisted between rsyslogd runs.

With a DA-queue, both disk-specific and in-memory specific configuration parameters can be set. From the user's point of view, think of a DA queue like a "super-queue" which does all within a single queue [from the code perspective, there is some specific handling for this case, so it is actually much like a single object].

DA queues are typically used to de-couple potentially long-running and unreliable actions (to make them reliable). For example, it is recommended to use a disk-assisted linked list in-memory queue in front of each database and "send via tcp" action. Doing so makes these actions reliable and de-couples their potential low execution speed from the rest of your rules (e.g. the local file writes). There is a howto on massive database inserts which nicely describes this use case. It may even be a good read if you do not intend to use databases.

With DA queues, we do not simply write out everything to disk and then run as a disk queue once the in-memory queue is full. A much smarter algorithm is used, which involves a "high watermark" and a "low watermark". Both specify numbers of queued items. If the queue size reaches high watermark elements, the queue begins to write data elements to disk. It does so until it reaches the low water mark elements. At this point, it stops writing until either high water mark is reached again or the on-disk queue becomes empty, in which case the queue reverts back to in-memory mode, only. While holding at the low watermark, new elements are actually enqueued in memory. They are eventually written to disk, but only if the high water mark is ever reached again. If it isn't, these items never touch the disk. So even when a queue runs disk-assisted, there is in-memory data present (this is a big difference to pure disk queues!).

This algorithm prevents unnecessary disk writes, but also leaves some additional buffer space for message bursts. Remember that creating disk files and writing to them is a lengthy operation. It is too lengthy to e.g. block receiving UDP messages. Doing so would result in message loss. Thus, the queue initiates DA mode, but still is able to receive messages and enqueue them - as long as the maximum queue size is not reached. The number of elements between the high water mark and the maximum queue size serves as this "emergency buffer". Size it according to your needs, if traffic is very bursty you will probably need a large buffer here. Keep in mind, though, that under normal operations these queue elements will probably never be used. Setting the high water mark too low will cause disk-assistance to be turned on more often than actually needed.

The water marks can be set via the "$<object>QueueHighWatermark" and  "$<object>QueueHighWatermark" configuration file directives. Note that these are actual numbers, not precentages. Be sure they make sense (also in respect to "$<object>QueueSize"), as rsyslodg does currently not perform any checks on the numbers provided. It is easy to screw up the system here (yes, a feature enhancement request is filed ;)).

### Limiting the Queue Size

All queues, including disk queues, have a limit of the number of elements they can enqueue. This is set via the "$<object>QueueSize" config parameter. Note that the size is specified in number of enqueued elements, not their actual memory size. Memory size limits can not be set. A conservative assumption is that a single syslog messages takes up 512 bytes on average (in-memory, NOT on the wire, this *is* a difference).

Disk assisted queues are special in that they do not have any size limit. The enqueue an unlimited amount of elements. To prevent running out of space, disk and disk-assisted queues can be size-limited via the "$<object>QueueMaxDiskSpace" configuration parameter. If it is not set, the limit is only available free space (and reaching this limit is currently not very gracefully handled, so avoid running into it!). If a limit is set, the queue can not grow larger than it. Note, however, that the limit is approximate. The engine always writes complete records. As such, it is possible that slightly more than the set limit is used (usually less than 1k, given the average message size). Keeping strictly on the limit would be a performance hurt, and thus the design decision was to favour performance. If you don't like that policy, simply specify a slightly lower limit (e.g. 999,999K instead of 1G).

In general, it is a good idea to limit the pysical disk space even if you dedicate a whole disk to rsyslog. That way, you prevent it from running out of space (future version will have an auto-size-limit logic, that then kicks in in such situations).

### Worker Thread Pools

Each queue (except in "direct" mode) has an associated pool of worker threads. Worker threads carry out the action to be performed on the data elements enqueued. As an actual sample, the main message queue's worker task is to apply filter logic to each incoming message and enqueue them to the relevant output queues (actions).

Worker threads are started and stopped on an as-needed basis. On a system without activity, there may be no worker at all running. One is automatically started when a message comes in. Similarily, additional workers are started if the queue grows above a specific size. The "$<object>QueueWorkerThreadMinimumMessages"  config parameter controls worker startup. If it is set to the minimum number of elements that must be enqueued in order to justify a new worker startup. For example, let's assume it is set to 100. As long as no more than 100 messages are in the queue, a single worker will be used. When more than 100 messages arrive, a new worker thread is automatically started. Similarily, a third worker will be started when there are at least 300 messages, a forth when reaching 400 and so on.

It, however, does not make sense to have too many worker threads running in parall. Thus, the upper limit ca be set via "$<object>QueueWorkerThreads". If it, for example, is set to four, no more than four workers will ever be started, no matter how many elements are enqueued.

Worker threads that have been started are kept running until an inactivity timeout happens. The timeout can be set via "$<object>QueueWorkerTimeoutThreadShutdown" and is specified in milliseconds. If you do not like to keep the workers running, simply set it to 0, which means immediate timeout and thus immediate shutdown. But consider that creating threads involves some overhead, and this is why we keep them running. If you would like to never shutdown any worker threads, specify -1 for this parameter.

### Discarding Messages

If the queue reaches the so called "discard watermark" (a number of queued elements), less important messages can automatically be discarded. This is in an effort to save queue space for more important messages, which you even less like to loose. Please note that whenever there are more than "discard watermark" messages, both newly incoming as well as already enqueued low-priority messages are discarded. The algorithm discards messages newly coming in and those at the front of the queue.

The discard watermark is a last resort setting. It should be set sufficiently high, but low enough to allow for large message burst. Please note that it take effect immediately and thus shows effect promptly - but that doesn't help if the burst mainly consist of high-priority messages...

The discard watermark is set via the "$<object>QueueDiscardMark" directive. The priority of messages to be discarded is set via "$<object>QueueDiscardSeverity". This directive accepts both the usual textual severity as well as a numerical one. To understand it, you must be aware of the numerical severity values. They are defined in RFC 3164:

        Numerical         Severity
          Code

           0       Emergency: system is unusable
           1       Alert: action must be taken immediately
           2       Critical: critical conditions
           3       Error: error conditions
           4       Warning: warning conditions
           5       Notice: normal but significant condition
           6       Informational: informational messages
           7       Debug: debug-level messages
           
Anything of the specified severity and (numerically) above it is discarded. To turn message discarding off, simply specify the discard watermark to be higher than the queue size. An alternative is to specify the numerical value 8 as DiscardSeverity. This is also the default setting to prevent unintentional message loss. So if you would like to use message discarding, you need to set" $<object>QueueDiscardSeverity" to an actual value.

An interesting application is with disk-assisted queues: if the discard watermark is set lower than the high watermark, message discarding will start before the queue becomes disk-assisted. This may be a good thing if you would like to switch to disk-assisted mode only in cases where it is absolutely unavoidable and you prefer to discard less important messages first.

### Filled-Up Queues

If the queue has either reached its configured maximum number of entries or disk space, it is finally full. If so, rsyslogd throttles the data element submitter. If that, for example, is a reliable input (TCP, local log socket), that will slow down the message originator which is a good resolution for this scenario.

During throtteling, a disk-assisted queue continues to write to disk and messages are also discarded based on severity as well as regular dequeuing and processing continues. So chances are good the situation will be resolved by simply throttling. Note, though, that throtteling is highly undesirable for unreliable sources, like UDP message reception. So it is not a good thing to run into throtteling mode at all.

We can not hold processing infinitely, not even when throtteling. For example, throtteling the local log socket too long would cause the system at whole come to a standstill. To prevent this, rsyslogd times out after a configured period ("$<object>QueueTimeoutEnqueue", specified in milliseconds) if no space becomes available. As a last resort, it then discards the newly arrived message.

If you do not like throtteling, set the timeout to 0 - the message will then immediately be discarded. If you use a high timeout, be sure you know what you do. If a high main message queue enqueue timeout is set, it can lead to something like a complete hang of the system. The same problem does not apply to action queues.

### Rate Limiting

Rate limiting provides a way to prevent rsyslogd from processing things too fast. It can, for example, prevent overruning a receiver system.

Currently, there are only limited rate-limiting features available. The "$<object>QueueDequeueSlowdown"  directive allows to specify how long (in microseconds) dequeueing should be delayed. While simple, it still is powerful. For example, using a DequeueSlowdown delay of 1,000 microseconds on a UDP send action ensures that no more than 1,000 messages can be sent within a second (actually less, as there is also some time needed for the processing itself).

Processing Timeframes
Queues can be set to dequeue (process) messages only during certain timeframes. This is useful if you, for example, would like to transfer the bulk of messages only during off-peak hours, e.g. when you have only limited bandwidth on the network path the the central server.

Currently, only a single timeframe is supported and, even worse, it can only be specified by the hour. It is not hard to extend rsyslog's capabilities in this regard - it was just not requested so far. So if you need more fine-grained control, let us know and we'll probably implement it. There are two configuration directives, both should be used together or results are unpredictable:" $<object>QueueDequeueTimeBegin <hour>" and "$<object>QueueDequeueTimeEnd <hour>". The hour parameter must be specified in 24-hour format (so 10pm is 22). A use case for this parameter can be found in the rsyslog wiki.

### Performance

The locking involved with maintaining the queue has a potentially large performance impact. How large this is, and if it exists at all, depends much on the configuration and actual use case. However, the queue is able to work on so-called "batches" when dequeueing data elements. With batches, multiple data elements are dequeued at once (with a single locking call). The queue dequeues all available elements up to a configured upper limit (<object>DequeueBatchSize <number>). It is important to note that the actual upper limit is dictated by availability. The queue engine will never wait for a batch to fill. So even if a high upper limit is configured, batches may consist of fewer elements, even just one, if there are no more elements waiting in the queue.

Batching can improve performance considerably. Note, however, that it affects the order in which messages are passed to the queue worker threads, as each worker now receive as batch of messages. Also, the larger the batch size and the higher the maximum number of permitted worker threads, the more main memory is needed. For a busy server, large batch sizes (around 1,000 or even more elements) may be useful. Please note that with batching, the main memory must hold BatchSize * NumOfWorkers objects in memory (worst-case scenario), even if running in disk-only mode. So if you use the default 5 workers at the main message queue and set the batch size to 1,000, you need to be prepared that the main message queue holds up to 5,000 messages in main memory in addition to the configured queue size limits!

The queue object's default maximum batch size is eight, but there exists different defaults for the actual parts of rsyslog processing that utilize queues. So you need to check these object's defaults.

### Terminating Queues
Terminating a process sounds easy, but can be complex. Terminating a running queue is in fact the most complex operation a queue object can perform. You don't see that from a user's point of view, but its quite hard work for the developer to do everything in the right order.

The complexity arises when the queue has still data enqueued when it finishes. Rsyslog tries to preserve as much of it as possible. As a first measure, there is a regular queue time out ("$<object>QueueTimeoutShutdown", specified in milliseconds): the queue workers are given that time period to finish processing the queue.

If after that period there is still data in the queue, workers are instructed to finish the current data element and then terminate. This essentially means any other data is lost. There is another timeout ("$<object>QueueTimeoutActionCompletion", also specified in milliseconds) that specifies how long the workers have to finish the current element. If that timeout expires, any remaining workers are cancelled and the queue is brought down.

If you do not like to lose data on shutdown, the "$<object>QueueSaveOnShutdown" 
parameter can be set to "on". This requires either a disk or disk-assisted queue. 
If set, rsyslogd ensures that any queue elements are saved to disk before it terminates. 
This includes data elements there were begun being processed by workers that needed 
to be cancelled due to too-long processing. For a large queue, this operation may be lengthy. 
No timeout applies to a required shutdown save.

